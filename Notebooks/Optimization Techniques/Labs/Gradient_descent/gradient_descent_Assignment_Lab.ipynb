{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent and Stocastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Welcome! In this notebook, you will learn about Gradient Descent and Stochastic Gradient Descent (SGD) for optimizing a linear model. This assignment will guide you through the process of implementing these optimization algorithms and help you understand their underlying principles.\n",
    "\n",
    "#### Instructions\n",
    "- Do not modify any of the existing code.\n",
    "- Only write code when prompted. For example, in some sections, you will find the following,\n",
    "```\n",
    "# TODO\n",
    "```\n",
    "\n",
    "Only modify those sections of the code.\n",
    "\n",
    "\n",
    "You will learn to:\n",
    "\n",
    "- Build a simple model\n",
    "- Calculate Gradients for your model\n",
    "- Build your own Gradient Descent Model\n",
    "- Build your own Stocastic Gradient Descent Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Gradient Descent in NLP\n",
    "\n",
    "Gradient Descent is an optimization algorithm used to minimize a cost function in machine learning models, particularly in NLP tasks. It works by iteratively adjusting model parameters in the direction that reduces the cost function the most.\n",
    "\n",
    "The general update rule for Gradient Descent is:\n",
    "\n",
    "$$\n",
    "\\theta^{(t+1)} = \\theta^{(t)} - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta^{(t)}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- The current parameter value is:\n",
    "  $\n",
    "  \\theta^{(t)}\n",
    "  $\n",
    "\n",
    "- The learning rate is:\n",
    "  $\n",
    "  \\alpha\n",
    "  $\n",
    "\n",
    "- The gradient of the cost function with respect to the parameters is:\n",
    "  $\n",
    "  \\frac{\\partial J(\\theta)}{\\partial \\theta^{(t)}}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create our training data 12 pairs {x_i, y_i}\n",
    "# We'll try to fit the straight line model to these data\n",
    "data = np.array([[0.03,0.19,0.34,0.46,0.78,0.81,1.08,1.18,1.39,1.60,1.65,1.90],\n",
    "                 [0.67,0.85,1.05,1.00,1.40,1.50,1.30,1.54,1.55,1.68,1.73,1.60]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Model\n",
    "\n",
    "To start with, let's create a simple linear model. From there we can proceed for both gradient descent and SGD.\n",
    "In this model, we are predicting the output $ \\ y_{\\text{pred}} \\ $ based on a simple linear relationship between the input $ \\ x \\ $ and two parameters: **intercept** $ \\ \\theta_0 \\ $ and **slope** $ \\ \\theta_1 \\ $.\n",
    "\n",
    "The equation for the model is:\n",
    "\n",
    "$$ \\\n",
    "y_{\\text{pred}} = \\theta_0 + \\theta_1 \\cdot x\n",
    "\\ $$\n",
    "\n",
    "Where:\n",
    "- $ \\ \\theta_0 \\ $ is the **intercept** (the value of $ \\ y \\ $ when $ \\ x = 0 \\ $).\n",
    "- $ \\ \\theta_1 \\ $ is the **slope** (the rate at which $ \\ y_{\\text{pred}} \\ $ changes with respect to $ \\ x \\ $).\n",
    "- $ \\ x \\ $ is the input feature.\n",
    "\n",
    "This is a simple linear regression model that fits a straight line to the data. The goal of training the model is to find the best values for $ \\ \\theta_0 \\ $ and $ \\ \\theta_1 \\ $ that minimize the difference between the predicted values ($ \\ y_{\\text{pred}} \\ $) and the actual values in dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define our model -- just a straight line with intercept theta[0] and slope theta[1]\n",
    "def model(theta, x):\n",
    "    #TODO: implement the model\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's visualize how the model looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the parameters to some arbitrary values and draw the model\n",
    "theta = np.zeros((2, 1))\n",
    "theta[0] = 0.6      # Intercept\n",
    "theta[1] = -0.2     # Slope\n",
    "\n",
    "# Draw the model with initial parameters\n",
    "draw_model(data, model, theta, \"Initial parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the sum of squares loss for the training data, we have given compute_loss function to test that out\n",
    "### Sum of Squares Loss Function:\n",
    "\n",
    "The sum of squares loss function is given by:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\sum_{i=1}^{N} \\left( y_{\\text{pred}}^{(i)} - y_{\\text{true}}^{(i)} \\right)^2\n",
    "$$\n",
    "\n",
    "\n",
    "Where:\n",
    "- $ \\ y_{\\text{pred}}^{(i)} = \\theta_0 + \\theta_1 x^{(i)} \\ $ is the predicted value for the $ \\ i\\ $-th data point.\n",
    "- $ \\ y_{\\text{true}}^{(i)} \\ $ is the actual value for the \\(i\\)-th data point.\n",
    "- $ \\ N \\ $ is the number of data points.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just test that we got that right\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = compute_loss(data[0,:],data[1,:],model,np.array([[0.6],[-0.2]]))\n",
    "print('Your loss = %3.3f, Correct loss = %3.3f'%(loss, 12.367))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the whole loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_loss_function(compute_loss, data, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Gradient Calculation:\n",
    "\n",
    "Now we need to compute the partial derivatives of the loss function with respect to $\\ \\theta_0 \\ $ and $ \\ \\theta_1 \\ $ (the intercept and slope, respectively).\n",
    "\n",
    "#### Derivative with respect to $ \\ \\theta_0 \\ $ (intercept):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_0} = \\frac{2}{N} \\sum_{i=1}^{N} \\left( y_{\\text{pred}}^{(i)} - y_{\\text{true}}^{(i)} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "#### Derivative with respect to $ \\ \\theta_1 \\ $ (slope):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_1} = \\frac{2}{N} \\sum_{i=1}^{N} \\left( y_{\\text{pred}}^{(i)} - y_{\\text{true}}^{(i)} \\right) x^{(i)}\n",
    "$$\n",
    "\n",
    "\n",
    "Where:\n",
    "- $ \\ y_{\\text{pred}}^{(i)} = \\theta_0 + \\theta_1 x^{(i)} \\ $ is the predicted value for the $ \\ i\\ $-th data point.\n",
    "- $ \\ x^{(i)} \\ $ is the feature value for the $ \\ i\\ $-th data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(data_x, data_y, theta):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the sum of squares loss function with respect to theta (parameters).\n",
    "    \n",
    "    Parameters:\n",
    "    data_x : array-like\n",
    "        Input feature data (independent variable).\n",
    "    data_y : array-like\n",
    "        True output data (dependent variable).\n",
    "    theta : array-like\n",
    "        Model parameters (weights: intercept and slope).\n",
    "    \n",
    "    Returns:\n",
    "    gradient : array-like\n",
    "        The gradient of the loss function with respect to theta_0 and theta_1.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Number of data points\n",
    "    N = len(data_x)\n",
    "    \n",
    "    # Model predictions: y_pred = theta[0] + theta[1] * x\n",
    "    y_pred = theta[0] + theta[1] * data_x\n",
    "    \n",
    "    # Compute gradients\n",
    "    \n",
    "    dl_dtheta0 = #TODO: compute the gradient for theta[0]\n",
    "    dl_dtheta1 = #TODO: compute the gradient for theta[1]\n",
    "    \n",
    "    # Return the gradient as a column vector\n",
    "    return np.array([[dl_dtheta0], [dl_dtheta1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, let's see you got that right.\n",
    "We should expect somewhere near these values\n",
    "- Your gradients: (-1.825, -2.237)\n",
    "- Approx gradients: (-21.903, -26.839)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the gradient using your function\n",
    "gradient = compute_gradient(data[0,:], data[1,:], theta)\n",
    "print(\"Your gradients: (%3.3f, %3.3f)\" % (gradient[0], gradient[1]))\n",
    "\n",
    "# Approximate the gradients with finite differences\n",
    "delta = 0.0001\n",
    "\n",
    "# Compute finite difference approximation for theta_0 (intercept)\n",
    "dl_dtheta0_est = (compute_loss(data[0,:], data[1,:], model, theta + np.array([[delta], [0]])) - \\\n",
    "                  compute_loss(data[0,:], data[1,:], model, theta)) / delta\n",
    "\n",
    "# Compute finite difference approximation for theta_1 (slope)\n",
    "dl_dtheta1_est = (compute_loss(data[0,:], data[1,:], model, theta + np.array([[0], [delta]])) - \\\n",
    "                  compute_loss(data[0,:], data[1,:], model, theta)) / delta\n",
    "\n",
    "print(\"Approx gradients: (%3.3f, %3.3f)\" % (dl_dtheta0_est, dl_dtheta1_est))\n",
    "\n",
    "# There might be small differences in the last significant figure because finite gradients is an approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Step Explanation\n",
    "\n",
    "The `gradient_descent_step` function performs one iteration of **Gradient Descent** to update the model parameters (intercept and slope) in order to minimize the loss function.\n",
    "\n",
    "1. **Compute the Gradient**:\n",
    "   - The gradient represents the direction and rate of the steepest increase in the loss function with respect to the model parameters.\n",
    "   - The function `compute_gradient` is used to calculate this gradient.\n",
    "   \n",
    "   $ \\text{gradient} = \\frac{\\partial J(\\theta)}{\\partial \\theta} $\n",
    "   \n",
    "   Where:\n",
    "   - $ J(\\theta) $ is the loss function (e.g., Mean Squared Error).\n",
    "   - $ \\theta $ represents the model parameters (intercept and slope).\n",
    "\n",
    "2. **Line Search for Optimal Step Size**:\n",
    "   - We have included Line search function for you. The function takes in these parameter in order `line_search(data, model, parameter, gradient)`\n",
    "   - The step size $ \\alpha $ determines how far we move in the direction of the negative gradient.\n",
    "   - The `line_search` function is used to find the optimal step size that minimizes the loss along the direction of the negative gradient $ \\ (-\\text{gradient}) \\ $\n",
    "   \n",
    "   This ensures that we take an appropriate step size that reduces the loss function effectively.\n",
    "\n",
    "3. **Update Parameters**:\n",
    "   - Once we have computed the gradient and found the optimal step size, we update the parameters $ \\theta $ by moving in the direction of the negative gradient (i.e., downhill).\n",
    "   \n",
    "   $ \\theta = \\theta + \\alpha (-\\text{gradient}) $\n",
    "   \n",
    "   This update rule ensures that we are moving towards minimizing the loss function.\n",
    "\n",
    "#### Full Update Rule:\n",
    "\n",
    "The complete update rule for Gradient Descent can be written as:\n",
    "\n",
    "$$\n",
    "\\theta^{t+1} = \\theta^t - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta^t}\n",
    "$$\n",
    "\n",
    "\n",
    "Where:\n",
    "- $ t $ represents the current iteration.\n",
    "- $ J(\\theta) $ is the cost function.\n",
    "- $ \\frac{\\partial J(\\theta)}{\\partial \\theta^t} $ is the gradient of $ J(\\theta) $ with respect to $ \\theta^t $.\n",
    "- $ \\alpha $ is a positive scalar known as the **learning rate** or **step size**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_step(theta, data, model):\n",
    "    \"\"\"\n",
    "    Performs one step of gradient descent.\n",
    "\n",
    "    Parameters:\n",
    "    - theta: Current parameters (intercept and slope).\n",
    "    - data: The dataset (x and y values).\n",
    "    - model: The model used to make predictions.\n",
    "\n",
    "    Returns:\n",
    "    - Updated theta after one gradient descent step.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Compute the gradient (you wrote this function above)\n",
    "    gradient = compute_gradient(data[0, :], data[1, :], theta)\n",
    "\n",
    "    # Step 2: Find the best step size alpha using line search function (above)\n",
    "    # Use negative gradient as we are going downhill\n",
    "    alpha = #TODO call line_search function with appropriate parameters\n",
    "\n",
    "    # Step 3: Update the parameters theta based on the gradient and step size alpha\n",
    "    theta = #TODO: update theta\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if you got that right, run the below cell and for the final iteration your loss should be approximately 1.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the parameters and draw the model\n",
    "n_steps = 10\n",
    "theta_all = np.zeros((2, n_steps + 1))\n",
    "theta_all[0, 0] = 1.6  # Initial intercept\n",
    "theta_all[1, 0] = -0.5  # Initial slope\n",
    "\n",
    "# Measure loss and draw initial model\n",
    "loss = compute_loss(data[0, :], data[1, :], model, theta_all[:, 0:1])\n",
    "draw_model(data, model, theta_all[:, 0:1], \"Initial parameters, Loss = %f\" % (loss))\n",
    "\n",
    "# Repeatedly take gradient descent steps\n",
    "for c_step in range(n_steps):\n",
    "    # Do gradient descent step\n",
    "    theta_all[:, c_step+1:c_step+2] = gradient_descent_step(theta_all[:, c_step:c_step+1], data, model)\n",
    "    \n",
    "    # Measure loss and draw model\n",
    "    loss = compute_loss(data[0, :], data[1, :], model, theta_all[:, c_step+1:c_step+2])\n",
    "    draw_model(data, model, theta_all[:, c_step+1], \"Iteration %d, loss = %f\" % (c_step + 1, loss))\n",
    "\n",
    "# Draw the trajectory on the loss function\n",
    "draw_loss_function(compute_loss, data, model, theta_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent Step Explanation\n",
    "\n",
    "The `stochastic_gradient_descent_step` function performs one iteration of **Stochastic Gradient Descent (SGD)** to update the model parameters (intercept and slope) in order to minimize the loss function.\n",
    "\n",
    "\n",
    "1. **Shuffle the Data**:\n",
    "    - The data is shuffled to ensure that the model does not learn in a specific order, which helps in better generalization.\n",
    "\n",
    "2. **Iterate Over Each Data Point**:\n",
    "    - For each data point in the shuffled dataset, the following steps are performed:\n",
    "\n",
    "3. **Compute the Gradient**:\n",
    "    - The gradient represents the direction and rate of the steepest increase in the loss function with respect to the model parameters.\n",
    "    - The function `compute_gradient` is used to calculate this gradient for the current data point.\n",
    "\n",
    "    $ \\text{gradient} = \\frac{\\partial J(\\theta)}{\\partial \\theta} $\n",
    "\n",
    "    Where:\n",
    "    - $ J(\\theta) $ is the loss function.\n",
    "    - $ \\theta $ represents the model parameters (intercept and slope).\n",
    "\n",
    "4. **Update Parameters**:\n",
    "    - The parameters $ \\theta $ are updated based on the gradient and the learning rate.\n",
    "    - The update rule is:\n",
    "\n",
    "    $ \\theta = \\theta - \\text{learning\\_rate} \\times \\text{gradient} $\n",
    "\n",
    "    This update rule ensures that we are moving towards minimizing the loss function.\n",
    "\n",
    "#### Full Update Rule:\n",
    "\n",
    "The complete update rule for Stochastic Gradient Descent can be written as:\n",
    "\n",
    "$$\n",
    "\\theta^{t+1} = \\theta^t - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta^t}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ t $ represents the current iteration.\n",
    "- $ J(\\theta) $ is the cost function.\n",
    "- $ \\frac{\\partial J(\\theta)}{\\partial \\theta^t} $ is the gradient of $ J(\\theta) $ with respect to $ \\theta^t $.\n",
    "- $ \\alpha $ is a positive scalar known as the **learning rate**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent_step(theta, data, model, learning_rate=0.01):\n",
    "    \"\"\"\n",
    "    Performs one step of stochastic gradient descent.\n",
    "\n",
    "    Parameters:\n",
    "    - theta: Current parameters (intercept and slope).\n",
    "    - data: The dataset (x and y values).\n",
    "    - model: The model used to make predictions.\n",
    "    - learning_rate: The learning rate for SGD.\n",
    "\n",
    "    Returns:\n",
    "    - Updated theta after one stochastic gradient descent step.\n",
    "    \"\"\"\n",
    "    # Shuffle the data\n",
    "    indices = np.random.permutation(data.shape[1])\n",
    "    data_shuffled = data[:, indices]\n",
    "\n",
    "    for i in range(data.shape[1]):\n",
    "        x_i = data_shuffled[0, i]\n",
    "        y_i = data_shuffled[1, i]\n",
    "\n",
    "        # Compute the gradient for the current data point\n",
    "        gradient = #TODO: compute the gradient for the current data point\n",
    "\n",
    "        # Update the parameters theta based on the gradient and learning rate\n",
    "        theta = #TODO: update theta\n",
    "\n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if you got that right, run the below cell and for the final iteration your loss should be approximately 1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the parameters and draw the model\n",
    "n_epochs = 10\n",
    "theta_sgd = np.zeros((2, 1))\n",
    "theta_sgd[0] = 1.6  # Initial intercept\n",
    "theta_sgd[1] = -0.5  # Initial slope\n",
    "\n",
    "# Measure loss and draw initial model\n",
    "loss = compute_loss(data[0, :], data[1, :], model, theta_sgd)\n",
    "draw_model(data, model, theta_sgd, \"Initial parameters, Loss = %f\" % (loss))\n",
    "\n",
    "# Repeatedly take stochastic gradient descent steps\n",
    "for epoch in range(n_epochs):\n",
    "    # Do stochastic gradient descent step\n",
    "    theta_sgd = stochastic_gradient_descent_step(theta_sgd, data, model)\n",
    "\n",
    "    # Measure loss and draw model\n",
    "    loss = compute_loss(data[0, :], data[1, :], model, theta_sgd)\n",
    "    draw_model(data, model, theta_sgd, \"Epoch %d, loss = %f\" % (epoch + 1, loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "In this Lab we learned about Stochastic Gradient Descent (SGD) and how it can be used to optimize a model by updating the parameters based on the gradient of the loss function. \n",
    "The key takeaways from this lab are:\n",
    "\n",
    "1. **Gradient Descent**:\n",
    "    - Gradient Descent is an optimization algorithm used to minimize a cost function by iteratively adjusting model parameters.\n",
    "    - The update rule for Gradient Descent is: \n",
    "      $$\n",
    "      \\theta^{(t+1)} = \\theta^{(t)} - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta^{(t)}}\n",
    "      $$\n",
    "    - The notebook demonstrates how to compute gradients and perform gradient descent steps to minimize the loss function.\n",
    "\n",
    "2. **Linear Model**:\n",
    "    - A simple linear model is used to predict the output based on a linear relationship between the input and two parameters: intercept ($\\theta_0$) and slope ($\\theta_1$).\n",
    "    - The equation for the model is: \n",
    "      $$\n",
    "      y_{\\text{pred}} = \\theta_0 + \\theta_1 \\cdot x\n",
    "      $$\n",
    "\n",
    "3. **Loss Function**:\n",
    "    - The sum of squares loss function is used to measure the difference between the predicted values and the actual values.\n",
    "    - The loss function is given by:\n",
    "      $$\n",
    "      J(\\theta) = \\sum_{i=1}^{N} \\left( y_{\\text{pred}}^{(i)} - y_{\\text{true}}^{(i)} \\right)^2\n",
    "      $$\n",
    "\n",
    "4. **Stochastic Gradient Descent (SGD)**:\n",
    "    - SGD is a variant of Gradient Descent where the model parameters are updated for each data point, rather than the entire dataset.\n",
    "    - The update rule for SGD is:\n",
    "      $$\n",
    "      \\theta^{t+1} = \\theta^t - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta^t}\n",
    "      $$\n",
    "\n",
    "\n",
    "For a better understanding on how Stochastic Gradient Descent works, you can refer to the https://github.com/udlbook/udlbook/blob/main/Notebooks/Chap06/6_3_Stochastic_Gradient_Descent.ipynb. \n",
    "\n",
    "In the above given notebook they have used Gabor model to explain the Stochastic Gradient Descent.\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
