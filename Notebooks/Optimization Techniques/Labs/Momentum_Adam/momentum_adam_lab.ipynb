{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum and Adam\n",
    "\n",
    "Welcome! In this notebook, you will learn about building Momentum and Adam. This assignment will guide you through the process of implementing these optimization algorithms and help you understand their underlying principles.\n",
    "\n",
    "#### Instructions\n",
    "- Do not modify any of the existing code.\n",
    "- Only write code when prompted. For example, in some sections, you will find the following,\n",
    "```\n",
    "# TODO\n",
    "```\n",
    "\n",
    "Only modify those sections of the code.\n",
    "\n",
    "\n",
    "You will learn to:\n",
    "\n",
    "- Build your own Momentum\n",
    "- Build your own Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start by running gradient descent with different learning rate and steps, purpose is to observe how the learning rate affects the convergence of the algorithm.\n",
    "\n",
    "**Get Loss Function for Plotting**:\n",
    "    We are calling the `get_loss_function_for_plot` function to obtain the loss function and the mesh grids (`x1mesh` and `x2mesh`) for plotting.\n",
    "\n",
    "**Initialize Starting Position**:\n",
    "    This block initializes the starting position for the gradient descent algorithm.\n",
    "\n",
    "**Run Gradient Descent with Small Learning Rate**:\n",
    "    This block runs the gradient descent algorithm with a learning rate (`alpha`). The `grad_descent` function returns the path taken by the gradient descent, which is then plotted using the `draw_function`.\n",
    "\n",
    "**Run Gradient Descent with Large Learning Rate**:\n",
    "    This block runs the gradient descent algorithm again, but this time with a larger learning rate (`alpha`). The resulting path is also plotted using the `draw_function`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function, x1mesh, x2mesh = get_loss_function_for_plot()\n",
    "\n",
    "start_posn = np.zeros((2,1))\n",
    "start_posn[0,0] = -0.7\n",
    "start_posn[1,0] = -0.9\n",
    "\n",
    "# Run gradient descent\n",
    "grad_path1 = grad_descent(start_posn, n_steps=200, alpha=0.08)\n",
    "draw_function(x1mesh, x2mesh, loss_function, my_colormap, grad_path1)\n",
    "\n",
    "grad_path2 = grad_descent(start_posn, n_steps=40, alpha=1.0)\n",
    "draw_function(x1mesh, x2mesh, loss_function, my_colormap, grad_path2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the function changes much faster in x2 than in x1, there is no great step size to choose. If we set the step size so that it makes sensible progress in the x2 direction, then it takes many iterations to converge. If we set the step size so that we make sensible progress in the x1 direction, then the path oscillates in the x2 direction.\n",
    "This motivates Adam. At the core of Adam is the idea that we should just determine which way is downhill along each axis (i.e., left/right for x1 or up/down for x2) and move a fixed distance in that direction.\n",
    "The `normalized_gradients` function implements a variant of gradient descent that normalizes the gradients to improve convergence. This method is inspired by the RMSProp optimization algorithm.\n",
    "\n",
    "\n",
    "**Function Steps**:\n",
    "1. **Initialization**:\n",
    "    - `grad_path` is initialized to store the path of the gradient descent.\n",
    "    - `v` is initialized to store the accumulated squared gradients.\n",
    "\n",
    "2. **Gradient Descent Loop**:\n",
    "    - For each step, the gradient `m` is computed using the `get_loss_gradient` function.\n",
    "    - The squared gradient `v` is updated by accumulating the squared gradients.\n",
    "    - The position is updated using the normalized gradient\n",
    "\n",
    "For this we should note that:\n",
    "- Gradient measurement\n",
    "- Accumulation of squared gradients:\n",
    "  $$\n",
    "  v = v + m^2\n",
    "  $$\n",
    "- Update rule:\n",
    "  $$\n",
    "  \\text{grad\\_path}[:, c\\_step + 1] = \\text{grad\\_path}[:, c\\_step] - \\alpha \\frac{m}{\\sqrt{v} + \\epsilon}\n",
    "  $$\n",
    "\n",
    "This method helps in stabilizing the learning process by normalizing the gradients, which can lead to better convergence properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_gradients(start_posn, n_steps, alpha, epsilon=1e-20):\n",
    "    grad_path = np.zeros((2, n_steps + 1))\n",
    "    grad_path[:, 0] = start_posn[:, 0]\n",
    "    \n",
    "    v = np.zeros_like(grad_path[:, 0])  # Initialize velocity (as per momentum)\n",
    "\n",
    "    for c_step in range(n_steps):\n",
    "        # Measure the gradient as in equation 6.13 (first line)\n",
    "        m = get_loss_gradient(grad_path[0, c_step], grad_path[1, c_step])\n",
    "        \n",
    "        # TODO Compute the squared gradient \n",
    "        v = 0  # Accumulate squared gradients\n",
    "        \n",
    "        # TODO Apply the update rule \n",
    "        grad_path[:, c_step + 1] = grad_path[:, c_step] # complete this line\n",
    "    \n",
    "    return grad_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot with fixed positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try out normalized gradients\n",
    "start_posn = np.zeros((2,1))\n",
    "start_posn[0,0] = -0.7\n",
    "start_posn[1,0] = -0.9\n",
    "\n",
    "# Run gradient descent\n",
    "grad_path1 = normalized_gradients(start_posn, n_steps=40, alpha=0.08)\n",
    "draw_function(x1mesh, x2mesh, loss_function, my_colormap, grad_path1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `momentum` function implements gradient descent with momentum, which helps accelerate gradients vectors in the right directions, thus leading to faster converging.\n",
    "\n",
    "**Function Steps**:\n",
    "1. **Initialization**:\n",
    "    - `grad_path` is initialized to store the path of the gradient descent.\n",
    "    - `velocity` is initialized to store the momentum term.\n",
    "\n",
    "2. **Gradient Descent Loop**:\n",
    "    - For each step, the gradient `grad` is computed using the `get_loss_gradient` function.\n",
    "    - The velocity is updated using the momentum equation:\n",
    "      $$\n",
    "      \\text{velocity} = \\beta \\cdot \\text{velocity} + (1 - \\beta) \\cdot \\text{grad}\n",
    "      $$\n",
    "    - The position is updated using the velocity:\n",
    "      $$\n",
    "      \\text{grad\\_path}[:, c\\_step + 1] = \\text{grad\\_path}[:, c\\_step] - \\alpha \\cdot \\text{velocity}\n",
    "      $$\n",
    "\n",
    "This method helps in smoothing the optimization path and can lead to faster convergence by dampening oscillations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum(start_posn, n_steps, alpha, beta=0.9):\n",
    "    \"\"\"\n",
    "    Implements gradient descent with momentum based on the equations from the PDF.\n",
    "    \n",
    "    Args:\n",
    "        start_posn: Initial position (numpy array of shape (2, 1)).\n",
    "        n_steps: Number of steps to run the optimization.\n",
    "        alpha: Learning rate.\n",
    "        beta: Momentum coefficient (controls how much of the past gradients are retained).\n",
    "        \n",
    "    Returns:\n",
    "        grad_path: History of positions during optimization.\n",
    "    \"\"\"\n",
    "    grad_path = np.zeros((2, n_steps + 1))\n",
    "    grad_path[:, 0] = start_posn[:, 0]\n",
    "    \n",
    "    velocity = np.zeros_like(grad_path[:, 0])  # Initialize velocity as zero\n",
    "\n",
    "    for c_step in range(n_steps):\n",
    "        # Measure the gradient\n",
    "        grad = get_loss_gradient(grad_path[0, c_step], grad_path[1, c_step])\n",
    "        \n",
    "        # TODO: Update the velocity using momentum (equation from PDF)\n",
    "        velocity = 0 # Update the velocity\n",
    "        \n",
    "        # TODO: Apply the update rule\n",
    "        grad_path[:, c_step + 1] = grad_path[:, c_step] # complete this line\n",
    "    \n",
    "    return grad_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot and see how Momentum has converged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting function to visualize gradients properly\n",
    "start_posn = np.zeros((2, 1))\n",
    "start_posn[0, 0] = -0.7\n",
    "start_posn[1, 0] = -0.9\n",
    "\n",
    "# Run momentum-based gradient descent with higher learning rate and momentum coefficient\n",
    "grad_path_momentum = momentum(start_posn, n_steps=200, alpha=0.1, beta=0.95)\n",
    "\n",
    "# Plot the results\n",
    "draw_function(x1mesh, x2mesh, loss_function, my_colormap, grad_path_momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We never actually converge -- the solution just bounces back and forth between the last two points. To make it converge, we add momentum to both the estimates of the gradient and the pointwise squared gradient. We also modify the statistics by a factor that depends on the time to make sure the progress is not slow to start with.\n",
    "\n",
    "\n",
    "\n",
    "The `adam` function implements the Adam optimization algorithm, which combines the advantages of both the Momentum and RMSProp algorithms. It adapts the learning rate for each parameter by computing individual adaptive learning rates.\n",
    "\n",
    "**Function Steps**:\n",
    "1. **Initialization**:\n",
    "    - `grad_path` is initialized to store the path of the gradient descent.\n",
    "    - `m` is initialized to store the first moment (mean of gradients).\n",
    "    - `v_t` is initialized to store the second moment (mean of squared gradients).\n",
    "\n",
    "2. **Gradient Descent Loop**:\n",
    "    - For each step, the gradient `grad` is computed using the `get_loss_gradient` function.\n",
    "    - The first moment `m` is updated using the momentum-based gradient estimate:\n",
    "      $$\n",
    "      m_t = \\beta_1 \\cdot m + (1 - \\beta_1) \\cdot \\text{grad}\n",
    "      $$\n",
    "    - The second moment `v_t` is updated using the momentum-based squared gradient estimate:\n",
    "      $$\n",
    "      v_t = \\beta_2 \\cdot v_{t-1} + (1 - \\beta_2) \\cdot \\text{grad}^2\n",
    "      $$\n",
    "    - The statistics are modified to correct the bias:\n",
    "      $$\n",
    "      \\hat{m} = \\frac{m}{1 - \\beta_1^{t}}\n",
    "      $$\n",
    "      $$\n",
    "      \\hat{v_t} = \\frac{v_t}{1 - \\beta_2^{t}}\n",
    "      $$\n",
    "    - The position is updated using the corrected estimates:\n",
    "      $$\n",
    "      \\theta_{t+1} = \\theta_t - \\alpha \\cdot \\frac{\\hat{m}}{\\sqrt{\\hat{v_t}} + \\epsilon}\n",
    "      $$\n",
    "\n",
    "This method helps in stabilizing the learning process by normalizing the gradients and adapting the learning rate, which can lead to better convergence properties.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam(start_posn, n_steps, alpha, beta=0.9, gamma=0.99, epsilon=1e-20):\n",
    "    grad_path = np.zeros((2, n_steps + 1))\n",
    "    grad_path[:, 0] = start_posn[:, 0]\n",
    "    m = np.zeros_like(grad_path[:, 0])  # First moment (mean of gradients)\n",
    "    v = np.zeros_like(grad_path[:, 0])  # Second moment (mean of squared gradients)\n",
    "\n",
    "    for c_step in range(n_steps):\n",
    "        # Measure the gradient\n",
    "        grad = get_loss_gradient(grad_path[0, c_step], grad_path[1, c_step])\n",
    "\n",
    "        # TODO: \n",
    "        m = 0 # Update the momentum-based gradient estimate\n",
    "\n",
    "        # TODO: \n",
    "        v = 0 # Update the momentum-based squared gradient estimate\n",
    "\n",
    "        # TODO -- Modify the statistics according to equation 6.16\n",
    "        # You will need the function np.power\n",
    "        # Replace these lines\n",
    "        m_tilde = m \n",
    "        v_tilde = v \n",
    "\n",
    "        # TODO: Apply the update rule for Adam\n",
    "        grad_path[:, c_step + 1] = grad_path[:, c_step] # complete this line\n",
    "\n",
    "    return grad_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try out our Adam algorithm\n",
    "start_posn = np.zeros((2,1))\n",
    "start_posn[0,0] = -0.7\n",
    "start_posn[1,0] = -0.9\n",
    "\n",
    "# Run Adam optimizer\n",
    "grad_path1 = adam(start_posn, n_steps=60, alpha=0.05)\n",
    "draw_function(x1mesh, x2mesh, loss_function, my_colormap, grad_path1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "**Momentum**:\n",
    "    - Momentum helps accelerate gradients vectors in the right directions, leading to faster converging.\n",
    "    - The momentum term (`beta`) helps in smoothing the optimization path and dampening oscillations.\n",
    "\n",
    "**Normalized Gradients**:\n",
    "    - Normalizing gradients can improve convergence by stabilizing the learning process.\n",
    "    - This method is inspired by the RMSProp optimization algorithm.\n",
    "\n",
    "**Adam Optimization**:\n",
    "    - Adam combines the advantages of both Momentum and RMSProp algorithms.\n",
    "    - It adapts the learning rate for each parameter by computing individual adaptive learning rates.\n",
    "    - The algorithm uses first moment (mean of gradients) and second moment (mean of squared gradients) to update the parameters.\n",
    "\n",
    "By understanding and implementing these optimization algorithms, you gain insights into their behavior and how they can be applied to various machine learning problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
