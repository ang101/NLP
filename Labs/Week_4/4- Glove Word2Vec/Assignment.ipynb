{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove and Word2Vec Assignment (Graded)\n",
    "Welcome to your programming assignment on Glove and Word2Vec, you will learn how to build Glove and Word2Vec using pretrained libraries on different Restaurant reviews and Amazon's Cell phone and accessories reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Description\n",
    "- In Glove, You will be building to    \n",
    "    - Preprocess the dataset\n",
    "    - Find word similarity using Cosine distance\n",
    "    - Finding Probability of event of one word to another one\n",
    "\n",
    "- In Word2Vec, you will building to\n",
    "    - Preprocess and tokenize them\n",
    "    - Build the vocab\n",
    "    - Train the model\n",
    "    - Find Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "- Do not modify any of the codes.\n",
    "- Only write code when prompted. For example in some sections you will find the following,\n",
    "```\n",
    "# YOUR CODE GOES HERE\n",
    "# YOUR CODE ENDS HERE\n",
    "# TODO\n",
    "```\n",
    "\n",
    "Only modify those sections of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table(\"Restaurant_Reviews.tsv\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x = df['Liked'], data=df, palette='hls')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Download stopwords, punkt_tab using `nltk.download()` function.\n",
    "- for the `stop_words` add punctuations like `, . \" '`\n",
    "- Convert Upper case words to Lower Case\n",
    "- Remove White space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Download the stopwords and punkt_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "#////\n",
    "\n",
    "array = df.to_numpy()#convert to numpy\n",
    "\n",
    "#stop words list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#add other things to stop_words\n",
    "# TODO: Add punctuation\n",
    "# TODO: Add punctuation\n",
    "# TODO: Add punctuation\n",
    "# TODO: Add punctuation\n",
    "stemmer= PorterStemmer()\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "l = len(array)\n",
    "\n",
    "for i in range(l):\n",
    "    array[i][0] =  # TODO: Upper to lower\n",
    "    array[i][0] = ''.join((z for z in array[i][0] if not z.isdigit())) #removing numbers\n",
    "    array[i][0] =  # TODO: Removing white space\n",
    "    array[i][0] = array[i][0].translate(str.maketrans('', '', string.punctuation))#remove Punctuation\n",
    "    array[i][0] = re.sub('http://\\S+|https://\\S+', '', array[i][0])#remove http adress\n",
    "    word_tokens = word_tokenize(array[i][0]) #Tokenize\n",
    "    array[i][0] = word_tokens\n",
    "\n",
    "    array[i][0] = [word for word in array[i][0] if not word in stopwords.words()]\n",
    "    a = []\n",
    "    le = len(array[i][0])\n",
    "\n",
    "    for word in array[i][0]:\n",
    "\n",
    "            w1 = stemmer.stem(word)\n",
    "            w2 = \"\".join(word)\n",
    "            #w2 = lemmatizer.lemmatize(word)#lemmatize\n",
    "            a.append(w2)\n",
    "            array[i][0] = a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WordCloud of Like and Dislike Comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_tokens = ''\n",
    "comment_words = ''\n",
    "for i in range(l):\n",
    "    l2 =len(array[i][0])\n",
    "    for j in range(l2):\n",
    "        Total_tokens = Total_tokens+' '+array[i][0][j]\n",
    "\n",
    "wordcloud = WordCloud().generate(Total_tokens)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task: Word Cloud of Like Comment and Dislike comment\n",
    "- Generate for Like comment and Dislike, Note: like 1, Dislike 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_tokens = ''\n",
    "comment_words = ''\n",
    "\n",
    "for i in range(l):\n",
    "    if # TODO: Complete this if statement for Like Comment\n",
    "        l2 =len(array[i][0])\n",
    "        for j in range(l2):\n",
    "            Total_tokens = Total_tokens+' '+array[i][0][j]\n",
    "wordcloud = WordCloud().generate(Total_tokens)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_tokens = ''\n",
    "comment_words = ''\n",
    "\n",
    "for i in range(l):\n",
    "    if # TODO: Complete this if statement for Dislike Comment\n",
    "        l2 =len(array[i][0])\n",
    "        for j in range(l2):\n",
    "            Total_tokens = Total_tokens+' '+array[i][0][j]\n",
    "wordcloud = WordCloud().generate(Total_tokens)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Word Embedding by Glove\n",
    "#### Task: List of Total Keywords\n",
    "- Append the list to Total Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_list = array[:,0]\n",
    "Total_keywords = []\n",
    "\n",
    "l = len(a_list)\n",
    "for i in range(l):\n",
    "    l2 = len(a_list[i])\n",
    "    for j in range(l2):\n",
    "        # TODO: Append the word to Total_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a unique vectore\n",
    "Total_keywords = set(Total_keywords)\n",
    "print('Total number of keywords is : ',len(Total_keywords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected total Number of Keyword is 1714"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Factorization matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(Total_keywords)#lenght of the dimention of the quadratic matrix\n",
    "matrix = np.zeros(shape=(n,n))# n*n zero matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_keywords = list(Total_keywords)\n",
    "l = len(array)\n",
    "for i in range(n):\n",
    "    for j in range(l):\n",
    "        l2 = len(array[j,0])\n",
    "        for k in range(l2):\n",
    "            if Total_keywords[i] == array[j,0][k]:\n",
    "                if k>0 and k<l2-1:\n",
    "                    for s in range(n):\n",
    "                        if array[j,0][k-1] == Total_keywords[s]:\n",
    "                            matrix[s][i] = matrix[s][i]+1\n",
    "                        elif array[j,0][k+1] == Total_keywords[s]:\n",
    "                            matrix[s][i] = matrix[s][i]+1\n",
    "                elif k==0 and l2>1:\n",
    "                    for s in range(n):\n",
    "                        if array[j,0][k+1] == Total_keywords[s]:\n",
    "                            matrix[s][i] = matrix[s][i]+1\n",
    "                elif k==l2-1 and l2!=0:\n",
    "                    for s in range(n):\n",
    "                        if array[j,0][k-1] == Total_keywords[s]:\n",
    "                            matrix[s][i] = matrix[s][i]+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Task: Create data frame Glove Matrix\n",
    "- create a glove data frame with glove matrix and total keywords as columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Glove = pd.DataFrame() # TODO: Complete the dataframe by adding the matrix and the column\n",
    "Glove.set_index([pd.Index(Total_keywords)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task: Finding word similarity by Cosine distance\n",
    "- convert a and b from Pandas series to Numpy format\n",
    "- Calculate cosine distane between a and b using `distance.cosine()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing two vectors by Cosin distance\n",
    "from scipy.spatial import distance\n",
    "a = Glove['unsatisfying']#word vector a\n",
    "b = Glove['refused']#word vector b\n",
    "a = # TODO: Convert a to numpy format from Pandas series\n",
    "b = # TODO: Convert b to numpy format from Pandas series\n",
    "# TODO: Calculate the cosine distance between a and b and print it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected distance 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task: Finding Probability of event of one word to another one\n",
    "##### P(word a|word b) = (Total repetition a after or befor b/ Total of a in corpus)\n",
    "- find the probablity of `menu` to `great`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = # TODO: From the Glove dataframe get the vector of the word 'menu'\n",
    "b = # TODO: From the Glove dataframe get the vector of the word 'great'\n",
    "a = # TODO: Convert a to numpy format from Pandas series\n",
    "b = # TODO: Convert b to numpy format from Pandas series\n",
    "l1 = len(array)\n",
    "counter = 0\n",
    "for i in range(l1):\n",
    "    l2 = len(array[i,0])\n",
    "    for j in range(l2):\n",
    "        if 'menu' == array[i,0][j]:\n",
    "            counter = counter + 1\n",
    "d = len(Total_keywords)            \n",
    "for i in range(d):\n",
    "    if 'menu' == Total_keywords[i]:\n",
    "        f = i\n",
    "s = Glove.loc[f, 'great']        \n",
    "print('Total number of word a :',counter)  \n",
    "print('Total number of word a after or befor b:',Glove.loc[f, 'great'])\n",
    "print('probability of event word a to word b is:',s/counter*100,'%')\n",
    "print('cosin similarity is :', round(distance.cosine(a, b),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected probability of event word is 13% and cosin similarity 0.961"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we are using here is a subset of Amazon reviews from the Cell Phones & Accessories category. The data is stored as a JSON file and can be read using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"Cell_Phones_and_Accessories_5.json\", lines=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task: Simple Preprocessing & Tokenization\n",
    "The first thing to do for any data science task is to clean the data. For NLP, we apply various processing like converting all the words to lower case, trimming spaces, removing punctuations. This is something we will do over here too.\n",
    "\n",
    "- simple preprocess\n",
    "- remove stop words like 'and', 'or', 'is', 'the', 'a', 'an'.\n",
    "- convert words to their root forms like 'running' to 'run'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_text = df['reviewText'] # TODO: apply gensim's simple_preprocess to reviewText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: retrieve the review text of the first review in the dataset using loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output: \"They look good and stick good! I just don't like the rounded shape because I was always bumping it and Siri kept popping up and it was irritating. I just won't buy a product like this again\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task: Training the Word2Vec Model\n",
    "- Train the model for reviews. Use a window of size 10 i.e. 10 words before the present word and 10 words ahead. \n",
    "- A sentence with at least 2 words should only be considered, configure this using *min_count* parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = # TODO: Create Word2Vec model using gensim's Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build Vocabulary build_vocab, report its progress after processing every 1000 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(review_text, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output (61501454, 83868975)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find most similar word to 'bad' using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find similarity between 'great' and 'good' using the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output: 0.7848497"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
