{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Bidirectional, TimeDistributed, Dropout, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from seqeval.metrics import classification_report as seqeval_report\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report\n",
    "from tests import test_data, test_model, test_validation_accuracy\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to load the dataset and fill missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filepath):\n",
    "    \"\"\"Load the dataset and fill missing values.\"\"\"\n",
    "    data = pd.read_csv(filepath, encoding=\"latin1\").fillna(method=\"ffill\")\n",
    "    test_data(data)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a class to aggregate sentences from the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter:\n",
    "    def __init__(self, data):\n",
    "        self.sentences = self.aggregate_sentences(data)\n",
    "\n",
    "    def aggregate_sentences(self, data):\n",
    "        agg_func = lambda s: [(w, t) for w, t in zip(s[\"Word\"].values, s[\"Tag\"].values)]\n",
    "        return data.groupby(\"Sentence #\").apply(agg_func).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to create vocabulary and tag mappings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile create_mappings.py\n",
    "\n",
    "def create_mappings(data):\n",
    "    \"\"\"Create mappings for words and tags.\"\"\"\n",
    "    words = list(set(data[\"Word\"].values))\n",
    "    words.append(\"PAD\")\n",
    "    tags = list(set(data[\"Tag\"].values))\n",
    "\n",
    "    word2idx = {w: i for i, w in enumerate(words)}\n",
    "    tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "    idx2tag = {i: t for t, i in tag2idx.items()}\n",
    "\n",
    "    return words, tags, word2idx, tag2idx, idx2tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation\n",
    "Write functions to prepare sequences and pad them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(sentences, word2idx, tag2idx, max_len):\n",
    "    \"\"\"Prepare padded sequences for words and tags.\"\"\"\n",
    "    X = [[word2idx.get(w[0], word2idx[\"PAD\"]) for w in s] for s in sentences]\n",
    "    y = [[tag2idx.get(w[1], tag2idx[\"O\"]) for w in s] for s in sentences]\n",
    "\n",
    "    X = pad_sequences(X, maxlen=max_len, padding=\"post\")\n",
    "    y = pad_sequences(y, maxlen=max_len, padding=\"post\", value=tag2idx[\"O\"])\n",
    "    \n",
    "    y = tf.keras.utils.to_categorical(y, num_classes=len(tag2idx))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Definition\n",
    "Write a function to define the model using the Keras functional API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Model\n",
    "EMBEDDING_DIM = 128\n",
    "LSTM_UNITS = 128\n",
    "DROPOUT_RATE = 0.4\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 0.002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_dim, output_dim, input_length, lstm_units, dropout_rate):\n",
    "    \"\"\"Build the BiLSTM model.\"\"\"\n",
    "    input_layer = Input(shape=(input_length,))\n",
    "    embedding = Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length)(input_layer)\n",
    "    dropout1 = Dropout(dropout_rate)(embedding)\n",
    "\n",
    "    lstm1 = Bidirectional(LSTM(lstm_units, return_sequences=True, dropout=0.3, recurrent_dropout=0.3))(dropout1)\n",
    "    dropout2 = Dropout(dropout_rate)(lstm1)\n",
    "    lstm2 = Bidirectional(LSTM(lstm_units // 2, return_sequences=True, dropout=0.3, recurrent_dropout=0.3))(dropout2)\n",
    "    dropout3 = Dropout(dropout_rate)(lstm2)\n",
    "\n",
    "    output = TimeDistributed(Dense(len(tag2idx), activation=\"softmax\"))(dropout3)\n",
    "\n",
    "    model = Model(input_layer, output)\n",
    "    \n",
    "    test_model(model, input_dim, EMBEDDING_DIM, input_length, LSTM_UNITS, len(tag2idx))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training\n",
    "Write a function to compile, train, and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, batch_size, epochs, learning_rate):\n",
    "    \"\"\"Compile and train the model.\"\"\"\n",
    "    optimizer = Nadam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1, min_lr=1e-5)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_split=0.1,\n",
    "        verbose=1,\n",
    "        callbacks=[lr_scheduler]\n",
    "    )\n",
    "    test_validation_accuracy(history)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Evaluation\n",
    "Write a function to evaluate the model and generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, idx2tag):\n",
    "    \"\"\"Evaluate the model and generate predictions.\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_tags = np.argmax(y_pred, axis=-1)\n",
    "    y_true_tags = np.argmax(y_test, axis=-1)\n",
    "\n",
    "    y_pred_flat = [idx2tag[i] for row in y_pred_tags for i in row]\n",
    "    y_true_flat = [idx2tag[i] for row in y_true_tags for i in row]\n",
    "\n",
    "    report = classification_report(y_true_flat, y_pred_flat)\n",
    "    return report, y_pred_tags, y_true_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display Predictions\n",
    "Write a function to display random sample predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_tags(sentence_idx, y_pred_tags, y_true_tags, idx2tag, X_test, idx2word):\n",
    "    \"\"\"Decode tags for a given sentence index.\"\"\"\n",
    "    words = [idx2word[idx] for idx in X_test[sentence_idx] if idx != word2idx[\"PAD\"]]\n",
    "    true_tags = [idx2tag[idx] for idx in y_true_tags[sentence_idx] if idx != tag2idx[\"O\"]]\n",
    "    pred_tags = [idx2tag[idx] for idx in y_pred_tags[sentence_idx] if idx != tag2idx[\"O\"]]\n",
    "    return words, true_tags, pred_tags\n",
    "\n",
    "\n",
    "def display_sample_predictions(X_test, y_pred_tags, y_true_tags, idx2tag, word2idx):\n",
    "    \"\"\"Display random sample predictions.\"\"\"\n",
    "    idx2word = {i: w for w, i in word2idx.items()}\n",
    "    num_examples = 5\n",
    "    print(\"\\nSample Predictions vs Actual Tags:\\n\")\n",
    "\n",
    "    for _ in range(num_examples):\n",
    "        sentence_idx = random.randint(0, len(X_test) - 1)\n",
    "        words, true_tags, pred_tags = decode_tags(sentence_idx, y_pred_tags, y_true_tags, idx2tag, X_test, idx2word)\n",
    "        print(\"Sentence: \", \" \".join(words))\n",
    "        print(\"Actual Tags: \", \" \".join(true_tags))\n",
    "        print(\"Predicted Tags: \", \" \".join(pred_tags))\n",
    "        print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Function\n",
    "Bring everything together in a main function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape is correct.\n",
      "Model test passed with 4932625 trainable parameters!\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 50)]              0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 50, 128)           4502912   \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 50, 128)           0         \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirecti  (None, 50, 256)           263168    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 50, 256)           0         \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirecti  (None, 50, 128)           164352    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 50, 128)           0         \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDi  (None, 50, 17)            2193      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4932625 (18.82 MB)\n",
      "Trainable params: 4932625 (18.82 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Nadam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Nadam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Nadam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "607/607 [==============================] - 209s 339ms/step - loss: 0.1639 - accuracy: 0.9594 - val_loss: 0.0648 - val_accuracy: 0.9811 - lr: 0.0020\n",
      "Validation accuracy is sufficient: 0.98 (>= 0.25).\n",
      "150/150 [==============================] - 6s 33ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/divyahegde/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/divyahegde/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       B-art       0.00      0.00      0.00        46\n",
      "       B-eve       0.00      0.00      0.00        35\n",
      "       B-geo       0.82      0.88      0.85      3796\n",
      "       B-gpe       0.92      0.91      0.92      1592\n",
      "       B-nat       0.00      0.00      0.00        20\n",
      "       B-org       0.68      0.64      0.66      2053\n",
      "       B-per       0.84      0.68      0.75      1666\n",
      "       B-tim       0.90      0.76      0.83      2032\n",
      "       I-art       0.00      0.00      0.00        40\n",
      "       I-eve       0.00      0.00      0.00        39\n",
      "       I-geo       0.74      0.74      0.74       712\n",
      "       I-gpe       0.00      0.00      0.00        16\n",
      "       I-nat       0.00      0.00      0.00         7\n",
      "       I-org       0.72      0.70      0.71      1697\n",
      "       I-per       0.78      0.88      0.82      1657\n",
      "       I-tim       0.86      0.44      0.58       583\n",
      "           O       0.99      1.00      1.00    223809\n",
      "\n",
      "    accuracy                           0.98    239800\n",
      "   macro avg       0.49      0.45      0.46    239800\n",
      "weighted avg       0.98      0.98      0.98    239800\n",
      "\n",
      "\n",
      "Sample Predictions vs Actual Tags:\n",
      "\n",
      "Sentence:  On Monday , President Bush said Hezbollah , along with Syria and Iran , are responsible for last week 's violence in Lebanon between government supporters and the Hezbollah-led opposition . Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer\n",
      "Actual Tags:  B-tim B-per I-per B-org B-geo B-geo B-geo\n",
      "Predicted Tags:  B-tim B-per I-per B-per B-geo B-geo B-geo\n",
      "------------------------------------------------------------\n",
      "Sentence:  But the minority leader said they will fight the President 's plans on Social Security if he proposes to privatize the system or cut benefits . Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer\n",
      "Actual Tags:  B-per B-org I-org\n",
      "Predicted Tags:  B-per B-org I-org\n",
      "------------------------------------------------------------\n",
      "Sentence:  Earlier Wednesday , several dozen young people forced their way into Kyrgyzstan 's Supreme Court building and evicted supporters of rival political groups who occupied the building since April . Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer\n",
      "Actual Tags:  B-tim B-geo B-org I-org B-tim I-tim\n",
      "Predicted Tags:  B-tim B-gpe B-org I-org B-tim\n",
      "------------------------------------------------------------\n",
      "Sentence:  January 's increase follows on the heels of a 17.7 percent increase in December . Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer\n",
      "Actual Tags:  B-tim B-tim I-tim\n",
      "Predicted Tags:  B-tim B-tim\n",
      "------------------------------------------------------------\n",
      "Sentence:  Eighteen new cases were also discovered on the Baltic island of Ruegen . Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer Downer\n",
      "Actual Tags:  B-geo I-geo I-geo I-geo\n",
      "Predicted Tags:  B-geo\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/divyahegde/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "filepath = \"ner_dataset.csv\"\n",
    "data = load_dataset(filepath)\n",
    "\n",
    "# Prepare Sentences\n",
    "getter = SentenceGetter(data)\n",
    "sentences = getter.sentences\n",
    "\n",
    "# Create Mappings\n",
    "words, tags, word2idx, tag2idx, idx2tag = create_mappings(data)\n",
    "\n",
    "# Prepare Data\n",
    "MAX_LEN = 50\n",
    "X, y = prepare_sequences(sentences, word2idx, tag2idx, MAX_LEN)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "model = build_model(len(words), EMBEDDING_DIM, MAX_LEN, LSTM_UNITS, DROPOUT_RATE)\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# Train Model\n",
    "train_model(model, X_train, y_train, BATCH_SIZE, EPOCHS, LEARNING_RATE)\n",
    "\n",
    "# Evaluate Model\n",
    "report, y_pred_tags, y_true_tags = evaluate_model(model, X_test, y_test, idx2tag)\n",
    "print(\"Classification Report:\\n\", report)\n",
    "\n",
    "# Display Sample Predictions\n",
    "display_sample_predictions(X_test, y_pred_tags, y_true_tags, idx2tag, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
