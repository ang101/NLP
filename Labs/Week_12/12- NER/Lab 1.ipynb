{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition using LSTM\n",
    "\n",
    "In this lab, we will look at building a NER model. \n",
    "\n",
    "Named Entity Recognition (NER) is a subtask of Natural Language Processing (NLP) that focuses on identifying and classifying entities within a text into predefined categories. These entities include names of persons, organizations, locations, dates, numerical expressions, products, and other significant terms.\n",
    "\n",
    "## How NER Works:\n",
    "\n",
    "- Tokenization: Splitting the text into smaller units (tokens) such as words or phrases.\n",
    "- Entity Detection: Identifying segments of text that potentially represent entities.\n",
    "- Entity Classification: Assigning each identified segment to a category, e.g., “John Smith” → Person, “New York” → Location.\n",
    "\n",
    "## Applications of NER:\n",
    "\n",
    "- Information Extraction: Pulling key information from documents or web pages.\n",
    "- Search Optimization: Improving search engines by tagging entities for more relevant results.\n",
    "- Customer Support Automation: Recognizing names, locations, or products to automate queries.\n",
    "- Healthcare: Extracting disease names, medications, and patient data from medical records.\n",
    "- Finance: Identifying companies, dates, and financial figures in reports.\n",
    "\n",
    "**Example**:\n",
    "For the sentence:\n",
    "**\"Apple Inc. was founded in Cupertino by Steve Jobs in 1976.\"**\n",
    "NER identifies:\n",
    "\n",
    "Organization: Apple Inc.\n",
    "\n",
    "Location: Cupertino\n",
    "\n",
    "\n",
    "Person: Steve Jobs\n",
    "\n",
    "Date: 1976"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seqeval in /Users/divyahegde/anaconda3/lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /Users/divyahegde/anaconda3/lib/python3.10/site-packages (from seqeval) (1.23.5)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /Users/divyahegde/anaconda3/lib/python3.10/site-packages (from seqeval) (1.2.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/divyahegde/anaconda3/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.10.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/divyahegde/anaconda3/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/divyahegde/anaconda3/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install seqeval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Bidirectional, TimeDistributed, Dropout, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from seqeval.metrics import classification_report as seqeval_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "data = pd.read_csv(\"ner_dataset.csv\", encoding=\"latin1\").fillna(method=\"ffill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SentenceGetter is designed to aggregate words and their tags into sentences from a structured pandas DataFrame. It's typically used in Natural Language Processing (NLP) tasks, such as Named Entity Recognition (NER), where words and their corresponding labels (tags) need to be grouped by sentence.\n",
    "\n",
    "It produces output like so:\n",
    "[\n",
    "    [('John', 'PERSON'), ('lives', 'O'), ('in', 'O'), ('Paris', 'LOCATION')],\n",
    "    [('He', 'O'), ('works', 'O'), ('at', 'O'), ('Google', 'ORGANIZATION')]\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Aggregator\n",
    "class SentenceGetter:\n",
    "    def __init__(self, data):\n",
    "        self.sentences = self.aggregate_sentences(data)\n",
    "\n",
    "    def aggregate_sentences(self, data):\n",
    "        agg_func = lambda s: [(w, t) for w, t in zip(s[\"Word\"].values, s[\"Tag\"].values)]\n",
    "        sentences = data.groupby(\"Sentence #\").apply(agg_func).tolist()\n",
    "        return [s for s in sentences if len(s) > 0]  # Exclude empty sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter = SentenceGetter(data)\n",
    "sentences = getter.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Thousands', 'O'), ('of', 'O'), ('demonstrators', 'O'), ('have', 'O'), ('marched', 'O'), ('through', 'O'), ('London', 'B-geo'), ('to', 'O'), ('protest', 'O'), ('the', 'O'), ('war', 'O'), ('in', 'O'), ('Iraq', 'B-geo'), ('and', 'O'), ('demand', 'O'), ('the', 'O'), ('withdrawal', 'O'), ('of', 'O'), ('British', 'B-gpe'), ('troops', 'O'), ('from', 'O'), ('that', 'O'), ('country', 'O'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "## Printing first sentence and corresponding tags\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code prepares vocabulary and tag mappings for a Named Entity Recognition (NER) task. It creates three main mappings to convert words and tags into numerical indices and vice versa, which is essential for training machine learning models.\n",
    "\n",
    "Extract Unique Words and Tags:\n",
    "\n",
    "Collects all unique words and tags from the data.\n",
    "Adds a special padding token \"PAD\" to handle sentence length variations.\n",
    "Generate Mappings:\n",
    "\n",
    "word2idx: Maps each unique word to a unique index.\n",
    "\n",
    "tag2idx: Maps each unique tag (e.g., PERSON, LOCATION) to a unique index.\n",
    "\n",
    "idx2tag: A reverse mapping to retrieve the tags from their indices.\n",
    "\n",
    "Example:\n",
    "\n",
    "words = ['John', 'lives', 'in', 'Paris', 'PAD']\n",
    "tags = ['O', 'PERSON', 'LOCATION']\n",
    "\n",
    "End result is:\n",
    "{0: 'O', 1: 'PERSON', 2: 'LOCATION'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary and Tag Mappings\n",
    "words = list(set(data[\"Word\"].values))\n",
    "words.append(\"PAD\")\n",
    "tags = list(set(data[\"Tag\"].values))\n",
    "word2idx = {w: i for i, w in enumerate(words)}\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "idx2tag = {i: t for t, i in tag2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MAX_LEN = 50\n",
    "EMBEDDING_DIM = 100\n",
    "LSTM_UNITS = 64\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code prepares the input sequences (X) and the output label sequences (y) for a Named Entity Recognition (NER) task. It transforms textual data into numerical form, pads the sequences to a uniform length, and converts the labels into a one-hot encoded format to make them suitable for training a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Sequences\n",
    "X = [[word2idx.get(w[0], word2idx[\"PAD\"]) for w in s] for s in sentences]\n",
    "y = [[tag2idx.get(w[1], tag2idx[\"O\"]) for w in s] for s in sentences]\n",
    "X = pad_sequences(X, maxlen=MAX_LEN, padding=\"post\")\n",
    "y = pad_sequences(y, maxlen=MAX_LEN, padding=\"post\", value=tag2idx[\"O\"])\n",
    "\n",
    "# Convert Labels to One-Hot Encoding\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=len(tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Handle Class Imbalance\n",
    "flat_y_train = np.argmax(y_train, axis=-1).flatten()\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(flat_y_train), y=flat_y_train)\n",
    "class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 50)]              0         \n",
      "                                                                 \n",
      " embedding_7 (Embedding)     (None, 50, 100)           3517900   \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 50, 100)           0         \n",
      "                                                                 \n",
      " bidirectional_7 (Bidirecti  (None, 50, 128)           84480     \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 50, 128)           0         \n",
      "                                                                 \n",
      " time_distributed_7 (TimeDi  (None, 50, 17)            2193      \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3604573 (13.75 MB)\n",
      "Trainable params: 3604573 (13.75 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "1214/1214 [==============================] - 78s 63ms/step - loss: 0.1862 - accuracy: 0.9571 - val_loss: 0.0702 - val_accuracy: 0.9796\n",
      "150/150 [==============================] - 1s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define Model\n",
    "input_layer = Input(shape=(MAX_LEN,))\n",
    "embedding = Embedding(input_dim=len(words), output_dim=EMBEDDING_DIM, input_length=MAX_LEN)(input_layer)\n",
    "dropout1 = Dropout(0.3)(embedding)\n",
    "lstm1 = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True, dropout=0.3, recurrent_dropout=0.3))(dropout1)\n",
    "dropout2 = Dropout(0.3)(lstm1)\n",
    "output = TimeDistributed(Dense(len(tags), activation=\"softmax\"))(dropout2)\n",
    "\n",
    "model = Model(input_layer, output)\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=0.1, verbose=1)\n",
    "\n",
    "\n",
    "# Predict on Test Data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_tags = np.argmax(y_pred, axis=-1)\n",
    "y_true_tags = np.argmax(y_test, axis=-1)\n",
    "\n",
    "# Flatten Predictions and Ground Truth for Evaluation\n",
    "y_pred_flat = [[idx2tag[i] for i in row] for row in y_pred_tags]\n",
    "y_true_flat = [[idx2tag[i] for i in row] for row in y_true_tags]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Predictions vs Actual Tags:\n",
      "\n",
      "Sentence:  The report calls on President Bush and Congress to urge Chinese officials not to use the global war against terrorism as a pretext to suppress minorities ' rights . Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya\n",
      "Actual Tags:  B-per I-per B-org B-gpe\n",
      "Predicted Tags:  B-per I-per B-org B-gpe\n",
      "------------------------------------------------------------\n",
      "Sentence:  The construction on the Baku-T'bilisi-Ceyhan oil pipeline , the Baku-T'bilisi-Erzerum gas pipeline , and the Kars-Akhalkalaki Railroad are part of a strategy to capitalize on Georgia 's strategic location between Europe and Asia and develop its role as a transit point for gas , oil and other goods . Moya\n",
      "Actual Tags:  B-org I-org B-geo B-geo B-geo\n",
      "Predicted Tags:  I-org B-geo B-geo B-geo\n",
      "------------------------------------------------------------\n",
      "Sentence:  The pact was initially approved after discussions between President Bush and Peruvian President Alan Garcia , but Democrats in Congress forced U.S. officials to reopen negotiations and add stronger labor and environmental provisions . Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya\n",
      "Actual Tags:  B-per I-per B-gpe B-per I-per I-per B-org B-org B-geo\n",
      "Predicted Tags:  B-per I-per B-gpe B-per I-per I-per B-org B-geo\n",
      "------------------------------------------------------------\n",
      "Sentence:  Zelenovic had lived in Khanty-Mansiisk , some 2,000 kilometers east of Moscow , for several years under an assumed name and had worked in the construction industry . Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya\n",
      "Actual Tags:  B-per B-geo B-geo B-tim\n",
      "Predicted Tags:  B-geo\n",
      "------------------------------------------------------------\n",
      "Sentence:  Exports have grown significantly because of the trade benefits contained in the Africa Growth and Opportunity Act . Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya Moya\n",
      "Actual Tags:  B-geo I-geo B-geo I-geo\n",
      "Predicted Tags:  B-geo I-geo\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Show Sample Predictions\n",
    "print(\"\\nSample Predictions vs Actual Tags:\\n\")\n",
    "for i in range(5):  # Show 5 sample sentences\n",
    "    words_example = [words[idx] for idx in X_test[i] if idx != word2idx[\"PAD\"]]\n",
    "    true_tags_example = [tag for tag in y_true_flat[i] if tag != \"O\"]\n",
    "    pred_tags_example = [tag for tag in y_pred_flat[i] if tag != \"O\"]\n",
    "    \n",
    "    print(\"Sentence: \", \" \".join(words_example))\n",
    "    print(\"Actual Tags: \", \" \".join(true_tags_example))\n",
    "    print(\"Predicted Tags: \", \" \".join(pred_tags_example))\n",
    "    print(\"-\" * 60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
