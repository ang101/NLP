{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine translation using Seq2seq model\n",
    "\n",
    "A **Seq2Seq** (Sequence-to-Sequence) model is a type of neural network architecture used for tasks where the input and output are both sequences, but the length of the input sequence can be different from the output sequence. It's particularly useful in problems like:\n",
    "\n",
    "- **Machine Translation** (translating a sentence from one language to another)\n",
    "- **Text Summarization** (converting a long article into a shorter summary)\n",
    "- **Speech Recognition** (transcribing spoken words into text)\n",
    "- **Question Answering** (generating an answer based on a question)\n",
    "- **Image Captioning** (generating a description of an image)\n",
    "\n",
    "### Key Components of a Seq2Seq Model:\n",
    "\n",
    "1. **Encoder**: \n",
    "   - The encoder processes the input sequence and converts it into a fixed-length context vector (also called the \"thought vector\" or \"hidden state\"). This part of the model can be a Recurrent Neural Network (RNN), Long Short-Term Memory (LSTM), or Gated Recurrent Unit (GRU), which are designed to handle sequences.\n",
    "   - The encoder processes the input sequence step by step, updating its internal state to represent the information from the sequence.\n",
    "\n",
    "2. **Decoder**: \n",
    "   - The decoder generates the output sequence from the context vector produced by the encoder. The decoder is also typically an RNN, LSTM, or GRU.\n",
    "   - The decoder starts with the context vector as its initial hidden state and generates one token (word or character) at a time.\n",
    "   - During training, the decoder is provided the correct output token at each step (this is called **teacher forcing**). During inference (prediction), the decoder uses its own previous output as input for the next step.\n",
    "\n",
    "### General Workflow:\n",
    "1. The input sequence is fed into the **encoder**.\n",
    "2. The encoder generates a context vector (a compressed representation of the entire input sequence).\n",
    "3. The context vector is passed to the **decoder**.\n",
    "4. The decoder generates the output sequence, one token at a time.\n",
    "\n",
    "### Seq2Seq Example:\n",
    "For example, in **machine translation**:\n",
    "- **Input**: \"How are you?\"\n",
    "- **Output**: \"¿Cómo estás?\"\n",
    "\n",
    "The encoder processes \"How are you?\" and outputs a context vector. The decoder then uses this context vector to generate the translation word by word: \"¿\", \"Cómo\", \"estás\", \"?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = [\n",
    "    \"hello\", \"how are you\", \"good morning\", \"what is your name\", \"thank you\",\n",
    "    \"good night\", \"see you later\", \"have a good day\", \"what time is it\", \"where are you from\",\n",
    "    \"I am fine\", \"I love you\", \"please help me\", \"excuse me\", \"I'm sorry\",\n",
    "    \"it's okay\", \"can you speak English\", \"how much does it cost\", \"I don't understand\", \"what is this\"\n",
    "]\n",
    "\n",
    "target_texts = [\n",
    "    \"hola\", \"como estas\", \"buenos dias\", \"cual es tu nombre\", \"gracias\",\n",
    "    \"buenas noches\", \"hasta luego\", \"que tengas un buen dia\", \"que hora es\", \"de donde eres\",\n",
    "    \"estoy bien\", \"te quiero\", \"por favor ayudame\", \"perdon\", \"lo siento\",\n",
    "    \"esta bien\", \"puedes hablar ingles\", \"cuanto cuesta\", \"no entiendo\", \"que es esto\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize input and target texts\n",
    "input_tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "input_tokenizer.fit_on_texts(input_texts)\n",
    "input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n",
    "input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_sequences, padding='post')\n",
    "\n",
    "output_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "output_tokenizer.fit_on_texts(target_texts)\n",
    "output_sequences = output_tokenizer.texts_to_sequences(target_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add start and end tokens\n",
    "\n",
    "The ` <start> ` and ` <end> ` tokens are critical in Seq2Seq models for several reasons:\n",
    "\n",
    "Start token tells the decoder when to begin generating the output sequence.\n",
    "\n",
    "End token signals when the sequence generation is complete, preventing the model from generating endless sequences.\n",
    "\n",
    "They help the model learn the structure of sequences by providing clear boundaries.\n",
    "\n",
    "They improve learning during training and are essential for handling variable-length sequences.\n",
    "\n",
    "They guide the generation process during inference, enabling the model to stop when appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step to add <start> and <end> tokens to the target sequences\n",
    "def add_start_end_tokens_manually(tokenizer, sequences):\n",
    "    if '<start>' not in tokenizer.word_index:\n",
    "        tokenizer.word_index['<start>'] = len(tokenizer.word_index) + 1\n",
    "    if '<end>' not in tokenizer.word_index:\n",
    "        tokenizer.word_index['<end>'] = len(tokenizer.word_index) + 1\n",
    "\n",
    "    updated_sequences = []\n",
    "    for seq in sequences:\n",
    "        updated_seq = [tokenizer.word_index['<start>']] + seq + [tokenizer.word_index['<end>']]\n",
    "        updated_sequences.append(updated_seq)\n",
    "    return tf.keras.preprocessing.sequence.pad_sequences(updated_sequences, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add <start> and <end> tokens\n",
    "output_sequences = add_start_end_tokens_manually(output_tokenizer, output_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary sizes\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "output_vocab_size = len(output_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model parameters\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "batch_size = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder model\n",
    "\n",
    "The **Encoder** model in a Seq2Seq architecture processes an input sequence and transforms it into a context vector that summarizes the sequence. It consists of an **embedding layer** that converts input tokens into dense vectors, followed by a **GRU layer** that captures the temporal dependencies of the sequence. The model returns both the sequence of hidden states (useful for attention mechanisms) and the final hidden state (used as the context for the decoder). This encoded information helps the decoder generate the output sequence.\n",
    "\n",
    "For example, if the input is a sequence of word indices like [1, 2, 3, 4]:\n",
    "\n",
    "The embedding layer will convert these indices into dense vectors (e.g., of size embedding_dim).\n",
    "The GRU layer will process the sequence of embedded vectors, and the output will be a sequence of hidden states, and the final hidden state will capture the context of the entire sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = GRU(enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "In traditional Seq2Seq models, the encoder produces a single fixed-size context vector, which can sometimes fail to capture all the important information, especially in long sequences. To address this, **attention mechanisms** were introduced. Attention allows the decoder to focus on different parts of the input sequence at each decoding step, improving the model’s performance and handling long sequences better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Model\n",
    "\n",
    "The Decoder class defines the decoder part of a Seq2Seq model with attention. It receives the context vector from the encoder, the current hidden state, and the current token as input. \n",
    "\n",
    "It applies the Bahdanau attention mechanism to focus on important parts of the encoder's output, processes the input through an embedding layer and GRU, and produces a set of logits for the next token. \n",
    "\n",
    "It also returns the updated hidden state and attention weights, which are used in subsequent decoding steps and for interpreting which parts of the input sequence were most important at each timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = GRU(dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "        self.fc = Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(dec_units)\n",
    "\n",
    "    def call(self, x, enc_output, hidden):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        output, state = self.gru(x)\n",
    "        x = self.fc(output)\n",
    "        x = tf.squeeze(x, axis=1)\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the models\n",
    "encoder = Encoder(input_vocab_size, embedding_dim, units)\n",
    "decoder = Decoder(output_vocab_size, embedding_dim, units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(input_seq, target_seq, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(input_seq)\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([output_tokenizer.word_index['<start>']] * batch_size, 1)\n",
    "\n",
    "        for t in range(1, target_seq.shape[1]):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, enc_output, dec_hidden)\n",
    "            loss += loss_function(target_seq[:, t], predictions)\n",
    "\n",
    "            dec_input = tf.expand_dims(target_seq[:, t], 1)\n",
    "\n",
    "    batch_loss = loss / int(target_seq.shape[1])\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 17.512834548950195\n",
      "Epoch 2, Loss: 15.508331298828125\n",
      "Epoch 3, Loss: 14.64116382598877\n",
      "Epoch 4, Loss: 13.69356632232666\n",
      "Epoch 5, Loss: 12.304492950439453\n",
      "Epoch 6, Loss: 10.427021980285645\n",
      "Epoch 7, Loss: 8.448477745056152\n",
      "Epoch 8, Loss: 6.822537422180176\n",
      "Epoch 9, Loss: 5.6506428718566895\n",
      "Epoch 10, Loss: 4.793063163757324\n"
     ]
    }
   ],
   "source": [
    "# Example training call\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    enc_hidden = tf.zeros((batch_size, units))\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in range(len(input_sequences) // batch_size):\n",
    "        batch_input = input_sequences[batch * batch_size:(batch + 1) * batch_size]\n",
    "        batch_target = output_sequences[batch * batch_size:(batch + 1) * batch_size]\n",
    "\n",
    "        batch_loss = train_step(batch_input, batch_target, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss.numpy()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    # Preprocess the input sentence\n",
    "    sentence_seq = input_tokenizer.texts_to_sequences([sentence])\n",
    "    sentence_seq = tf.keras.preprocessing.sequence.pad_sequences(sentence_seq, maxlen=input_sequences.shape[1], padding='post')\n",
    "\n",
    "    enc_hidden = tf.zeros((1, units))\n",
    "    enc_output, enc_hidden = encoder(sentence_seq)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([output_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for t in range(output_sequences.shape[1]):\n",
    "        # Get the decoder's prediction\n",
    "        predictions, dec_hidden, _ = decoder(dec_input, enc_output, dec_hidden)\n",
    "\n",
    "        # Use greedy decoding: pick the token with the highest probability\n",
    "        predicted_id = tf.argmax(predictions[0], axis=-1).numpy()\n",
    "\n",
    "        # Get the predicted word from the tokenizer\n",
    "        predicted_word = output_tokenizer.index_word.get(predicted_id, None)\n",
    "\n",
    "        # If no valid predicted word, break the loop\n",
    "        if predicted_word is None:\n",
    "            #print(f\"Unknown token ID {predicted_id} detected.\")\n",
    "            break\n",
    "\n",
    "        # If we reach the <end> token, stop\n",
    "        if predicted_word == '<end>':\n",
    "            break\n",
    "\n",
    "        # Append the predicted word to the result\n",
    "        result.append(predicted_word)\n",
    "\n",
    "        # Use the predicted word as the next input to the decoder\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return ' '.join(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: hello\n",
      "Translated: perdon\n",
      "------------------------------\n",
      "Input: how are you\n",
      "Translated: de donde eres\n",
      "------------------------------\n",
      "Input: good morning\n",
      "Translated: buenos dias\n",
      "------------------------------\n",
      "Input: what is your name\n",
      "Translated: cual es tu nombre\n",
      "------------------------------\n",
      "Input: thank you\n",
      "Translated: gracias\n",
      "------------------------------\n",
      "Input: good night\n",
      "Translated: buenas noches\n",
      "------------------------------\n",
      "Input: see you later\n",
      "Translated: hasta luego\n",
      "------------------------------\n",
      "Input: have a good day\n",
      "Translated: que tengas un buen dia\n",
      "------------------------------\n",
      "Input: what time is it\n",
      "Translated: que hora es\n",
      "------------------------------\n",
      "Input: where are you from\n",
      "Translated: de donde eres\n",
      "------------------------------\n",
      "Input: I am fine\n",
      "Translated: estoy bien\n",
      "------------------------------\n",
      "Input: I love you\n",
      "Translated: te quiero\n",
      "------------------------------\n",
      "Input: please help me\n",
      "Translated: por favor ayudame\n",
      "------------------------------\n",
      "Input: excuse me\n",
      "Translated: perdon\n",
      "------------------------------\n",
      "Input: I'm sorry\n",
      "Translated: lo siento\n",
      "------------------------------\n",
      "Input: it's okay\n",
      "Translated: perdon\n",
      "------------------------------\n",
      "Input: can you speak English\n",
      "Translated: puedes hablar ingles\n",
      "------------------------------\n",
      "Input: how much does it cost\n",
      "Translated: cuanto cuesta\n",
      "------------------------------\n",
      "Input: I don't understand\n",
      "Translated: estoy bien\n",
      "------------------------------\n",
      "Input: what is this\n",
      "Translated: que es\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Example translations\n",
    "for text in input_texts:\n",
    "    print(f\"Input: {text}\")\n",
    "    print(f\"Translated: {translate(text)}\")\n",
    "    print(\"-\" * 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
