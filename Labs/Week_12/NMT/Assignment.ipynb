{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation using Encoder-Decoder model\n",
    "\n",
    "The assignment is divided like below:\n",
    "Neural Machine Translation (NMT) using an encoder-decoder model is a deep learning-based approach that efficiently translates text from one language to another, such as Spanish to English. Here's a brief overview of how it works:\n",
    "\n",
    "1. **Encoder:**\n",
    "   - The encoder processes the input Spanish sentence and converts it into a sequence of fixed-length numerical representations (called embeddings).\n",
    "   - It uses recurrent layers like LSTMs or GRUs (or transformers in modern implementations) to capture the semantic and syntactic information of the sentence. \n",
    "   - The output is a context vector (or a series of vectors in attention-based models) summarizing the meaning of the entire Spanish sentence.\n",
    "\n",
    "2. **Decoder:**\n",
    "   - The decoder takes the context vector from the encoder and generates the English translation one word at a time.\n",
    "   - At each step, the decoder predicts the next word in the sequence, based on the context vector and the previously generated words.\n",
    "\n",
    "3. **Attention Mechanism (optional but commonly used):**\n",
    "   - Attention allows the decoder to focus on relevant parts of the input sentence while translating, rather than relying solely on the fixed context vector. This is particularly useful for longer sentences.\n",
    "\n",
    "4. **Training:**\n",
    "   - The model is trained on a large parallel corpus of Spanish-English sentence pairs.\n",
    "   - It minimizes a loss function (e.g., cross-entropy loss) to align the predicted English words with the actual translation.\n",
    "\n",
    "5. **Inference:**\n",
    "   - During translation, the decoder uses techniques like beam search to produce fluent and contextually accurate English sentences.\n",
    "\n",
    "The encoder-decoder architecture enables the model to capture complex language relationships and produce high-quality translations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions\n",
    "Do not modify any of the codes.\n",
    "Only write code when prompted. For example in some sections you will find the following,\n",
    "\n",
    "```python\n",
    "# YOUR CODE GOES HERE\n",
    "# YOUR CODE ENDS HERE\n",
    "# TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pathlib\n",
    "import re\n",
    "import random\n",
    "from helper import create_dataset, tokenize, preprocess_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and tokenization\n",
    "\n",
    "Here we are using 'spa.txt' file which has Spanish sentences and its corresponding English translations. \n",
    "\n",
    "We are also preprocessing the sentences to lower the cases, trimming whitespace, removing non-alphabetic characters.\n",
    "\n",
    "We are also looking at wrapping the sentence with <start> and <end> tokens for sequence modeling\n",
    "\n",
    "The create dataset function loads the dataset, preprocesses each sentence pair, and returns separate lists for source and target languages.\n",
    "Tokinzation function tokenizes a list of sentences and converts them into padded numerical sequences suitable for neural network inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = 'spa.txt'\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "num_examples = 50000  # Set the number of examples to load\n",
    "source_lang, target_lang = create_dataset(path_to_file, num_examples)\n",
    "\n",
    "# Tokenize the source and target languages\n",
    "input_tensor, input_tokenizer = tokenize(source_lang)\n",
    "target_tensor, target_tokenizer = tokenize(target_lang)\n",
    "\n",
    "# Vocabulary sizes\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "target_vocab_size = len(target_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder class\n",
    "The below codeblock defines the encoder for the sequence-to-sequence model. It uses an embedding layer and GRU for encoding the input sentence into context vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = # TODO: Set the Batch Size\n",
    "        self.enc_units = # TODO: Set the Number of Encoder Units \n",
    "        self.embedding = # TODO: Initialize the Embedding Layer\n",
    "        self.gru = # TODO: Initialize the GRU Layer\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bahdanau Attention\n",
    "\n",
    "The below code defines the attention mechanism for the model. It calculates a context vector by weighting encoder outputs based on relevance to the decoder's current state. You can learn more about this attention mechanism [here](https://machinelearningmastery.com/the-bahdanau-attention-mechanism/) if interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Mechanism\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder class\n",
    "\n",
    "Defines the decoder for the sequence-to-sequence model. It uses the attention mechanism, GRU, and a dense layer to generate output sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder with Teacher Forcing\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = # TODO: Set the Batch Size\n",
    "        self.dec_units = # TODO: Set the Decoder Units\n",
    "        self.embedding = # TODO: Initialize the Embedding Laye\n",
    "        self.gru = # TODO: Initialize the GRU Layer\n",
    "        self.fc = # TODO: Initialize the Fully Connected (Dense) Layer\n",
    "        self.attention = # TODO: Initialize the Attention Mechanism\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        output, state = self.gru(x)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "In the below code, set training parameters, including batch size, embedding dimension, and GRU unit size.\n",
    "We are creating a TensorFlow dataset, shuffling it, and making batches of it for training.\n",
    "Later, we are initializing encoder and decoder models with defined parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "BUFFER_SIZE = len(input_tensor)\n",
    "BATCH_SIZE = #TODO\n",
    "steps_per_epoch = len(input_tensor) // BATCH_SIZE\n",
    "embedding_dim = #TODO\n",
    "units = #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor, target_tensor))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize encoder and decoder\n",
    "encoder = # TODO: Initialize Encoder\n",
    "decoder = # TODO: Initialize Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and loss function\n",
    "optimizer = # TODO: Adam optimizer initialization\n",
    "loss_object = # TODO: SparseCategoricalCrossentropy calculated from logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are implementing a single training step with teacher forcing to improve sequence generation.\n",
    "\n",
    "Teacher forcing is a training strategy used in sequence-to-sequence models (such as in machine translation or text generation) where the ground truth (actual target sequence) is used as the next input to the decoder during training, instead of using the decoder's own predicted output from the previous step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training step with teacher forcing\n",
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([target_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)  # Teacher forcing\n",
    "\n",
    "    batch_loss = loss / int(targ.shape[1])\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop with multiple epochs\n",
    "EPOCHS = # TODO: Assign number of epochs\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = # TODO: call training step\n",
    "        total_loss += batch_loss\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Loss {total_loss / steps_per_epoch:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    # Preprocess the input sentence\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = [input_tokenizer.word_index.get(word, 0) for word in sentence.split()]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=input_tensor.shape[1], padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    # Encode the input sentence\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([target_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    # Decode the output sequence\n",
    "    for t in range(target_tensor.shape[1]):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        if target_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            break\n",
    "\n",
    "        result += target_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        # Use the predicted word as the next input\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To test on 20 random sentences from the dataset\n",
    "def translate_random_sentences():\n",
    "    random_indices = # TODO: Sample random 20 sentences from the dataset\n",
    "    \n",
    "\n",
    "    for i in random_indices:\n",
    "        source_sentence = source_lang[i]\n",
    "        target_sentence = target_lang[i]  # Actual translation (target language sentence)\n",
    "        \n",
    "        print(f\"Translating sentence {i+1}: {source_sentence}\")\n",
    "        print(f\"Actual translation: {target_sentence}\")\n",
    "        \n",
    "        predicted_translation = # TODO: Evaluate each sentence\n",
    "        print(f\"Predicted translation: {predicted_translation}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Call the function to translate random sentences\n",
    "translate_random_sentences()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
