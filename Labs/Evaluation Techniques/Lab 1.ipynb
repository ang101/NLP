{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Perplexity in Language Models\n",
    "\n",
    "To understand and compute perplexity, a key evaluation metric for language models, and analyze how it reflects the quality of text predictions.\n",
    "\n",
    "**What is Perplexity?**\n",
    "Perplexity measures the uncertainty of a language model in predicting a sequence of words. It indicates how \"perplexed\" the model is by the text.\n",
    "\n",
    "- Low perplexity: The model predicts the sequence with high confidence.\n",
    "- High perplexity: The model struggles to predict the sequence, indicating poor performance.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Perplexity Comparison ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Text: The quick brown fox jumps over the lazy dog.\n",
      "Tokenized Input: [[  464  2068  7586 21831 18045   625   262 16931  3290    13]]\n",
      "Loss: [5.0905147]\n",
      "Perplexity: [162.47346]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Text: Quick the brown fox over lazy jumps dog the.\n",
      "Tokenized Input: [[21063   262  7586 21831   625 16931 18045  3290   262    13]]\n",
      "Loss: [8.565364]\n",
      "Perplexity: [5246.7485]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Text: Lorem ipsum dolor sit amet, consectetur adipiscing elit.\n",
      "Tokenized Input: [[   43 29625   220  2419   388   288 45621  1650   716   316    11   369\n",
      "   8831   316   333 31659   271  2259  1288   270    13]]\n",
      "Loss: [0.9613916]\n",
      "Perplexity: [2.6153336]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Text: Random gibberish xzq mfnweor pasd.\n",
      "Tokenized Input: [[29531 46795   527   680  2124    89    80   285 22184   732   273 38836\n",
      "     67    13]]\n",
      "Loss: [6.982953]\n",
      "Perplexity: [1078.0974]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def calculate_perplexity(text, model_name='gpt2'):\n",
    "    \"\"\"\n",
    "    Calculates the perplexity of the given text using a GPT-2 model in TensorFlow.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "        model_name (str): Name of the Hugging Face model (default: 'gpt2').\n",
    "    \n",
    "    Returns:\n",
    "        float: Perplexity score.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = TFAutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    tokens = tokenizer.encode(text, return_tensors='tf')\n",
    "\n",
    "    # Log the tokenized input\n",
    "    print(f\"\\nOriginal Text: {text}\")\n",
    "    print(f\"Tokenized Input: {tokens}\")\n",
    "\n",
    "    # Calculate loss and perplexity\n",
    "    outputs = model(tokens, labels=tokens)\n",
    "    loss = outputs.loss\n",
    "    perplexity = tf.exp(loss)\n",
    "\n",
    "    print(f\"Loss: {loss.numpy()}\")\n",
    "    return perplexity.numpy()\n",
    "\n",
    "# Compare perplexity of different examples\n",
    "texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",  # Grammatically correct and meaningful\n",
    "    \"Quick the brown fox over lazy jumps dog the.\",  # Grammatically incorrect and jumbled\n",
    "    \"Lorem ipsum dolor sit amet, consectetur adipiscing elit.\",  # Latin placeholder text\n",
    "    \"Random gibberish xzq mfnweor pasd.\"  # Completely random gibberish\n",
    "]\n",
    "\n",
    "print(\"\\n--- Perplexity Comparison ---\")\n",
    "for text in texts:\n",
    "    perplexity = calculate_perplexity(text)\n",
    "    print(f\"Perplexity: {perplexity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower perplexity for natural text (first example) indicates the model is confident in predicting the sequence.\n",
    "Higher perplexity for random gibberish reflects the model's struggle to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-datasets in /Users/divyahegde/anaconda3/lib/python3.10/site-packages (4.9.7)\n",
      "Requirement already satisfied: absl-py in /Users/divyahegde/anaconda3/lib/python3.10/site-packages (from tensorflow-datasets) (1.4.0)\n",
      "Requirement already satisfied: click in /Users/divyahegde/anaconda3/lib/python3.10/site-packages (from tensorflow-datasets) (8.1.7)\n",
      "Requirement already satisfied: dm-tree in /Users/divyahegde/anaconda3/lib/python3.10/site-packages (from tensorflow-datasets) (0.1.8)\n",
      "Requirement already satisfied: immutabledict in /Users/divyahegde/anaconda3/lib/python3.10/site-packages (from tensorflow-datasets) (4.2.1)\n",
      "Requirement already satisfied: numpy in /Users/divyahegde/anaconda3/lib/python3.10/site-packages (from tensorflow-datasets) (1.23.5)\n",
      "Requirement already satisfied: promise in /Users/divyahegde/anaconda3/lib/python3.10/site-packages (from tensorflow-datasets) (2.3)\n",
      "Requirement already satisfied: protobuf>=3.20 in /Users/divyahegde/anaconda3/lib/python3.10/site-packages (from tensorflow-datasets) (3.20.3)\n",
      "Requirement already satisfied: psutil in /Users/divyahegde/anaconda3/lib/python3.10/site-packages (from tensorflow-datasets) (5.9.0)\n",
      "Requirement already satisfied: pyarrow in /Users/divyahegde/anaconda3/lib/python3.10/site-packages (from tensorflow-datasets) (16.0.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/divyahegde/anaconda3/lib/python3.10/site-packages (from tensorflow-datasets) (2.32.3)\n",
      "Requirement already satisfied: simple-parsing in /Users/divyahegde/anaconda3/lib/python3.10/site-packages (from tensorflow-datasets) (0.1.6)\n",
      "Requirement already satisfied: tensorflow-metadata in /Users/divyahegde/anaconda3/lib/python3.10/site-packages (from tensorflow-datasets) (1.16.1)\n",
      "Requirement already satisfied: termcolor in /Users/divyahegde/anaconda3/lib/python3.10/site-packages (from tensorflow-datasets) (2.3.0)\n",
      "Requirement already satisfied: toml in /Users/divyahegde/anaconda3/lib/python3.10/site-packages (from tensorflow-datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in /Users/divyahegde/anaconda3/lib/python3.10/site-packages (from tensorflow-datasets) (4.66.4)\n",
      "Requirement already satisfied: wrapt in /Users/divyahegde/anaconda3/lib/python3.10/site-packages (from tensorflow-datasets) (1.14.1)\n",
      "Requirement already satisfied: etils>=1.6.0 in /Users/divyahegde/anaconda3/lib/python3.10/site-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (1.10.0)\n",
      "Requirement already satisfied: typing_extensions in /Users/divyahegde/anaconda3/lib/python3.10/site-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (4.9.0)\n",
      "Requirement already satisfied: fsspec in /Users/divyahegde/anaconda3/lib/python3.10/site-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (2024.5.0)\n",
      "Requirement already satisfied: importlib_resources in /Users/divyahegde/anaconda3/lib/python3.10/site-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (6.4.5)\n",
      "Requirement already satisfied: zipp in /Users/divyahegde/anaconda3/lib/python3.10/site-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets) (3.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/divyahegde/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/divyahegde/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/divyahegde/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/divyahegde/anaconda3/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets) (2023.7.22)\n",
      "Requirement already satisfied: six in /Users/divyahegde/anaconda3/lib/python3.10/site-packages (from promise->tensorflow-datasets) (1.16.0)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /Users/divyahegde/anaconda3/lib/python3.10/site-packages (from simple-parsing->tensorflow-datasets) (0.16)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the IMDb movie reviews dataset, which is a collection of movie reviews along with sentiment labels. We will focus on the text data and ignore the labels for this task.\n",
    "\n",
    "**as_supervised=True** allows us to retrieve the data in a format where the input data is paired with its label (although we won't use the labels in this case).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Text Data\n",
    "\n",
    "The IMDb dataset contains raw text data that we need to preprocess. We will tokenize the text (split the text into words) and convert them into trigrams (3 consecutive words).\n",
    "\n",
    "Here, the tokenize() function converts the text from a byte string to a regular Python string and then splits it into individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return text.numpy().decode('utf-8').split()\n",
    "\n",
    "def extract_trigrams(text):\n",
    "    words = tokenize(text)\n",
    "    trigrams = [(words[i], words[i + 1], words[i+2]) for i in range(len(words) - 2)]\n",
    "    return trigrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limit Data Size for Training and Testing\n",
    "To keep the experiment manageable, we will limit the training and test data to a smaller number of samples (500 for training and 100 for testing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 17:49:31.472832: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2024-11-21 17:49:31.495157: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "train_data = dataset['train'].map(lambda x, y: x)\n",
    "test_data = dataset['test'].map(lambda x, y: x)\n",
    "\n",
    "train_texts = list(train_data.take(500))  # Limit to 500 training samples\n",
    "test_texts = list(test_data.take(100))    # Limit to 100 test samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocabulary and Convert Words to Indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_trigrams = []\n",
    "for text in train_texts:\n",
    "    train_trigrams.extend(extract_trigrams(text))\n",
    "\n",
    "test_trigrams = []\n",
    "for text in test_texts:\n",
    "    test_trigrams.extend(extract_trigrams(text))\n",
    "\n",
    "train_words = [w for trigram in train_trigrams for w in trigram]\n",
    "test_words = [w for trigram in test_trigrams for w in trigram]\n",
    "\n",
    "vocab = list(set(train_words))\n",
    "vocab_size = len(vocab)\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "word_to_idx[\"<UNK>\"] = vocab_size  # Add a special token for unknown words\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Words to Indices\n",
    "\n",
    "Every word in the training and test sets is replaced by its corresponding index in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_index(word):\n",
    "    return word_to_idx.get(word, word_to_idx[\"<UNK>\"])  # Use <UNK> for unknown words\n",
    "\n",
    "train_sequences = [word_to_index(word) for word in train_words]\n",
    "test_sequences = [word_to_index(word) for word in test_words]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data for the LSTM Model\n",
    "\n",
    "From the sequences of word indices, we need to prepare the data for the LSTM model. Specifically, we create input-output pairs where the input is a sequence of words, and the output is the next word in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_output(sequences, sequence_length=2):\n",
    "    X, y = [], []\n",
    "    for i in range(len(sequences) - sequence_length):\n",
    "        X.append(sequences[i:i + sequence_length - 1])\n",
    "        y.append(sequences[i + sequence_length - 1])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_train, y_train = create_input_output(train_sequences)\n",
    "X_test, y_test = create_input_output(test_sequences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the LSTM Model\n",
    "\n",
    "The model will have the following layers:\n",
    "\n",
    "- Embedding Layer: Converts word indices into dense word embeddings.\n",
    "- LSTM Layer: Processes the sequence of words to capture temporal dependencies.\n",
    "- Dense Layer: Outputs the probability distribution over all possible next words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size + 1, output_dim=128, input_length=X_train.shape[1]),  # +1 for <UNK>\n",
    "    tf.keras.layers.LSTM(128, return_sequences=False),\n",
    "    tf.keras.layers.Dense(vocab_size + 1, activation='softmax')  # +1 for <UNK>\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "5093/5093 [==============================] - 65s 13ms/step - loss: 7.4481 - accuracy: 0.0667 - val_loss: 7.5404 - val_accuracy: 0.0773\n",
      "Epoch 2/3\n",
      "5093/5093 [==============================] - 59s 12ms/step - loss: 6.8613 - accuracy: 0.0994 - val_loss: 7.7460 - val_accuracy: 0.0886\n",
      "Epoch 3/3\n",
      "5093/5093 [==============================] - 58s 11ms/step - loss: 6.4854 - accuracy: 0.1209 - val_loss: 7.9276 - val_accuracy: 0.0925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2d5dca200>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1896/1896 [==============================] - 4s 2ms/step\n",
      "Perplexity: 2765.4765572578326\n"
     ]
    }
   ],
   "source": [
    "def calculate_perplexity(model, X, y):\n",
    "    with tf.device('/GPU:0'):\n",
    "        predictions = model.predict(X)\n",
    "        log_prob_sum = 0\n",
    "        N = len(y)\n",
    "        \n",
    "        for i in range(N):\n",
    "            prob = predictions[i, y[i]]\n",
    "            log_prob_sum += np.log(prob + 1e-10)  # Smoothing to avoid log(0)\n",
    "        \n",
    "        perplexity = np.exp(-log_prob_sum / N)\n",
    "        return perplexity\n",
    "\n",
    "perplexity = calculate_perplexity(model, X_test, y_test)\n",
    "print(f'Perplexity: {perplexity}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategies to improve the score\n",
    "\n",
    "- Limited Context:\n",
    "\n",
    "    A trigram model relies on only two preceding words to predict the next word. This is often insufficient for capturing long-range dependencies in natural language.\n",
    "    Many words in a sentence depend on context from earlier words or the entire sentence, not just the last two words.\n",
    "\n",
    "- Small or Insufficient Data:\n",
    "\n",
    "    Language models require large amounts of data to estimate probabilities accurately, especially for trigrams where combinations of three words must be seen during training.\n",
    "    Sparse Data Problem:\n",
    "\n",
    "    Trigram models suffer from sparsity, as many possible word combinations may not appear in the training data. This makes it difficult for the model to generalize.\n",
    "    Vocabulary Size:\n",
    "\n",
    "    A large vocabulary size increases the model's difficulty in accurately estimating probabilities for rare or unseen words, leading to higher perplexity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
