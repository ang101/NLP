{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Perplexity in Language Models\n",
    "\n",
    "To understand and compute perplexity, a key evaluation metric for language models, and analyze how it reflects the quality of text predictions.\n",
    "\n",
    "**What is Perplexity?**\n",
    "Perplexity measures the uncertainty of a language model in predicting a sequence of words. It indicates how \"perplexed\" the model is by the text.\n",
    "\n",
    "- Low perplexity: The model predicts the sequence with high confidence.\n",
    "- High perplexity: The model struggles to predict the sequence, indicating poor performance.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Perplexity Comparison ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Text: The quick brown fox jumps over the lazy dog.\n",
      "Tokenized Input: [[  464  2068  7586 21831 18045   625   262 16931  3290    13]]\n",
      "Loss: [5.0905147]\n",
      "Perplexity: [162.47346]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Text: Quick the brown fox over lazy jumps dog the.\n",
      "Tokenized Input: [[21063   262  7586 21831   625 16931 18045  3290   262    13]]\n",
      "Loss: [8.565364]\n",
      "Perplexity: [5246.7485]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Text: Lorem ipsum dolor sit amet, consectetur adipiscing elit.\n",
      "Tokenized Input: [[   43 29625   220  2419   388   288 45621  1650   716   316    11   369\n",
      "   8831   316   333 31659   271  2259  1288   270    13]]\n",
      "Loss: [0.9613916]\n",
      "Perplexity: [2.6153336]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Text: Random gibberish xzq mfnweor pasd.\n",
      "Tokenized Input: [[29531 46795   527   680  2124    89    80   285 22184   732   273 38836\n",
      "     67    13]]\n",
      "Loss: [6.982953]\n",
      "Perplexity: [1078.0974]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def calculate_perplexity(text, model_name='gpt2'):\n",
    "    \"\"\"\n",
    "    Calculates the perplexity of the given text using a GPT-2 model in TensorFlow.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "        model_name (str): Name of the Hugging Face model (default: 'gpt2').\n",
    "    \n",
    "    Returns:\n",
    "        float: Perplexity score.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = TFAutoModelForCausalLM.from_pretrained(model_name)\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    tokens = tokenizer.encode(text, return_tensors='tf')\n",
    "\n",
    "    # Log the tokenized input\n",
    "    print(f\"\\nOriginal Text: {text}\")\n",
    "    print(f\"Tokenized Input: {tokens}\")\n",
    "\n",
    "    # Calculate loss and perplexity\n",
    "    outputs = model(tokens, labels=tokens)\n",
    "    loss = outputs.loss\n",
    "    perplexity = tf.exp(loss)\n",
    "\n",
    "    print(f\"Loss: {loss.numpy()}\")\n",
    "    return perplexity.numpy()\n",
    "\n",
    "# Compare perplexity of different examples\n",
    "texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",  # Grammatically correct and meaningful\n",
    "    \"Quick the brown fox over lazy jumps dog the.\",  # Grammatically incorrect and jumbled\n",
    "    \"Lorem ipsum dolor sit amet, consectetur adipiscing elit.\",  # Latin placeholder text\n",
    "    \"Random gibberish xzq mfnweor pasd.\"  # Completely random gibberish\n",
    "]\n",
    "\n",
    "print(\"\\n--- Perplexity Comparison ---\")\n",
    "for text in texts:\n",
    "    perplexity = calculate_perplexity(text)\n",
    "    print(f\"Perplexity: {perplexity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower perplexity for natural text (first example) indicates the model is confident in predicting the sequence.\n",
    "Higher perplexity for random gibberish reflects the model's struggle to make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the IMDb movie reviews dataset, which is a collection of movie reviews along with sentiment labels. We will focus on the text data and ignore the labels for this task.\n",
    "\n",
    "**as_supervised=True** allows us to retrieve the data in a format where the input data is paired with its label (although we won't use the labels in this case).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 23:14:10.233458: W tensorflow/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to /Users/divyahegde/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abf62e3aa4b44f12abc5c832e6c763de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "415722f8d5cd4040b419c4901b036bbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3848e26470e74936a09232b278e23b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe73355bf9845de94bc42b630246678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fbdca61a62244b18866bc1751a6b72b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /Users/divyahegde/tensorflow_datasets/imdb_reviews/plain_text/incomplete.7NDLW5_1.0.0/imdb_reviews-t…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e39b084d93314fab97cf2add7d31f1ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a2c83c69bc143d794dd2f6a70585e62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /Users/divyahegde/tensorflow_datasets/imdb_reviews/plain_text/incomplete.7NDLW5_1.0.0/imdb_reviews-t…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1baa22d87a4648538885009928b48e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised examples...:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6a629c689ea4a82b022f0a4a94ce5d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /Users/divyahegde/tensorflow_datasets/imdb_reviews/plain_text/incomplete.7NDLW5_1.0.0/imdb_reviews-u…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to /Users/divyahegde/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dataset, info = tfds.load('imdb_reviews', with_info=True, as_supervised=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Text Data\n",
    "\n",
    "The IMDb dataset contains raw text data that we need to preprocess. We will tokenize the text (split the text into words) and convert them into bigrams (pairs of consecutive words).\n",
    "\n",
    "Here, the tokenize() function converts the text from a byte string to a regular Python string and then splits it into individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return text.numpy().decode('utf-8').split()\n",
    "\n",
    "def extract_bigrams(text):\n",
    "    words = tokenize(text)\n",
    "    bigrams = [(words[i], words[i + 1]) for i in range(len(words) - 1)]\n",
    "    return bigrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limit Data Size for Training and Testing\n",
    "To keep the experiment manageable, we will limit the training and test data to a smaller number of samples (500 for training and 100 for testing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 23:32:01.452421: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2024-11-19 23:32:01.483479: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "train_data = dataset['train'].map(lambda x, y: x)\n",
    "test_data = dataset['test'].map(lambda x, y: x)\n",
    "\n",
    "train_texts = list(train_data.take(500))  # Limit to 500 training samples\n",
    "test_texts = list(test_data.take(100))    # Limit to 100 test samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Vocabulary and Convert Words to Indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bigrams = []\n",
    "for text in train_texts:\n",
    "    train_bigrams.extend(extract_bigrams(text))\n",
    "\n",
    "test_bigrams = []\n",
    "for text in test_texts:\n",
    "    test_bigrams.extend(extract_bigrams(text))\n",
    "\n",
    "train_words = [w for bigram in train_bigrams for w in bigram]\n",
    "test_words = [w for bigram in test_bigrams for w in bigram]\n",
    "\n",
    "vocab = list(set(train_words))\n",
    "vocab_size = len(vocab)\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "word_to_idx[\"<UNK>\"] = vocab_size  # Add a special token for unknown words\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Words to Indices\n",
    "\n",
    "Every word in the training and test sets is replaced by its corresponding index in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_index(word):\n",
    "    return word_to_idx.get(word, word_to_idx[\"<UNK>\"])  # Use <UNK> for unknown words\n",
    "\n",
    "train_sequences = [word_to_index(word) for word in train_words]\n",
    "test_sequences = [word_to_index(word) for word in test_words]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data for the LSTM Model\n",
    "\n",
    "From the sequences of word indices, we need to prepare the data for the LSTM model. Specifically, we create input-output pairs where the input is a sequence of words, and the output is the next word in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_output(sequences, sequence_length=2):\n",
    "    X, y = [], []\n",
    "    for i in range(len(sequences) - sequence_length):\n",
    "        X.append(sequences[i:i + sequence_length - 1])\n",
    "        y.append(sequences[i + sequence_length - 1])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_train, y_train = create_input_output(train_sequences)\n",
    "X_test, y_test = create_input_output(test_sequences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the LSTM Model\n",
    "\n",
    "The model will have the following layers:\n",
    "\n",
    "- Embedding Layer: Converts word indices into dense word embeddings.\n",
    "- LSTM Layer: Processes the sequence of words to capture temporal dependencies.\n",
    "- Dense Layer: Outputs the probability distribution over all possible next words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size + 1, output_dim=128, input_length=X_train.shape[1]),  # +1 for <UNK>\n",
    "    tf.keras.layers.LSTM(128, return_sequences=False),\n",
    "    tf.keras.layers.Dense(vocab_size + 1, activation='softmax')  # +1 for <UNK>\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "3411/3411 [==============================] - 44s 13ms/step - loss: 6.9333 - accuracy: 0.1827 - val_loss: 6.3954 - val_accuracy: 0.2806\n",
      "Epoch 2/3\n",
      "3411/3411 [==============================] - 46s 13ms/step - loss: 5.6455 - accuracy: 0.3139 - val_loss: 6.1638 - val_accuracy: 0.3337\n",
      "Epoch 3/3\n",
      "3411/3411 [==============================] - 43s 13ms/step - loss: 4.9628 - accuracy: 0.3678 - val_loss: 6.0533 - val_accuracy: 0.3644\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2cc0d68c0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1270/1270 [==============================] - 3s 2ms/step\n",
      "Perplexity: 424.87818777474286\n"
     ]
    }
   ],
   "source": [
    "def calculate_perplexity(model, X, y):\n",
    "    predictions = model.predict(X)\n",
    "    log_prob_sum = 0\n",
    "    N = len(y)\n",
    "    \n",
    "    for i in range(N):\n",
    "        prob = predictions[i, y[i]]\n",
    "        log_prob_sum += np.log(prob + 1e-10)  # Smoothing to avoid log(0)\n",
    "    \n",
    "    perplexity = np.exp(-log_prob_sum / N)\n",
    "    return perplexity\n",
    "\n",
    "perplexity = calculate_perplexity(model, X_test, y_test)\n",
    "print(f'Perplexity: {perplexity}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
