{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove and Word2Vec\n",
    "### Recap of Glove\n",
    "GloVe (Global Vectors for Word Representation) is an unsupervised learning algorithm for obtaining vector representations for words. It is based on the idea that the meaning of a word can be derived from the company it keeps, i.e., the context in which it appears.\n",
    "\n",
    "### Training Process:\n",
    "1. **Corpus Preparation**: A large corpus of text is collected. This corpus is used to gather word co-occurrence statistics.\n",
    "2. **Co-occurrence Matrix**: A co-occurrence matrix is constructed, where each element (i, j) represents the number of times word j appears in the context of word i.\n",
    "3. **Weighting Function**: A weighting function is applied to the co-occurrence matrix to give more importance to frequent co-occurrences and less importance to rare ones.\n",
    "4. **Cost Function**: The GloVe model uses a cost function that minimizes the difference between the dot product of the word vectors and the logarithm of the word co-occurrence probabilities.\n",
    "5. **Optimization**: The cost function is optimized using methods like stochastic gradient descent (SGD) to learn the word vectors.\n",
    "\n",
    "The resulting word vectors capture semantic relationships between words, such that words with similar meanings are close to each other in the vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing GloVe Word Embeddings\n",
    "\n",
    "To implement GloVe (Global Vectors for Word Representation) word embeddings, follow these steps:\n",
    "\n",
    "1. **Download Pre-trained GloVe Embeddings**:\n",
    "    - GloVe provides pre-trained word vectors for different dimensions (e.g., 50, 100, 200, 300). You can download these embeddings from the [GloVe website](https://nlp.stanford.edu/projects/glove/).\n",
    "\n",
    "2. **Load the GloVe Embeddings**:\n",
    "    - Load the pre-trained GloVe embeddings into a dictionary where the keys are words and the values are their corresponding vectors.\n",
    "\n",
    "3. **Prepare Your Text Data**:\n",
    "    - Tokenize and preprocess your text data. This involves splitting the text into sentences and words, and performing any necessary preprocessing steps such as lowercasing, removing punctuation, and lemmatization.\n",
    "\n",
    "4. **Create a Tokenizer**:\n",
    "    - Use a tokenizer to convert your text data into sequences of integers, where each integer represents a word in the vocabulary.\n",
    "\n",
    "5. **Create an Embedding Matrix**:\n",
    "    - Create an embedding matrix where each row corresponds to a word in the vocabulary and contains the GloVe vector for that word. If a word is not found in the GloVe embeddings, you can initialize its vector with zeros or random values.\n",
    "\n",
    "6. **Use the Embedding Matrix**:\n",
    "    - Use the embedding matrix in your machine learning models, such as neural networks, to represent words as dense vectors.\n",
    "\n",
    "4. **Use Embedding Matrix in Models**:\n",
    "    - You can now use the `embedding_matrix_vocab` in your machine learning models to represent words as dense vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap of Word2Vec\n",
    "Word2Vec is a popular word embedding technique that uses neural networks to learn vector representations of words. It captures semantic relationships between words by training on a large corpus of text. There are two main models in Word2Vec:\n",
    "\n",
    "1. **Continuous Bag of Words (CBOW)**: Predicts the target word based on its context (surrounding words).\n",
    "2. **Skip-gram**: Predicts the context words given a target word.\n",
    "\n",
    "### Implementing Word2Vec with Gensim\n",
    "\n",
    "1. **Prepare Your Text Data**:\n",
    "    - Tokenize and preprocess your text data. This involves splitting the text into sentences and words, and performing any necessary preprocessing steps such as lowercasing, removing punctuation, and lemmatization.\n",
    "\n",
    "2. **Train the Word2Vec Model**:\n",
    "    - Use the `Word2Vec` class from the Gensim library to train the model on your preprocessed text data. You can specify parameters such as `vector_size` (dimensionality of the word vectors), `window` (context window size), and `sg` (training algorithm: 0 for CBOW, 1 for Skip-gram).\n",
    "\n",
    "3. **Access Word Vectors**:\n",
    "    - Once the model is trained, you can access the word vectors using the `wv` attribute of the model. You can also find similar words and calculate similarities between words using methods like `similarity` and `most_similar`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-u9E9FanxKZD"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import os\n",
    "import urllib.request\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import spatial\n",
    "from sklearn.manifold import TSNE\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from Word2Vec_Glove_helper import *\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Using NLTK Punkt and WordNet for GloVe and Word2Vec\n",
    "\n",
    "#### NLTK Punkt\n",
    "- **Sentence Tokenization**: Punkt is a pre-trained tokenizer that helps in splitting a text into sentences. This is particularly useful for preparing text data for both GloVe and Word2Vec models.\n",
    "- **Word Tokenization**: After splitting the text into sentences, Punkt can also be used to tokenize each sentence into words. This step is crucial for creating the co-occurrence matrix in GloVe and for training the Word2Vec model.\n",
    "\n",
    "#### NLTK WordNet\n",
    "- **Lemmatization**: WordNet is a lexical database for the English language. It provides the `WordNetLemmatizer` which helps in reducing words to their base or root form. This is important for both GloVe and Word2Vec as it ensures that different forms of a word (e.g., \"running\", \"ran\", \"runs\") are treated as a single word (\"run\").\n",
    "- **Synonyms and Semantic Relationships**: WordNet can also be used to find synonyms and understand semantic relationships between words, which can be beneficial for enhancing the quality of word embeddings.\n",
    "\n",
    "By using NLTK's Punkt and WordNet, we can preprocess the text data effectively, ensuring that the GloVe and Word2Vec models learn meaningful and high-quality word representations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kRamKNzgBJO_",
    "outputId": "1afbb627-7233-46bd-ae03-eabf13d17a1e"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Dataset\n",
    "\n",
    "### Context\n",
    "This dataset consists of reviews of fine foods from Amazon. The data span a period of more than 10 years, including all ~500,000 reviews up to October 2012. Reviews include product and user information, ratings, and a plain text review. It also includes reviews from all other Amazon categories.\n",
    "\n",
    "### Contents\n",
    "**Reviews.csv**: Pulled from the Amazon food reviews\n",
    "\n",
    "**Data includes:**\n",
    "- Reviews from Oct 1999 - Oct 2012\n",
    "- 568,454 reviews\n",
    "- 256,059 users\n",
    "- 74,258 products\n",
    "- 260 users with > 50 reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bYn_B6Jb3dOZ"
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"Reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 597
    },
    "id": "0eFpU8UW8TcF",
    "outputId": "018fec9b-1b13-4466-a688-c409d1aae080"
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DtO66Vk68mwt"
   },
   "outputs": [],
   "source": [
    "corpus_text = '\\n'.join(data[:50000]['Text'])\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Preprocessing Text Data\n",
    "\n",
    "In this step, we preprocess the text data by tokenizing sentences and words, and then lemmatizing the tokens. We also filter out non-alphanumeric tokens and convert the remaining tokens to lowercase. The preprocessed data is stored in the `t_data` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BhWL5smN-lU3"
   },
   "outputs": [],
   "source": [
    "t_data=[]\n",
    "for i in sent_tokenize(corpus_text):\n",
    "  temp=[]\n",
    "  tokens=word_tokenize(i)\n",
    "  lemmatized_tokens=[lemmatizer.lemmatize(token) for token in tokens]\n",
    "  for j in lemmatized_tokens:\n",
    "    if(j.isalnum()):\n",
    "      temp.append(j.lower())\n",
    "  t_data.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check if the words have converted to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PqDE5OWfAqYA",
    "outputId": "a2997dee-19a8-4348-dac4-4093b5e279bd"
   },
   "outputs": [],
   "source": [
    "t_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORD2VEC WORD EMBEDDINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Training Word2Vec Models\n",
    "\n",
    "In this step, we train two Word2Vec models using the preprocessed text data (`t_data`). We use the Gensim library to create the models:\n",
    "\n",
    "1. **CBOW Model** (`model1`): This model uses the Continuous Bag of Words (CBOW) approach, where the context (surrounding words) is used to predict the target word.\n",
    "2. **Skip-gram Model** (`model2`): This model uses the Skip-gram approach, where the target word is used to predict the context (surrounding words).\n",
    "\n",
    "Both models are trained with the following parameters:\n",
    "- `min_count=1`: Ignores all words with a total frequency lower than this.\n",
    "- `vector_size=100`: Dimensionality of the word vectors.\n",
    "- `window=5`: Maximum distance between the current and predicted word within a sentence.\n",
    "- `sg`: Training algorithm, 0 for CBOW and 1 for Skip-gram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LgPojNmZB8OH"
   },
   "outputs": [],
   "source": [
    "model1=Word2Vec(t_data,min_count=1,vector_size=100,window=5,sg=0)\n",
    "model2=Word2Vec(t_data,min_count=1,vector_size=100,window=5,sg=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's check similarity scores of both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a28uVOI1Dj0h",
    "outputId": "508d8798-eccf-4813-bc56-ca8569908003"
   },
   "outputs": [],
   "source": [
    "print('similarity between two words is ',model1.wv.similarity('highly','recommend'))\n",
    "print('similarity between two words is ',model2.wv.similarity('highly','recommend'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's try differnt pair of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fFn_DJ-cdOQR",
    "outputId": "ca740c77-4282-447a-e7ac-4d4834e52c0a"
   },
   "outputs": [],
   "source": [
    "print('similarity between two words is ',model2.wv.similarity('tea','coffee'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Accessing Word Embeddings\n",
    "\n",
    "In this step, we access the word embedding for the word \"recommend\" from the Skip-gram Word2Vec model (`model2`). The embedding is a dense vector representation of the word, capturing its semantic meaning based on the context in which it appears in the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_EM3AogFOjr",
    "outputId": "5e772863-e73c-4d5c-d00a-06bc64987f6c"
   },
   "outputs": [],
   "source": [
    "embedding = model2.wv['recommend']\n",
    "print(f\"Embedding for '{'recommend'}':\\n{embedding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRyQJfvrKdwl"
   },
   "source": [
    "\n",
    "### Sentence Similarity Using Word2Vec\n",
    "\n",
    "In this step, we calculate the cosine similarity between two sentences using the Word2Vec model (`model2`). The process involves tokenizing and preprocessing the sentences, converting them into vectors using the Word2Vec model, and then calculating the cosine similarity between the resulting sentence vectors.\n",
    "\n",
    "The sentences used for this example are:\n",
    "- Sentence 1: \"This Product is highly recommended.\"\n",
    "- Sentence 2: \"I like the product.\"\n",
    "\n",
    "The cosine similarity score indicates how similar the two sentences are based on their word embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q0Quy-jZGS7L",
    "outputId": "1fd9efac-6d75-42bd-a175-304f54ced33f"
   },
   "outputs": [],
   "source": [
    "sentence1 = \"This Product is highly recommended.\"\n",
    "sentence2 = \"I like the product.\"\n",
    "\n",
    "tokens1 = tokenize_and_preprocess_text(sentence1)\n",
    "tokens2 = tokenize_and_preprocess_text(sentence2)\n",
    "\n",
    "vector1 = get_sentence_vector(tokens1, model2)\n",
    "vector2 = get_sentence_vector(tokens2, model2)\n",
    "\n",
    "# Calculate the cosine similarity between the two sentence vectors\n",
    "similarity = cosine_similarity([vector1], [vector2])[0][0]\n",
    "\n",
    "print(f\"Cosine Similarity: {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Word2Vec Embeddings with t-SNE\n",
    "\n",
    "In this step, we visualize the Word2Vec embeddings using t-SNE (t-Distributed Stochastic Neighbor Embedding). t-SNE is a dimensionality reduction technique that helps in visualizing high-dimensional data in a 2D space. We use the pre-trained Skip-gram Word2Vec model (`model2`) to obtain the word vectors and then apply t-SNE to reduce the dimensionality of these vectors.\n",
    "\n",
    "The resulting 2D plot shows the word embeddings, where each point represents a word, and the distance between points indicates the similarity between the words. Words with similar meanings are expected to be closer together in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 699
    },
    "id": "-FQz6zIoLOhB",
    "outputId": "6164795d-5e2c-48c0-bbf5-9ac732f4ff43"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "# Get word vectors and corresponding words from the model\n",
    "words = list(model2.wv.index_to_key)\n",
    "words=words[:100]\n",
    "word_vectors = [model2.wv[word] for word in words]\n",
    "\n",
    "# Convert word_vectors to a NumPy array\n",
    "word_vectors = np.array(word_vectors)\n",
    "\n",
    "# Perform t-SNE dimensionality reduction\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "word_vectors_2d = tsne.fit_transform(word_vectors)\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(word_vectors_2d[:, 0], word_vectors_2d[:, 1], marker='o', s=30)\n",
    "\n",
    "# Label some points for reference (optional)\n",
    "sample_words = words[:100]  # Label the first 5 words from your list\n",
    "for word, (x, y) in zip(sample_words, word_vectors_2d[:100]):\n",
    "    plt.annotate(word, (x, y))\n",
    "\n",
    "# # Label some points for reference (optional)\n",
    "# sample_words = ['word1', 'word2', 'word3']  # Replace with words from your model\n",
    "# for word in sample_words:\n",
    "#     idx = words.index(word)\n",
    "#     plt.annotate(word, (word_vectors_2d[idx, 0], word_vectors_2d[idx, 1]))\n",
    "\n",
    "# Show the plot\n",
    "plt.title('t-SNE Plot of Word2Vec Embeddings')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0l3znEVaRv5"
   },
   "source": [
    "# GLOVE WORD EMBEDDINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Dictionary\n",
    "\n",
    "In this step, we create a dictionary of unique words from the preprocessed text data (`t_data`). We use the `Tokenizer` class from the TensorFlow Keras library to fit the tokenizer on the text data and generate a word index. The word index is a dictionary where the keys are words and the values are their corresponding integer indices.\n",
    "\n",
    "The output includes:\n",
    "- The number of unique words in the dictionary.\n",
    "- The dictionary itself, showing the mapping of words to their indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oBisrTEBeTqL",
    "outputId": "44defb41-5ad1-4927-8b20-8a7a1cbe1f5f"
   },
   "outputs": [],
   "source": [
    "# create the dict.\n",
    "x=[token for token in list(i for i in t_data)]\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x)\n",
    "\n",
    "# number of unique words in dict.\n",
    "print(\"Number of unique words in dictionary=\",\n",
    "\tlen(tokenizer.word_index))\n",
    "print(\"Dictionary is = \", tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Embedding Matrix\n",
    "\n",
    "In this step, we create an embedding matrix for the vocabulary using pre-trained GloVe embeddings. The embedding matrix is a 2D NumPy array where each row corresponds to a word in the vocabulary and contains the GloVe vector for that word. If a word is not found in the GloVe embeddings, its vector is initialized with zeros.\n",
    "\n",
    "The process involves:\n",
    "1. Defining the `embedding_for_vocab` function to load the GloVe embeddings and create the embedding matrix.\n",
    "2. Specifying the embedding dimension (e.g., 50).\n",
    "3. Creating the embedding matrix using the `embedding_for_vocab` function and the word index from the tokenizer.\n",
    "\n",
    "The output includes the dense vector for the first word in the vocabulary ('the')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8oTwIuUTeLY8",
    "outputId": "edf877ef-7349-4068-caec-429b76533390"
   },
   "outputs": [],
   "source": [
    "# matrix for vocab: word_index\n",
    "embedding_dim = 50\n",
    "embedding_matrix_vocab = embedding_for_vocab(\n",
    "\t'glove.6B.100d.txt', tokenizer.word_index,\n",
    "embedding_dim)\n",
    "\n",
    "print(\"Dense vector for first word 'the' is => \",\n",
    "\tembedding_matrix_vocab[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Accessing GloVe Embeddings and Calculating Similarity\n",
    "\n",
    "In this step, we access the GloVe word embedding for the word \"good\" from the embedding matrix (`embedding_matrix_vocab`). We also calculate the cosine similarity between two words (\"good\" and \"excellent\") using their GloVe embeddings.\n",
    "\n",
    "The process involves:\n",
    "1. Checking if the word \"good\" is in the tokenizer's word index and retrieving its embedding vector.\n",
    "2. Calculating the cosine similarity between the embeddings of \"good\" and \"excellent\".\n",
    "\n",
    "The output includes:\n",
    "- The word embedding vector for \"good\".\n",
    "- The cosine similarity score between \"good\" and \"excellent\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rYVTyXABiRW8",
    "outputId": "8ad51f9b-54de-40db-e893-52ee350cc24b"
   },
   "outputs": [],
   "source": [
    "word_to_find = 'good'\n",
    "if word_to_find in tokenizer.word_index:\n",
    "    idx = tokenizer.word_index[word_to_find]\n",
    "    # Access the word embedding vector for 'good'\n",
    "    embedding_of_good = embedding_matrix_vocab[idx]\n",
    "    print(f\"Word embedding vector for '{word_to_find}':\\n{embedding_of_good}\")\n",
    "else:\n",
    "    print(f\"'{word_to_find}' not found in the vocabulary.\")\n",
    "\n",
    "# Calculate similarity between two words (e.g., 'good' and 'excellent')\n",
    "word1 = 'good'\n",
    "word2 = 'excellent'\n",
    "if word1 in tokenizer.word_index and word2 in tokenizer.word_index:\n",
    "    idx1 = tokenizer.word_index[word1]\n",
    "    idx2 = tokenizer.word_index[word2]\n",
    "    embedding1 = embedding_matrix_vocab[idx1]\n",
    "    embedding2 = embedding_matrix_vocab[idx2]\n",
    "    similarity = cosine_similarity([embedding1], [embedding2])[0][0]\n",
    "    print(f\"Similarity between '{word1}' and '{word2}': {similarity}\")\n",
    "else:\n",
    "    print(\"One or both of the words not found in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gYg1x0Ppwkz"
   },
   "source": [
    "\n",
    "### Sentence Similarity Using GloVe\n",
    "\n",
    "In this step, we calculate the cosine similarity between two sentences using the GloVe embeddings. The process involves tokenizing and preprocessing the sentences, converting them into vectors using the GloVe embedding matrix, and then calculating the cosine similarity between the resulting sentence vectors.\n",
    "\n",
    "The sentences used for this example are:\n",
    "- Sentence 1: \"This Product is highly recommended.\"\n",
    "- Sentence 2: \"I like the product.\"\n",
    "\n",
    "The cosine similarity score indicates how similar the two sentences are based on their word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ofsXcX4DpxyI",
    "outputId": "b87a23b0-f1c4-48b6-a676-0483cfb4583b"
   },
   "outputs": [],
   "source": [
    "sentence1 = \"This Product is highly recommended.\"\n",
    "sentence2 = \"I like the product.\"\n",
    "\n",
    "def tokenize_and_preprocess_text(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tokens=[lemmatizer.lemmatize(token) for token in tokens]\n",
    "    tokens = [token.lower() for token in tokens if token.isalnum()]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "tokens1 = tokenize_and_preprocess_text(sentence1)\n",
    "tokens2 = tokenize_and_preprocess_text(sentence2)\n",
    "\n",
    "def get_sentence_vector(tokens, model):\n",
    "    # Filter out tokens that are not in the model's vocabulary\n",
    "    if not tokens:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(list(embedding_matrix_vocab[tokenizer.word_index[word]] for word in tokens), axis=0)\n",
    "\n",
    "vector1 = get_sentence_vector(tokens1, model2)\n",
    "vector2 = get_sentence_vector(tokens2, model2)\n",
    "\n",
    "# Calculate the cosine similarity between the two sentence vectors\n",
    "similarity = cosine_similarity([vector1], [vector2])[0][0]\n",
    "\n",
    "print(f\"Cosine Similarity: {similarity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Visualizing GloVe Embeddings with t-SNE\n",
    "\n",
    "In this step, we visualize the GloVe embeddings using t-SNE (t-Distributed Stochastic Neighbor Embedding). t-SNE is a dimensionality reduction technique that helps in visualizing high-dimensional data in a 2D space. We use the pre-trained GloVe embeddings to obtain the word vectors and then apply t-SNE to reduce the dimensionality of these vectors.\n",
    "\n",
    "The resulting 2D plot shows the word embeddings, where each point represents a word, and the distance between points indicates the similarity between the words. Words with similar meanings are expected to be closer together in the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 699
    },
    "id": "3fNV6OZcjmQ6",
    "outputId": "4107a3be-366a-47f5-8756-0c59f04087bd"
   },
   "outputs": [],
   "source": [
    "# Load pre-trained GloVe embeddings\n",
    "embedding_path = 'glove.6B.100d.txt'\n",
    "word_embeddings = {}\n",
    "with open(embedding_path, 'r', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        word_embeddings[word] = vector\n",
    "\n",
    "# Get word vectors and corresponding words from the GloVe model\n",
    "words = list(tokenizer.word_index.keys())\n",
    "words = words[200:300]  # Limit to the 100 words for the example\n",
    "word_vectors = [embedding_matrix_vocab[tokenizer.word_index[word]] for word in words]\n",
    "\n",
    "# Convert word_vectors to a NumPy array\n",
    "word_vectors = np.array(word_vectors)\n",
    "\n",
    "# Perform t-SNE dimensionality reduction\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "word_vectors_2d = tsne.fit_transform(word_vectors)\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(word_vectors_2d[:, 0], word_vectors_2d[:, 1], marker='o', s=30)\n",
    "\n",
    "# Label points with word labels\n",
    "for word, (x, y) in zip(words, word_vectors_2d):\n",
    "    plt.text(x, y, word, fontsize=10, ha='center', va='bottom')\n",
    "\n",
    "# Show the plot\n",
    "plt.title('t-SNE Plot of GloVe Embeddings with Word Labels')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, we explored two popular word embedding techniques: GloVe and Word2Vec. We covered the following key points:\n",
    "\n",
    "1. **GloVe (Global Vectors for Word Representation)**:\n",
    "    - We discussed the training process of GloVe, including corpus preparation, co-occurrence matrix construction, weighting function, cost function, and optimization.\n",
    "    - We implemented GloVe word embeddings by creating a dictionary of unique words, loading pre-trained GloVe embeddings, and creating an embedding matrix for our vocabulary.\n",
    "    - We accessed GloVe embeddings for specific words and calculated cosine similarity between word pairs.\n",
    "    - We calculated sentence similarity using GloVe embeddings and visualized the embeddings using t-SNE.\n",
    "\n",
    "2. **Word2Vec**:\n",
    "    - We provided an overview of Word2Vec, including the CBOW and Skip-gram models.\n",
    "    - We implemented Word2Vec word embeddings using the Gensim library, training both CBOW and Skip-gram models on our preprocessed text data.\n",
    "    - We accessed word embeddings for specific words and calculated similarity scores between word pairs.\n",
    "    - We calculated sentence similarity using Word2Vec embeddings and visualized the embeddings using t-SNE.\n",
    "\n",
    "By comparing and visualizing the embeddings from both GloVe and Word2Vec, we gained insights into how these techniques capture semantic relationships between words."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
