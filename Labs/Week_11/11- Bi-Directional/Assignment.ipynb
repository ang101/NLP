{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bi-directional RNN and Sentiment Analysis Assignement(Graded)\n",
        "Now you will be building Bi-directional LSTM and RNN using Fake news dataset. \n",
        "\n",
        "### Problem Description\n",
        "In this assignment, we aim to build a sentiment analysis model using a Bi-directional LSTM and RNN. The dataset used for this task is a Fake News dataset, which contains news articles labeled as real or fake. The goal is to preprocess the text data, convert it into a suitable format for model training, and then train a Bi-directional LSTM model to classify the news articles accurately. The performance of the model will be evaluated using various metrics such as accuracy, confusion matrix, and classification report.\n",
        "\n",
        "### Description of the Dataset\n",
        "A full training dataset with the following attributes:\n",
        "\n",
        "- **id**: unique id for a news article\n",
        "- **title**: the title of a news article\n",
        "- **author**: author of the news article\n",
        "- **text**: the text of the article; could be incomplete\n",
        "- **label**: a label that marks the article as potentially unreliable\n",
        "    - 1: unreliable\n",
        "    - 0: reliable\n",
        "\n",
        "### Assignement Task\n",
        "- Import necessary libraries and load the dataset.\n",
        "- Handle bad lines in the dataset and read it into a DataFrame.\n",
        "- Perform initial data exploration (head, shape, null values).\n",
        "- Drop rows with NaN values and verify the changes.\n",
        "- Separate the dataset into independent features (X) and dependent features (y).\n",
        "- Import TensorFlow and necessary Keras layers for model building.\n",
        "- Preprocess the text data (tokenization, padding).\n",
        "- Build and compile a Sequential model with Embedding and LSTM layers.\n",
        "- Train the model on the training data and validate it on the test data.\n",
        "- Evaluate the model's performance using confusion matrix, accuracy score, and classification report.\n",
        "- Build and train a Bi-directional LSTM model.\n",
        "- Evaluate the Bi-directional LSTM model's performance using the same metrics.\n",
        "\n",
        "### Instructions\n",
        "- Only write code when you see any of the below prompts,\n",
        "\n",
        "    ```\n",
        "    # YOUR CODE GOES HERE\n",
        "    # YOUR CODE ENDS HERE\n",
        "    # TODO\n",
        "    ```\n",
        "\n",
        "- Do not modify any other section of the code unless tated otherwise in the comments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7sqoc2NOmpwp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding# word 2 vec\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences# pre-padding and post padding\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "import re\n",
        "# setting a threshold value of 0.5->0.5=1 and <0.5 =0\n",
        "from sklearn.metrics import confusion_matrix\n",
        "### Dataset Preprocessing\n",
        "from nltk.stem.porter import PorterStemmer ##stemming purpose\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from nltk.corpus import stopwords\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task : We have to handle bad line\n",
        "To handle bad lines in the dataset, we define a function `handle_bad_line` that prints the bad line and returns `None`. We then use this function while reading the CSV file with `pd.read_csv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gFIZgq97nxRH"
      },
      "outputs": [],
      "source": [
        "def handle_bad_line(line):\n",
        "    print() # TODO: log the line to a file\n",
        "    return None \n",
        "\n",
        "df = pd.read_csv('FNC.csv', delimiter=',', encoding='utf-8', on_bad_lines=handle_bad_line, engine='python')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "NjtwqLNOqOel",
        "outputId": "6b243820-b4ad-4c75-cab3-fae46693f6f7"
      },
      "outputs": [],
      "source": [
        "#TODO : Check if there are any missing values in the dataset. If there are, drop the rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "s--2QTj3qwJa",
        "outputId": "f68ba3a1-c422-4960-cd6c-6c0cff794644"
      },
      "outputs": [],
      "source": [
        "# TODO: Check again to see if there are any missing values in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task: Making the features\n",
        "We will drop label column for the Independent features and we will add them to Dependent features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rvEpytuNqUSr"
      },
      "outputs": [],
      "source": [
        "## Get the Independent Features\n",
        "X=0 # TODO: Drop the label column from the dataset and store the remaining columns in a variable X with axis 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XwRX1uFpqWLi"
      },
      "outputs": [],
      "source": [
        "## Get the Dependent features\n",
        "y=df[0] # TODO: Store the label column in a variable y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Q9vSR70qXYN",
        "outputId": "81a8578e-6a86-48da-95d3-a3d2b49fa67a"
      },
      "outputs": [],
      "source": [
        "# TODO: Check the shape of the dataset dependent and independent features and make sure they are of the same length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ZiZfXDIeq9DF"
      },
      "outputs": [],
      "source": [
        "### Vocabulary size\n",
        "voc_size=5000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## One Hot representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "BcGsdzqgrHo6"
      },
      "outputs": [],
      "source": [
        "messages=X.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "plUhwAV4rJCd",
        "outputId": "c76dd0c1-1f62-45f9-a8a3-8ae84dc28ed6"
      },
      "outputs": [],
      "source": [
        "messages['title'][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "j2lk3XiTrKPP",
        "outputId": "986ad9c4-5504-4026-d3e3-b9b694e33bf4"
      },
      "outputs": [],
      "source": [
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "m9ChAqF3rM4M"
      },
      "outputs": [],
      "source": [
        "messages.reset_index(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "rBfD6lMUrZvX",
        "outputId": "4ae21d46-6391-4d8c-b3cb-aca680ccd5db"
      },
      "outputs": [],
      "source": [
        "messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task: Now we will remove stop words and punctuations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPGi5yE5rexa",
        "outputId": "d8c3892f-8bc8-4369-88bb-5d39ebd470ac"
      },
      "outputs": [],
      "source": [
        "# stopwords\n",
        "# TODO: download the stopwords from nltk library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "k252B0Murg3e"
      },
      "outputs": [],
      "source": [
        "ps = PorterStemmer()\n",
        "corpus = []\n",
        "for i in range(0, len(messages)):\n",
        "    # removing special characters and replacing it with blanks\n",
        "    review = re.sub(0, messages['title'][i]) # TODO: replace 0 special characters with blanks\n",
        "    review = review # TODO: convert the review to lowercase\n",
        "    review = review # TODO: split the review into words\n",
        "\n",
        "    review = 0 # TODO: stem the words and remove the stopwords\n",
        "    review = ' '.join(review)\n",
        "    # TODO: append the review to the corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3z1_3Rjrlim",
        "outputId": "00a33d4c-d3ef-415c-d905-55118d3808e9"
      },
      "outputs": [],
      "source": [
        "corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIlDuBNDrsSc",
        "outputId": "e694032e-32e0-47c1-c60c-9237be1fff4e"
      },
      "outputs": [],
      "source": [
        "onehot_repr=0 # TODO: convert the words in the corpus to onehot representation using the fucntion\n",
        "onehot_repr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9g3mKn89rx_n",
        "outputId": "8045008a-2b4f-46a8-95e8-e8d4fef019d1"
      },
      "outputs": [],
      "source": [
        "# TODO : Check the length of the onehot encoded corpus first sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmzGQbHZruSs",
        "outputId": "77cf744d-ab95-4100-854d-afc672b9f901"
      },
      "outputs": [],
      "source": [
        "# TODO: Check the onehot representation of the first sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task: Embedding representation\n",
        "To convert the text data into a numerical format suitable for model training, we use an embedding representation. This involves the following steps:\n",
        "\n",
        "1. **One Hot Encoding**: Convert each word in the corpus to a unique integer using one hot encoding.\n",
        "2. **Padding Sequences**: Ensure all sequences have the same length by padding shorter sequences with zeros.\n",
        "3. **Embedding Layer**: Use an embedding layer in the neural network to convert the integer-encoded words into dense vectors of fixed size. This layer learns the word embeddings during training.\n",
        "\n",
        "The embedding representation helps in capturing the semantic meaning of words and improves the performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtIPmz3mslkf",
        "outputId": "f4aba605-f5ef-4082-a74f-00a65e9dcae1"
      },
      "outputs": [],
      "source": [
        "sent_length=20\n",
        "embedded_docs=0 # TODO: pad the sequences to make them of the same length with post padding.\n",
        "print(embedded_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJm5jGSJsn9r",
        "outputId": "6ce72d48-3092-498f-833e-8f3266a086e9"
      },
      "outputs": [],
      "source": [
        "# TODO: Check the length of the first sentence after padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzS8KznNsl9g",
        "outputId": "0a46b02c-b18e-455b-d53a-2f0b38cfbc59"
      },
      "outputs": [],
      "source": [
        "# TODO: Check the first sentence after padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "Waj8YYtbsqD4",
        "outputId": "cf8aa60b-ce18-4a9d-e7dd-1285bbee81a6"
      },
      "outputs": [],
      "source": [
        "## Creating model\n",
        "# each and every word is going to get converted into a vector of 40 size\n",
        "embedding_vector_features=40 ##features representation\n",
        "model=0# TODO : Create a sequential model\n",
        "\n",
        "# embedding layer\n",
        "model.add() # TODO : Add the embedding layer with vocabulary size, embedding vector features and input length with sent_length\n",
        "\n",
        "# LSTM-100 NEURONS\n",
        "model.add(LSTM(100))\n",
        "\n",
        "# Sigmoid for binary prediction in model\n",
        "model.add() # TODO: Add a dense layer with 1 neuron and sigmoid activation function\n",
        "\n",
        "model.compile() # TODO : Compile the model with binary crossentropy loss function, adam optimizer and accuracy as the metric\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "lgmVVsrGszmc",
        "outputId": "eecfa61b-4ed1-4edd-b07f-95000cd48d18"
      },
      "outputs": [],
      "source": [
        "# Assuming voc_size and sent_length are predefined variables\n",
        "embedding_vector_features = 40  # Size of the embedding vector\n",
        "\n",
        "model = 0# TODO : Create a sequential model\n",
        "\n",
        "# Embedding layer with correct input_dim (voc_size) and without deprecated input_length\n",
        "model.add() # TODO : Add the embedding layer with vocabulary size, embedding vector features and input length with sent_length\n",
        "\n",
        "# LSTM layer\n",
        "model.add(LSTM(100))\n",
        "\n",
        "# Dense layer with sigmoid activation for binary classification\n",
        "model.add() # TODO: Add a dense layer with 1 neuron and sigmoid activation function\n",
        "\n",
        "# Compile the model\n",
        "model.compile() # TODO : Compile the model with binary crossentropy loss function, adam optimizer and accuracy as the metric\n",
        "\n",
        "# Display the model summary\n",
        "print(model.summary())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "2tDw6uWNuSyS"
      },
      "outputs": [],
      "source": [
        "model.build() # TODO: Build the model with input shape as None and sent_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "VAnYfDA0vrDu",
        "outputId": "092c3031-4f1a-4635-95eb-deef1d11eb28"
      },
      "outputs": [],
      "source": [
        "# Dummy data: batch size of 1, sentence length of sent_length\n",
        "dummy_input = np.random.randint(0, voc_size, (1, sent_length))\n",
        "model.predict(dummy_input)\n",
        "\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Embedding Layer\n",
        "- 20: This is the input length or sequence length, which represents the number of words in each input sequence.\n",
        "- 40: This is the embedding dimension size (embedding_vector_features), which is the size of each word's embedding vector.\n",
        "- This is the total number of parameters in the Embedding layer.\n",
        "- Calculated as voc_size * embedding_vector_features = 5000 * 40 = 200,000.\n",
        "\n",
        "### LSTM Layer\n",
        "- None: Again, the batch size is flexible.\n",
        "- 100: This is the number of LSTM units (neurons) in the layer.\n",
        "- This is the total number of parameters in the LSTM layer.\n",
        "- The LSTM parameters include:\n",
        "- 4 * [(embedding_vector_features + LSTM_units) * LSTM_units + LSTM_units]\n",
        "- Specifically: 4 * [(40 + 100) * 100 + 100] = 4 * [140 * 100 + 100] = 4 * [14,000 + 100] = 4 * 14,100 = 56,400.\n",
        "- These parameters include the weights for input, forget, cell, and output gates in the LSTM.\n",
        "\n",
        "### Dense Layer\n",
        "- Output Shape: (None, 1)\n",
        "- None: Again, the batch size is flexible.\n",
        "- 1: This is the output size, which is 1 because the model is set up for binary classification (predicting one of two classes).\n",
        "- Param # (101):\n",
        "- This is the total number of parameters in the Dense layer.\n",
        "- Calculated as LSTM_units + 1 = 100 + 1 = 101.\n",
        "\n",
        "All the parameters in the model are trainable, meaning they will be updated during training to minimize the loss.\n",
        "Non-trainable params: 0\n",
        "\n",
        "There are no non-trainable parameters in this model. Non-trainable parameters might exist in models with layers like Batch Normalization where some parameters are not updated during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "xgC13C3RwK5v"
      },
      "outputs": [],
      "source": [
        "X_final=np.array(embedded_docs)\n",
        "y_final=np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLJIw7_0wSJ3",
        "outputId": "3d1aaffb-5d93-4fcc-c180-67d0c26c4a8d"
      },
      "outputs": [],
      "source": [
        "# TODO: Check the shape of the final dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Ck8XbpKCwTqz"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split() # TODO: Split the dataset into training and testing sets with 33% data for testing and random state as 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htTX2__4wWQ4",
        "outputId": "f669d7d7-a479-4e0e-a7c4-77f61754c495"
      },
      "outputs": [],
      "source": [
        "### Finally Training\n",
        "model.fit() # TODO : Fit the model with training data, validation data, epochs as 10 and batch size as 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task: Performance Metrics & Accuracy\n",
        "To evaluate the performance of our trained model, we will use the following metrics:\n",
        "\n",
        "1. **Confusion Matrix**: This will help us understand the number of true positives, true negatives, false positives, and false negatives.\n",
        "2. **Accuracy Score**: This will give us the overall accuracy of the model.\n",
        "3. **Classification Report**: This will provide precision, recall, f1-score, and support for each class.\n",
        "\n",
        "We will use the test data to make predictions and then calculate these metrics to assess the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnyPec37wsC0",
        "outputId": "ccac3ace-c471-461d-a291-c3b4b06dadb6"
      },
      "outputs": [],
      "source": [
        "y_pred=model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "waDJaW-Ewtok"
      },
      "outputs": [],
      "source": [
        "y_pred=np.where(y_pred > 0.5, 1,0) ##AUC ROC Curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "vrPIRCb2ww5W"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yFhW3fFwyhh",
        "outputId": "8482c802-05ba-4e26-9bd4-0a1f60dee3bf"
      },
      "outputs": [],
      "source": [
        "#TODO: Print the confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSYvwa-yw0el",
        "outputId": "66e935a0-7fd5-4a4a-b159-393bbc14445d"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "# TODO: Print the accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiuhjGTgw13D",
        "outputId": "224409ef-fed9-4a24-bc0a-94d219ac87d5"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "# TODO: Print the classification report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bidirectional LSTM RNN\n",
        "In the upcoming cells, we will be building and training a Bidirectional LSTM model for sentiment analysis. The steps include:\n",
        "\n",
        "1. **Importing Bidirectional Layer**: Import the Bidirectional layer from TensorFlow Keras.\n",
        "2. **Creating the Model**: Define a Sequential model and add the necessary layers, including the Embedding layer and a Bidirectional LSTM layer.\n",
        "3. **Compiling the Model**: Compile the model with appropriate loss function, optimizer, and metrics.\n",
        "4. **Building the Model**: Build the model with the specified input shape.\n",
        "5. **Training the Model**: Train the model using the training data.\n",
        "6. **Evaluating the Model**: Evaluate the model's performance using confusion matrix, accuracy score, and classification report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "FlIXKjEzkOyY"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Bidirectional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "BeZihZ51kSrm",
        "outputId": "b8732935-9603-4a08-bab3-bc81a0395371"
      },
      "outputs": [],
      "source": [
        "embedding_vector_features=40 ##features representation\n",
        "model=0 # TODO : Create a sequential model\n",
        "\n",
        "# embedding layer\n",
        "model.add() # TODO : Add the embedding layer with vocabulary size, embedding vector features and input length with sent_length\n",
        "\n",
        "# LSTM NEURONS\n",
        "model.add() # TODO : Add a Bidirectional LSTM layer with 200 neurons\n",
        "\n",
        "# Sigmoid for binary prediction in model\n",
        "model.add() # TODO: Add a dense layer with 1 neuron and sigmoid activation function\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "vO4Km-Rskr5Y"
      },
      "outputs": [],
      "source": [
        "model.build(input_shape=(None, sent_length))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "wbhbOfyxkyVN",
        "outputId": "96da1b6f-d0d3-4074-8afa-9c8c31ff398f"
      },
      "outputs": [],
      "source": [
        "# Dummy data: batch size of 1, sentence length of sent_length\n",
        "dummy_input = 0 # TODO: Create a dummy input with random integers between 0 and voc_size with shape (1, sent_length)\n",
        "model.predict(dummy_input)\n",
        "\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "cL3BHqFxk2yF"
      },
      "outputs": [],
      "source": [
        "X_final=np.array(embedded_docs)\n",
        "y_final=np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "xirQTFa6lCmE"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split() # TODO: Split the data into training and testing data with 33% as the test data and random state as 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task: Model Training\n",
        "To train our Bidirectional LSTM model, we will follow these steps:\n",
        "\n",
        "1. **Training the Model**: Use the training data to train the Bidirectional LSTM model. We will specify the number of epochs and batch size.\n",
        "2. **Making Predictions**: Use the trained model to make predictions on the test data.\n",
        "3. **Evaluating the Model**: Evaluate the model's performance using confusion matrix, accuracy score, and classification report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q40jjvqqlLcc",
        "outputId": "67c743d3-eb1a-4d68-f14c-abe37d7b6f0d"
      },
      "outputs": [],
      "source": [
        "### Training\n",
        "model.fit() # TODO: Train the model with X_train and y_train with 10 epochs and batch size of 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Bxr0Lz3lTvg",
        "outputId": "ff619851-a6ca-4509-da66-159793f19131"
      },
      "outputs": [],
      "source": [
        "# Performance Metrics & Accuracy\n",
        "y_pred=model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqyopfJVlU1h",
        "outputId": "11c18b66-586f-42f2-fb60-a9f96e17cc72"
      },
      "outputs": [],
      "source": [
        "y_pred=np.where(y_pred > 0.5, 1,0) ##AUC ROC Curve\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "073SknRFljdx",
        "outputId": "3d815376-4fa0-4985-9fd3-824dd466df09"
      },
      "outputs": [],
      "source": [
        "# TODO: Print the confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCMCeMgBlm7C",
        "outputId": "6f98776d-95ea-4977-ea39-7adb43f59c74"
      },
      "outputs": [],
      "source": [
        "# TODO: Print the accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpMvWEUMluCe",
        "outputId": "1ef4b9dc-466c-4309-fc7d-d313838bf78d"
      },
      "outputs": [],
      "source": [
        "# TODO: Print the classification report"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyObM+FYoLKZicPsnX78EdKr",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
