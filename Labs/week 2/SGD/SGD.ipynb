{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create our training data of 30 pairs {x_i, y_i}\n",
    "# We'll try to fit the Gabor model to these data\n",
    "data = np.array([[-1.920e+00,-1.422e+01,1.490e+00,-1.940e+00,-2.389e+00,-5.090e+00,\n",
    "                 -8.861e+00,3.578e+00,-6.010e+00,-6.995e+00,3.634e+00,8.743e-01,\n",
    "                 -1.096e+01,4.073e-01,-9.467e+00,8.560e+00,1.062e+01,-1.729e-01,\n",
    "                  1.040e+01,-1.261e+01,1.574e-01,-1.304e+01,-2.156e+00,-1.210e+01,\n",
    "                 -1.119e+01,2.902e+00,-8.220e+00,-1.179e+01,-8.391e+00,-4.505e+00],\n",
    "                  [-1.051e+00,-2.482e-02,8.896e-01,-4.943e-01,-9.371e-01,4.306e-01,\n",
    "                  9.577e-03,-7.944e-02 ,1.624e-01,-2.682e-01,-3.129e-01,8.303e-01,\n",
    "                  -2.365e-02,5.098e-01,-2.777e-01,3.367e-01,1.927e-01,-2.222e-01,\n",
    "                  6.352e-02,6.888e-03,3.224e-02,1.091e-02,-5.706e-01,-5.258e-02,\n",
    "                  -3.666e-02,1.709e-01,-4.805e-02,2.008e-01,-1.904e-01,5.952e-01]])\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gabor Model Components:\n",
    "\n",
    "The model consists of two components:\n",
    "\n",
    "1. **Sine Component**:\n",
    "   $$\n",
    "   \\sin(\\theta_0 + 0.06 \\cdot \\theta_1 \\cdot x)\n",
    "   $$\n",
    "\n",
    "2. **Gaussian Component**:\n",
    "   $$\n",
    "   \\exp\\left(-\\frac{(\\theta_0 + 0.06 \\cdot \\theta_1 \\cdot x)^2}{32}\\right)\n",
    "   $$\n",
    "\n",
    "The final predicted value ($ \\ y_{\\text{pred}} \\ $) is the product of these two components:\n",
    "$$\n",
    "y_{\\text{pred}} = \\sin(\\theta_0 + 0.06 \\cdot \\theta_1 \\cdot x) \\cdot \\exp\\left(-\\frac{(\\theta_0 + 0.06 \\cdot \\theta_1 \\cdot x)^2}{32}\\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(theta, x):\n",
    "    sin_component = np.sin(theta[0] + 0.06 * theta[1] * x)\n",
    "    gauss_component = np.exp(-(theta[0] + 0.06 * theta[1] * x) ** 2 / 32)\n",
    "    y_pred = sin_component * gauss_component\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw model\n",
    "def draw_model(data, model, theta, title=None):\n",
    "    \"\"\"\n",
    "    Draws the model predictions along with the data points.\n",
    "\n",
    "    Parameters:\n",
    "    - data: The dataset (x and y values).\n",
    "    - model: The model used to make predictions.\n",
    "    - theta: The current parameter values (intercept and slope).\n",
    "    - title: Optional title for the plot.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate x values for the model\n",
    "    x_model = np.arange(-15, 15, 0.1)\n",
    "    \n",
    "    # Compute y values using the model and current theta\n",
    "    y_model = model(theta, x_model)\n",
    "\n",
    "    # Create a plot\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # Plot the actual data points\n",
    "    ax.plot(data[0, :], data[1, :], 'bo', label='Data')\n",
    "    \n",
    "    # Plot the model prediction line\n",
    "    ax.plot(x_model, y_model, 'm-', label='Model')\n",
    "\n",
    "    # Set plot limits and labels\n",
    "    ax.set_xlim([-15, 15])\n",
    "    ax.set_ylim([-1, 1])\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "\n",
    "    # Set title if provided\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the parameters and draw the model\n",
    "theta = np.zeros((2, 1))\n",
    "theta[0] = -5      # Horizontal offset\n",
    "theta[1] = 25      # Frequency\n",
    "\n",
    "# Draw the model with initial parameters\n",
    "draw_model(data, model, theta, \"Initial parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the sum of squares loss for the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error (MSE) Loss Function\n",
    "\n",
    "The **Mean Squared Error (MSE)** is a commonly used loss function in regression problems. It measures the average of the squared differences between the predicted values and the actual values.\n",
    "\n",
    "#### Steps to Compute MSE:\n",
    "\n",
    "1. **Model Predictions**: Use the model function to compute predictions for the input data ($ \\ \\text{data\\_x} \\ $) using the current parameters $ \\ \\theta \\ $.\n",
    "2. **Squared Differences**: Compute the squared difference between the predicted values ($ \\ y_{\\text{pred}} \\ $) and the actual values ( $ \\ y_{\\text{true}} \\ $).\n",
    "3. **Sum of Squared Differences**: Sum all these squared differences to get the total loss.\n",
    "\n",
    "#### MSE Formula:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\frac{1}{N} \\sum_{i=1}^{N} \\left( y_{\\text{pred}}^{(i)} - y_{\\text{true}}^{(i)} \\right)^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\ y_{\\text{pred}} \\ $ is the predicted value from the model.\n",
    "- $ \\ y_{\\text{true}} \\ $ is the actual value from the dataset.\n",
    "- $ \\ N \\ $ is the number of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(data_x, data_y, model, theta):\n",
    "  # TODO -- Write this function -- replace the line below\n",
    "  # TODO -- First make model predictions from data x\n",
    "  # TODO -- Then compute the squared difference between the predictions and true y values\n",
    "  # TODO -- Then sum them all and return\n",
    "  loss = 0\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just test that we got that right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = compute_loss(data[0,:],data[1,:],model,np.array([[0.6],[-0.2]]))\n",
    "print('Your loss = %3.3f, Correct loss = %3.3f'%(loss, 16.419))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the whole loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_loss_function(compute_loss, data, model, theta_iters=None):\n",
    "    \"\"\"\n",
    "    Draws the loss function landscape and optionally plots the trajectory of parameter updates.\n",
    "    \n",
    "    Parameters:\n",
    "    - compute_loss: Function to compute the loss.\n",
    "    - data: The dataset (x and y values).\n",
    "    - model: The model used to make predictions.\n",
    "    - theta_iters: Optional, a history of parameter updates (theta values) over iterations.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define a colormap\n",
    "    my_colormap_vals_hex = ('2a0902', '2b0a03', '2c0b04', '2d0c05', '2e0c06', '2f0d07', '300d08', '310e09', \n",
    "                            # Add more color values as needed...\n",
    "                           )\n",
    "    \n",
    "    my_colormap_vals_dec = np.array([int(element, base=16) for element in my_colormap_vals_hex])\n",
    "    r = np.floor(my_colormap_vals_dec / (256 * 256))\n",
    "    g = np.floor((my_colormap_vals_dec - r * 256 * 256) / 256)\n",
    "    b = np.floor(my_colormap_vals_dec - r * 256 * 256 - g * 256)\n",
    "    my_colormap = ListedColormap(np.vstack((r, g, b)).transpose() / 255.0)\n",
    "\n",
    "    # Make grid of offset/frequency values to plot\n",
    "    offsets_mesh, freqs_mesh = np.meshgrid(np.arange(-10, 10.0, 0.1), np.arange(2.5, 22.5, 0.1))\n",
    "    loss_mesh = np.zeros_like(freqs_mesh)\n",
    "\n",
    "    # Compute loss for every set of parameters\n",
    "    for idslope, slope in np.ndenumerate(freqs_mesh):\n",
    "        loss_mesh[idslope] = compute_loss(data[0, :], data[1, :], model,\n",
    "                                          np.array([[offsets_mesh[idslope]], [slope]]))\n",
    "\n",
    "    # Create plot\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(8, 8)\n",
    "    \n",
    "    # Plot the loss landscape using a contour plot\n",
    "    ax.contourf(offsets_mesh, freqs_mesh, loss_mesh, 256, cmap=my_colormap)\n",
    "    \n",
    "    # Add contour lines for better visualization\n",
    "    ax.contour(offsets_mesh, freqs_mesh, loss_mesh, 20, colors=['#80808080'])\n",
    "\n",
    "    # If theta_iters is provided (trajectory of theta values), plot them\n",
    "    if theta_iters is not None:\n",
    "        ax.plot(theta_iters[0, :], theta_iters[1, :], 'go-', label='Theta Trajectory')\n",
    "\n",
    "    # Set axis limits and labels\n",
    "    ax.set_ylim([2.5, 22.5])\n",
    "    ax.set_xlabel('Offset')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_loss_function(compute_loss, data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These came from writing out the expression for the sum of squares loss and taking the\n",
    "# derivative with respect to theta0 and theta1. It was a lot of hassle to get it right!\n",
    "# Derivative of the Gabor function with respect to theta_0 (offset)\n",
    "def gabor_deriv_theta0(data_x, data_y, theta0, theta1):\n",
    "    \"\"\"\n",
    "    Computes the derivative of the Gabor function with respect to theta_0 (offset).\n",
    "    \n",
    "    Parameters:\n",
    "    - data_x: Input feature data (independent variable).\n",
    "    - data_y: True output data (dependent variable).\n",
    "    - theta0: The current value of theta_0 (offset).\n",
    "    - theta1: The current value of theta_1 (frequency).\n",
    "    \n",
    "    Returns:\n",
    "    - The sum of the derivatives with respect to theta_0.\n",
    "    \"\"\"\n",
    "    x = 0.06 * theta1 * data_x + theta0\n",
    "    y = data_y\n",
    "    cos_component = np.cos(x)\n",
    "    sin_component = np.sin(x)\n",
    "    gauss_component = np.exp(-0.5 * x * x / 16)\n",
    "    \n",
    "    deriv = cos_component * gauss_component - sin_component * gauss_component * x / 16\n",
    "    deriv = 2 * deriv * (sin_component * gauss_component - y)\n",
    "    \n",
    "    return np.sum(deriv)\n",
    "\n",
    "# Derivative of the Gabor function with respect to theta_1 (frequency)\n",
    "def gabor_deriv_theta1(data_x, data_y, theta0, theta1):\n",
    "    \"\"\"\n",
    "    Computes the derivative of the Gabor function with respect to theta_1 (frequency).\n",
    "    \n",
    "    Parameters:\n",
    "    - data_x: Input feature data (independent variable).\n",
    "    - data_y: True output data (dependent variable).\n",
    "    - theta0: The current value of theta_0 (offset).\n",
    "    - theta1: The current value of theta_1 (frequency).\n",
    "    \n",
    "    Returns:\n",
    "    - The sum of the derivatives with respect to theta_1.\n",
    "    \"\"\"\n",
    "    x = 0.06 * theta1 * data_x + theta0\n",
    "    y = data_y\n",
    "    cos_component = np.cos(x)\n",
    "    sin_component = np.sin(x)\n",
    "    gauss_component = np.exp(-0.5 * x * x / 16)\n",
    "    \n",
    "    deriv = 0.06 * data_x * cos_component * gauss_component - 0.06 * data_x * sin_component * gauss_component * x / 16\n",
    "    deriv = 2 * deriv * (sin_component * gauss_component - y)\n",
    "    \n",
    "    return np.sum(deriv)\n",
    "\n",
    "# Compute gradient for both parameters (theta_0 and theta_1)\n",
    "def compute_gradient(data_x, data_y, theta):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the loss function with respect to both parameters: \n",
    "    theta_0 (offset) and theta_1 (frequency).\n",
    "    \n",
    "    Parameters:\n",
    "    - data_x: Input feature data (independent variable).\n",
    "    - data_y: True output data (dependent variable).\n",
    "    - theta: Current parameter values [theta_0, theta_1].\n",
    "    \n",
    "    Returns:\n",
    "    - Gradient as a numpy array [[dl_dtheta0], [dl_dtheta1]].\n",
    "    \"\"\"\n",
    "    \n",
    "    dl_dtheta0 = gabor_deriv_theta0(data_x, data_y, theta[0], theta[1])\n",
    "    dl_dtheta1 = gabor_deriv_theta1(data_x, data_y, theta[0], theta[1])\n",
    "    \n",
    "    # Return the gradient as a column vector\n",
    "    return np.array([[dl_dtheta0], [dl_dtheta1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the gradient using your function\n",
    "gradient = compute_gradient(data[0, :], data[1, :], theta)\n",
    "print(\"Your gradients: (%3.3f, %3.3f)\" % (gradient[0], gradient[1]))\n",
    "\n",
    "# Approximate the gradients with finite differences\n",
    "delta = 0.0001\n",
    "\n",
    "# Approximate gradient for theta_0 (offset)\n",
    "dl_dtheta0_est = (compute_loss(data[0, :], data[1, :], model, theta + np.array([[delta], [0]])) - \\\n",
    "                  compute_loss(data[0, :], data[1, :], model, theta)) / delta\n",
    "\n",
    "# Approximate gradient for theta_1 (frequency)\n",
    "dl_dtheta1_est = (compute_loss(data[0, :], data[1, :], model, theta + np.array([[0], [delta]])) - \\\n",
    "                  compute_loss(data[0, :], data[1, :], model, theta)) / delta\n",
    "\n",
    "print(\"Approx gradients: (%3.3f, %3.3f)\" % (dl_dtheta0_est, dl_dtheta1_est))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_1D(dist_prop, data, model, theta_start, search_direction):\n",
    "    \"\"\"\n",
    "    Computes the loss after moving a certain distance along the search direction.\n",
    "    \n",
    "    Parameters:\n",
    "    - dist_prop: The proportion of the search direction to move.\n",
    "    - data: The dataset (x and y values).\n",
    "    - model: The model used to make predictions.\n",
    "    - theta_start: The starting point of theta (parameters).\n",
    "    - search_direction: The direction along which to search for the minimum loss.\n",
    "    \n",
    "    Returns:\n",
    "    - The computed loss after moving along the search direction.\n",
    "    \"\"\"\n",
    "    return compute_loss(data[0,:], data[1,:], model, theta_start + search_direction * dist_prop)\n",
    "\n",
    "def line_search(data, model, theta, gradient, thresh=0.00001, max_dist=0.1, max_iter=15, verbose=False):\n",
    "    \"\"\"\n",
    "    Performs a line search to find the optimal step size along the gradient direction.\n",
    "\n",
    "    Parameters:\n",
    "    - data: The dataset (x and y values).\n",
    "    - model: The model used to make predictions.\n",
    "    - theta: Current parameters (intercept and slope).\n",
    "    - gradient: The gradient vector (direction of steepest descent).\n",
    "    - thresh: Threshold for stopping criteria based on distance between points.\n",
    "    - max_dist: Maximum distance to search along the gradient direction.\n",
    "    - max_iter: Maximum number of iterations for line search.\n",
    "    - verbose: If True, prints intermediate steps.\n",
    "\n",
    "    Returns:\n",
    "    - The optimal step size found by the line search.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize four points along the range we are going to search\n",
    "    a = 0\n",
    "    b = 0.33 * max_dist\n",
    "    c = 0.66 * max_dist\n",
    "    d = 1.0 * max_dist\n",
    "    n_iter = 0\n",
    "\n",
    "    # While we haven't found the minimum closely enough\n",
    "    while np.abs(b - c) > thresh and n_iter < max_iter:\n",
    "        # Increment iteration counter (just to prevent an infinite loop)\n",
    "        n_iter += 1\n",
    "        \n",
    "        # Calculate all four points\n",
    "        lossa = loss_function_1D(a, data, model, theta, gradient)\n",
    "        lossb = loss_function_1D(b, data, model, theta, gradient)\n",
    "        lossc = loss_function_1D(c, data, model, theta, gradient)\n",
    "        lossd = loss_function_1D(d, data, model, theta, gradient)\n",
    "\n",
    "        if verbose:\n",
    "            print(f'Iter {n_iter}, a={a:.3f}, b={b:.3f}, c={c:.3f}, d={d:.3f}')\n",
    "            print(f'a {lossa:.6f}, b {lossb:.6f}, c {lossc:.6f}, d {lossd:.6f}')\n",
    "\n",
    "        # Rule #1 If point A is less than points B, C, and D then halve distance from A to points B,C,D\n",
    "        if np.argmin((lossa, lossb, lossc, lossd)) == 0:\n",
    "            b = a + (b - a) / 2\n",
    "            c = a + (c - a) / 2\n",
    "            d = a + (d - a) / 2\n",
    "            continue\n",
    "\n",
    "        # Rule #2 If point B is less than point C then:\n",
    "        #                     D becomes C,\n",
    "        #                     B becomes 1/3 between A and new D,\n",
    "        #                     C becomes 2/3 between A and new D\n",
    "        if lossb < lossc:\n",
    "            d = c\n",
    "            b = a + (d - a) / 3\n",
    "            c = a + 2 * (d - a) / 3\n",
    "            continue\n",
    "\n",
    "        # Rule #3 If point C is less than point B then:\n",
    "        #                     A becomes B,\n",
    "        #                     B becomes 1/3 between new A and D,\n",
    "        #                     C becomes 2/3 between new A and D\n",
    "        a = b\n",
    "        b = a + (d - a) / 3\n",
    "        c = a + 2 * (d - a) / 3\n",
    "\n",
    "    # Return average of two middle points as optimal step size\n",
    "    return (b + c) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_step(theta, data, model):\n",
    "    \"\"\"\n",
    "    Performs one step of gradient descent.\n",
    "\n",
    "    Parameters:\n",
    "    - theta: Current parameters (offset and frequency).\n",
    "    - data: The dataset (x and y values).\n",
    "    - model: The model used to make predictions.\n",
    "\n",
    "    Returns:\n",
    "    - Updated theta after one gradient descent step.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Compute the gradient\n",
    "    gradient = compute_gradient(data[0, :], data[1, :], theta)\n",
    "\n",
    "    # Step 2: Update the parameters -- note we want to search in the negative (downhill) direction\n",
    "    alpha = line_search(data, model, theta, gradient * -1, max_dist=2.0)\n",
    "    \n",
    "    # Step 3: Update theta by moving in the negative gradient direction\n",
    "    theta = theta - alpha * gradient\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the parameters\n",
    "n_steps = 21\n",
    "theta_all = np.zeros((2, n_steps + 1))\n",
    "theta_all[0, 0] = -1.5  # Initial offset\n",
    "theta_all[1, 0] = 8.5   # Initial frequency\n",
    "\n",
    "# Measure loss and draw initial model\n",
    "loss = compute_loss(data[0, :], data[1, :], model, theta_all[:, 0:1])\n",
    "draw_model(data, model, theta_all[:, 0:1], \"Initial parameters, Loss = %f\" % (loss))\n",
    "\n",
    "# Perform gradient descent steps\n",
    "for c_step in range(n_steps):\n",
    "    # Do gradient descent step\n",
    "    theta_all[:, c_step + 1:c_step + 2] = gradient_descent_step(theta_all[:, c_step:c_step + 1], data, model)\n",
    "    \n",
    "    # Measure loss and draw model every 4th step\n",
    "    if c_step % 4 == 0:\n",
    "        loss = compute_loss(data[0, :], data[1, :], model, theta_all[:, c_step + 1:c_step + 2])\n",
    "        draw_model(data, model, theta_all[:, c_step + 1], \"Iteration %d, loss = %f\" % (c_step + 1, loss))\n",
    "\n",
    "# Draw the trajectory of parameter updates on the loss function landscape\n",
    "draw_loss_function(compute_loss, data, model, theta_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Experiment with starting the optimization in the previous cell in different places\n",
    "# and show that it heads to a local minimum if we don't start it in the right valley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_step_fixed_learning_rate(theta, data, alpha):\n",
    "  # TODO -- fill in this routine so that we take a fixed size step of size alpha without using line search\n",
    "\n",
    "  return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the parameters\n",
    "n_steps = 21\n",
    "theta_all = np.zeros((2, n_steps + 1))\n",
    "theta_all[0, 0] = -1.5  # Initial offset\n",
    "theta_all[1, 0] = 8.5   # Initial frequency\n",
    "\n",
    "# Measure loss and draw initial model\n",
    "loss = compute_loss(data[0, :], data[1, :], model, theta_all[:, 0:1])\n",
    "draw_model(data, model, theta_all[:, 0:1], \"Initial parameters, Loss = %f\" % (loss))\n",
    "\n",
    "# Perform gradient descent steps with a fixed learning rate\n",
    "alpha = 0.2  # Fixed learning rate\n",
    "for c_step in range(n_steps):\n",
    "    # Do gradient descent step with fixed learning rate\n",
    "    theta_all[:, c_step + 1:c_step + 2] = gradient_descent_step_fixed_learning_rate(\n",
    "        theta_all[:, c_step:c_step + 1], data, model, alpha=alpha)\n",
    "    \n",
    "    # Measure loss and draw model every 4th step\n",
    "    if c_step % 4 == 0:\n",
    "        loss = compute_loss(data[0, :], data[1, :], model, theta_all[:, c_step + 1:c_step + 2])\n",
    "        draw_model(data, model, theta_all[:, c_step + 1], \"Iteration %d, loss = %f\" % (c_step + 1, loss))\n",
    "\n",
    "# Draw the trajectory of parameter updates on the loss function landscape\n",
    "draw_loss_function(compute_loss, data, model, theta_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Experiment with the learning rate, alpha.\n",
    "# What happens if you set it too large?\n",
    "# What happens if you set it too small?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent_step(theta, data, alpha, batch_size):\n",
    "  # TODO -- fill in this routine so that we take a fixed size step of size alpha but only using a subset (batch) of the data\n",
    "  # at each step\n",
    "  # You can use the function np.random.permutation to generate a random permutation of the n_data = data.shape[1] indices\n",
    "  # and then just choose the first n=batch_size of these indices.  Then compute the gradient update\n",
    "  # from just the data with these indices.   More properly, you should sample without replacement, but this will do for now.\n",
    "\n",
    "\n",
    "  return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random number generator so you always get same numbers (disable if you don't want this)\n",
    "np.random.seed(1)\n",
    "\n",
    "# Initialize the parameters\n",
    "n_steps = 41\n",
    "theta_all = np.zeros((2, n_steps + 1))\n",
    "theta_all[0, 0] = 3.5  # Initial offset\n",
    "theta_all[1, 0] = 6.5  # Initial frequency\n",
    "\n",
    "# Measure loss and draw initial model\n",
    "loss = compute_loss(data[0, :], data[1, :], model, theta_all[:, 0:1])\n",
    "draw_model(data, model, theta_all[:, 0:1], \"Initial parameters, Loss = %f\" % (loss))\n",
    "\n",
    "# Perform stochastic gradient descent steps\n",
    "for c_step in range(n_steps):\n",
    "    # Do stochastic gradient descent step with fixed learning rate and batch size\n",
    "    theta_all[:, c_step + 1:c_step + 2] = stochastic_gradient_descent_step(\n",
    "        theta_all[:, c_step:c_step + 1], data, model, alpha=0.8, batch_size=5)\n",
    "    \n",
    "    # Measure loss and draw model every 8th step\n",
    "    if c_step % 8 == 0:\n",
    "        loss = compute_loss(data[0, :], data[1, :], model, theta_all[:, c_step + 1:c_step + 2])\n",
    "        draw_model(data, model, theta_all[:, c_step + 1], \"Iteration %d, loss = %f\" % (c_step + 1, loss))\n",
    "\n",
    "# Draw the trajectory of parameter updates on the loss function landscape\n",
    "draw_loss_function(compute_loss, data, model, theta_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO -- Experiment with different learning rates, starting points, batch sizes, number of steps.  Get a feel for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO -- Add a learning rate schedule.  Reduce the learning rate by a factor of beta every M iterations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
