{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Gradient Descent in NLP\n",
    "\n",
    "Gradient Descent is an optimization algorithm used to minimize a cost function in machine learning models, particularly in NLP tasks. It works by iteratively adjusting model parameters in the direction that reduces the cost function the most.\n",
    "\n",
    "The general update rule for Gradient Descent is:\n",
    "\n",
    "$$\n",
    "\\theta^{(t+1)} = \\theta^{(t)} - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta^{(t)}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- The current parameter value is:\n",
    "  $\n",
    "  \\theta^{(t)}\n",
    "  $\n",
    "\n",
    "- The learning rate is:\n",
    "  $\n",
    "  \\alpha\n",
    "  $\n",
    "\n",
    "- The gradient of the cost function with respect to the parameters is:\n",
    "  $\n",
    "  \\frac{\\partial J(\\theta)}{\\partial \\theta^{(t)}}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create our training data 12 pairs {x_i, y_i}\n",
    "# We'll try to fit the straight line model to these data\n",
    "data = np.array([[0.03,0.19,0.34,0.46,0.78,0.81,1.08,1.18,1.39,1.60,1.65,1.90],\n",
    "                 [0.67,0.85,1.05,1.00,1.40,1.50,1.30,1.54,1.55,1.68,1.73,1.60]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define our model -- just a straight line with intercept theta[0] and slope theta[1]\n",
    "def model(theta, x):\n",
    "    y_pred = theta[0] + theta[1] * x\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw model\n",
    "def draw_model(data, model, theta, title=None):\n",
    "    x_model = np.arange(0, 2, 0.01)\n",
    "    y_model = model(theta, x_model)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(data[0, :], data[1, :], 'bo')  # Plot the data points\n",
    "    ax.plot(x_model, y_model, 'm-')        # Plot the model (line)\n",
    "    \n",
    "    ax.set_xlim([0, 2])\n",
    "    ax.set_ylim([0, 2])\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How It Works:\n",
    "- The x_model values are generated from 0 to 2 with small increments (0.01).\n",
    "- The y_model values are computed using the model function and the current parameters theta.\n",
    "- The data points and the predicted line are plotted using Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the parameters to some arbitrary values and draw the model\n",
    "theta = np.zeros((2, 1))\n",
    "theta[0] = 0.6      # Intercept\n",
    "theta[1] = -0.2     # Slope\n",
    "\n",
    "# Draw the model with initial parameters\n",
    "draw_model(data, model, theta, \"Initial parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the sum of squares loss for the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Squared Error (MSE) Loss Function\n",
    "\n",
    "The **Mean Squared Error (MSE)** is a commonly used loss function in regression problems. It measures the average of the squared differences between the predicted values and the actual values.\n",
    "\n",
    "#### Steps to Compute MSE:\n",
    "\n",
    "1. **Model Predictions**: Use the model function to compute predictions for the input data ($ \\ \\text{data\\_x} \\ $) using the current parameters $ \\ \\theta \\ $.\n",
    "2. **Squared Differences**: Compute the squared difference between the predicted values ($ \\ y_{\\text{pred}} \\ $) and the actual values ( $ \\ y_{\\text{true}} \\ $).\n",
    "3. **Sum of Squared Differences**: Sum all these squared differences to get the total loss.\n",
    "\n",
    "#### MSE Formula:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\frac{1}{N} \\sum_{i=1}^{N} \\left( y_{\\text{pred}}^{(i)} - y_{\\text{true}}^{(i)} \\right)^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\ y_{\\text{pred}} \\ $ is the predicted value from the model.\n",
    "- $ \\ y_{\\text{true}} \\ $ is the actual value from the dataset.\n",
    "- $ \\ N \\ $ is the number of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(data_x, data_y, model, theta):\n",
    "    \"\"\"\n",
    "    Computes Mean Squared Error (MSE) loss between predicted and true values.\n",
    "    \n",
    "    Parameters:\n",
    "    data_x : array-like\n",
    "        Input feature data (independent variable).\n",
    "    data_y : array-like\n",
    "        True output data (dependent variable).\n",
    "    model : function\n",
    "        The model that predicts `y` values based on `x` and `theta`.\n",
    "    theta : array-like\n",
    "        Model parameters (weights).\n",
    "    \n",
    "    Returns:\n",
    "    loss : float\n",
    "        The computed MSE loss.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Make predictions using the model\n",
    "    pred_y = model(theta, data_x)\n",
    "    \n",
    "    # Step 2: Compute squared differences between predictions and actual values\n",
    "    squared_diffs = (pred_y - data_y) ** 2\n",
    "    \n",
    "    # Step 3: Sum all squared differences and return as loss\n",
    "    loss = np.sum(squared_diffs)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just test that we got that right\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = compute_loss(data[0,:],data[1,:],model,np.array([[0.6],[-0.2]]))\n",
    "print('Your loss = %3.3f, Correct loss = %3.3f'%(loss, 12.367))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the whole loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_loss_function(compute_loss, data, model, theta_iters=None):\n",
    "    # Define pretty colormap\n",
    "    my_colormap_vals_hex = ('2a0902', '2b0a03', '2c0b04', '2d0c05', '2e0c06', '2f0d07', '300d08', '310e09', \n",
    "                            '320f0a', '330f0b', '34100b', '35110c', '36110d', '37120e', '38120f', \n",
    "                            # Add more color values as needed...\n",
    "                           )\n",
    "    \n",
    "    my_colormap_vals_dec = np.array([int(element, base=16) for element in my_colormap_vals_hex])\n",
    "    r = np.floor(my_colormap_vals_dec / (256 * 256))\n",
    "    g = np.floor((my_colormap_vals_dec - r * 256 * 256) / 256)\n",
    "    b = np.floor(my_colormap_vals_dec - r * 256 * 256 - g * 256)\n",
    "    my_colormap = ListedColormap(np.vstack((r, g, b)).transpose() / 255.0)\n",
    "\n",
    "    # Make grid of intercept/slope values to plot\n",
    "    intercepts_mesh, slopes_mesh = np.meshgrid(np.arange(0.0, 2.0, 0.02), np.arange(-1.0, 1.0, 0.002))\n",
    "    loss_mesh = np.zeros_like(slopes_mesh)\n",
    "\n",
    "    # Compute loss for every set of parameters\n",
    "    for idslope, slope in np.ndenumerate(slopes_mesh):\n",
    "        loss_mesh[idslope] = compute_loss(data[0, :], data[1, :], model,\n",
    "                                          np.array([[intercepts_mesh[idslope]], [slope]]))\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(8, 8)\n",
    "    \n",
    "    # Contour plot for loss function\n",
    "    ax.contourf(intercepts_mesh, slopes_mesh, loss_mesh, 256, cmap=my_colormap)\n",
    "    ax.contour(intercepts_mesh, slopes_mesh, loss_mesh, 40, colors=['#80808080'])\n",
    "\n",
    "    # Plot theta iterations if provided\n",
    "    if theta_iters is not None:\n",
    "        ax.plot(theta_iters[0, :], theta_iters[1, :], 'go-')\n",
    "\n",
    "    ax.set_ylim([1, -1])\n",
    "    ax.set_xlabel('Intercept')\n",
    "    ax.set_ylabel('Slope')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_loss_function(compute_loss, data, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum of Squares Loss Function:\n",
    "\n",
    "The sum of squares loss function is given by:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\sum_{i=1}^{N} \\left( y_{\\text{pred}}^{(i)} - y_{\\text{true}}^{(i)} \\right)^2\n",
    "$$\n",
    "\n",
    "\n",
    "Where:\n",
    "- $ \\ y_{\\text{pred}}^{(i)} = \\theta_0 + \\theta_1 x^{(i)} \\ $ is the predicted value for the $ \\ i\\ $-th data point.\n",
    "- $ \\ y_{\\text{true}}^{(i)} \\ $ is the actual value for the \\(i\\)-th data point.\n",
    "- $ \\ N \\ $ is the number of data points.\n",
    "\n",
    "---\n",
    "\n",
    "### Gradient Calculation:\n",
    "\n",
    "We need to compute the partial derivatives of the loss function with respect to $\\ \\theta_0 \\ $ and $ \\ \\theta_1 \\ $ (the intercept and slope, respectively).\n",
    "\n",
    "#### Derivative with respect to $ \\ \\theta_0 \\ $ (intercept):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_0} = \\frac{2}{N} \\sum_{i=1}^{N} \\left( y_{\\text{pred}}^{(i)} - y_{\\text{true}}^{(i)} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "#### Derivative with respect to $ \\ \\theta_1 \\ $ (slope):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_1} = \\frac{2}{N} \\sum_{i=1}^{N} \\left( y_{\\text{pred}}^{(i)} - y_{\\text{true}}^{(i)} \\right) x^{(i)}\n",
    "$$\n",
    "\n",
    "\n",
    "Where:\n",
    "- $ \\ y_{\\text{pred}}^{(i)} = \\theta_0 + \\theta_1 x^{(i)} \\ $ is the predicted value for the $ \\ i\\ $-th data point.\n",
    "- $ \\ x^{(i)} \\ $ is the feature value for the $ \\ i\\ $-th data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(data_x, data_y, theta):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the sum of squares loss function with respect to theta (parameters).\n",
    "    \n",
    "    Parameters:\n",
    "    data_x : array-like\n",
    "        Input feature data (independent variable).\n",
    "    data_y : array-like\n",
    "        True output data (dependent variable).\n",
    "    theta : array-like\n",
    "        Model parameters (weights: intercept and slope).\n",
    "    \n",
    "    Returns:\n",
    "    gradient : array-like\n",
    "        The gradient of the loss function with respect to theta_0 and theta_1.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Number of data points\n",
    "    N = len(data_x)\n",
    "    \n",
    "    # Model predictions: y_pred = theta[0] + theta[1] * x\n",
    "    y_pred = theta[0] + theta[1] * data_x\n",
    "    \n",
    "    # Compute gradients\n",
    "    dl_dtheta0 = (2 / N) * np.sum(y_pred - data_y)  # Derivative w.r.t. theta_0 (intercept)\n",
    "    dl_dtheta1 = (2 / N) * np.sum((y_pred - data_y) * data_x)  # Derivative w.r.t. theta_1 (slope)\n",
    "    \n",
    "    # Return the gradient as a column vector\n",
    "    return np.array([[dl_dtheta0], [dl_dtheta1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the gradient using your function\n",
    "gradient = compute_gradient(data[0,:], data[1,:], theta)\n",
    "print(\"Your gradients: (%3.3f, %3.3f)\" % (gradient[0], gradient[1]))\n",
    "\n",
    "# Approximate the gradients with finite differences\n",
    "delta = 0.0001\n",
    "\n",
    "# Compute finite difference approximation for theta_0 (intercept)\n",
    "dl_dtheta0_est = (compute_loss(data[0,:], data[1,:], model, theta + np.array([[delta], [0]])) - \\\n",
    "                  compute_loss(data[0,:], data[1,:], model, theta)) / delta\n",
    "\n",
    "# Compute finite difference approximation for theta_1 (slope)\n",
    "dl_dtheta1_est = (compute_loss(data[0,:], data[1,:], model, theta + np.array([[0], [delta]])) - \\\n",
    "                  compute_loss(data[0,:], data[1,:], model, theta)) / delta\n",
    "\n",
    "print(\"Approx gradients: (%3.3f, %3.3f)\" % (dl_dtheta0_est, dl_dtheta1_est))\n",
    "\n",
    "# There might be small differences in the last significant figure because finite gradients is an approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_1D(dist_prop, data, model, theta_start, search_direction):\n",
    "    \"\"\"\n",
    "    Computes the loss after moving a certain distance along the search direction.\n",
    "    \n",
    "    Parameters:\n",
    "    - dist_prop: The proportion of the search direction to move.\n",
    "    - data: The dataset (x and y values).\n",
    "    - model: The model used to make predictions.\n",
    "    - theta_start: The starting point of theta (parameters).\n",
    "    - search_direction: The direction along which to search for the minimum loss.\n",
    "    \n",
    "    Returns:\n",
    "    - The computed loss after moving along the search direction.\n",
    "    \"\"\"\n",
    "    return compute_loss(data[0,:], data[1,:], model, theta_start + search_direction * dist_prop)\n",
    "\n",
    "def line_search(data, model, theta, gradient, thresh=0.00001, max_dist=0.1, max_iter=15, verbose=False):\n",
    "    \"\"\"\n",
    "    Performs a line search to find the optimal step size along the gradient direction.\n",
    "\n",
    "    Parameters:\n",
    "    - data: The dataset (x and y values).\n",
    "    - model: The model used to make predictions.\n",
    "    - theta: Current parameters (intercept and slope).\n",
    "    - gradient: The gradient vector (direction of steepest descent).\n",
    "    - thresh: Threshold for stopping criteria based on distance between points.\n",
    "    - max_dist: Maximum distance to search along the gradient direction.\n",
    "    - max_iter: Maximum number of iterations for line search.\n",
    "    - verbose: If True, prints intermediate steps.\n",
    "\n",
    "    Returns:\n",
    "    - The optimal step size found by the line search.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize four points along the range we are going to search\n",
    "    a = 0\n",
    "    b = 0.33 * max_dist\n",
    "    c = 0.66 * max_dist\n",
    "    d = 1.0 * max_dist\n",
    "    n_iter = 0\n",
    "\n",
    "    # While we haven't found the minimum closely enough\n",
    "    while np.abs(b - c) > thresh and n_iter < max_iter:\n",
    "        # Increment iteration counter (just to prevent an infinite loop)\n",
    "        n_iter += 1\n",
    "        \n",
    "        # Calculate all four points\n",
    "        lossa = loss_function_1D(a, data, model, theta, gradient)\n",
    "        lossb = loss_function_1D(b, data, model, theta, gradient)\n",
    "        lossc = loss_function_1D(c, data, model, theta, gradient)\n",
    "        lossd = loss_function_1D(d, data, model, theta, gradient)\n",
    "\n",
    "        if verbose:\n",
    "            print(f'Iter {n_iter}, a={a:.3f}, b={b:.3f}, c={c:.3f}, d={d:.3f}')\n",
    "            print(f'a {lossa:.6f}, b {lossb:.6f}, c {lossc:.6f}, d {lossd:.6f}')\n",
    "\n",
    "        # Rule #1 If point A is less than points B, C, and D then halve distance from A to points B,C,D\n",
    "        if np.argmin((lossa, lossb, lossc, lossd)) == 0:\n",
    "            b = a + (b - a) / 2\n",
    "            c = a + (c - a) / 2\n",
    "            d = a + (d - a) / 2\n",
    "            continue\n",
    "\n",
    "        # Rule #2 If point B is less than point C then:\n",
    "        #                     D becomes C,\n",
    "        #                     B becomes 1/3 between A and new D,\n",
    "        #                     C becomes 2/3 between A and new D\n",
    "        if lossb < lossc:\n",
    "            d = c\n",
    "            b = a + (d - a) / 3\n",
    "            c = a + 2 * (d - a) / 3\n",
    "            continue\n",
    "\n",
    "        # Rule #3 If point C is less than point B then:\n",
    "        #                     A becomes B,\n",
    "        #                     B becomes 1/3 between new A and D,\n",
    "        #                     C becomes 2/3 between new A and D\n",
    "        a = b\n",
    "        b = a + (d - a) / 3\n",
    "        c = a + 2 * (d - a) / 3\n",
    "\n",
    "    # Return average of two middle points as optimal step size\n",
    "    return (b + c) / 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_step(theta, data, model):\n",
    "    \"\"\"\n",
    "    Performs one step of gradient descent.\n",
    "\n",
    "    Parameters:\n",
    "    - theta: Current parameters (intercept and slope).\n",
    "    - data: The dataset (x and y values).\n",
    "    - model: The model used to make predictions.\n",
    "\n",
    "    Returns:\n",
    "    - Updated theta after one gradient descent step.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Compute the gradient (you wrote this function above)\n",
    "    gradient = compute_gradient(data[0, :], data[1, :], theta)\n",
    "\n",
    "    # Step 2: Find the best step size alpha using line search function (above)\n",
    "    # Use negative gradient as we are going downhill\n",
    "    alpha = line_search(data, model, theta, -gradient)\n",
    "\n",
    "    # Step 3: Update the parameters theta based on the gradient and step size alpha\n",
    "    theta = theta + alpha * (-gradient)\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the parameters and draw the model\n",
    "n_steps = 10\n",
    "theta_all = np.zeros((2, n_steps + 1))\n",
    "theta_all[0, 0] = 1.6  # Initial intercept\n",
    "theta_all[1, 0] = -0.5  # Initial slope\n",
    "\n",
    "# Measure loss and draw initial model\n",
    "loss = compute_loss(data[0, :], data[1, :], model, theta_all[:, 0:1])\n",
    "draw_model(data, model, theta_all[:, 0:1], \"Initial parameters, Loss = %f\" % (loss))\n",
    "\n",
    "# Repeatedly take gradient descent steps\n",
    "for c_step in range(n_steps):\n",
    "    # Do gradient descent step\n",
    "    theta_all[:, c_step+1:c_step+2] = gradient_descent_step(theta_all[:, c_step:c_step+1], data, model)\n",
    "    \n",
    "    # Measure loss and draw model\n",
    "    loss = compute_loss(data[0, :], data[1, :], model, theta_all[:, c_step+1:c_step+2])\n",
    "    draw_model(data, model, theta_all[:, c_step+1], \"Iteration %d, loss = %f\" % (c_step + 1, loss))\n",
    "\n",
    "# Draw the trajectory on the loss function\n",
    "draw_loss_function(compute_loss, data, model, theta_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
