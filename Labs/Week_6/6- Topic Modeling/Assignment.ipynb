{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling Assignment\n",
    "\n",
    "Topic modeling is a natural language processing technique used to discover hidden themes in a collection of documents. Latent Dirichlet Allocation (LDA) is a popular topic modeling algorithm that assumes each document is a mixture of topics and each topic is a mixture of words, using probabilistic methods to identify these structures. It helps in summarizing, organizing, and exploring large text datasets.\n",
    "\n",
    "In this assigment, we will use LDA on 20 news group dataset from sklearn.\n",
    "\n",
    "The 20 Newsgroups dataset is a popular dataset provided by scikit-learn for text classification and topic modeling tasks. It contains newsgroup posts organized into 20 different categories, making it a valuable resource for experimenting with natural language processing (NLP) techniques.\n",
    "The dataset is divided into 20 topics, including technology, politics, sports, religion, and more. Examples of categories include:\n",
    "- comp.graphics\n",
    "- rec.sport.hockey\n",
    "- sci.space\n",
    "- talk.politics.misc\n",
    "\n",
    "The data consists of newsgroup posts (documents) with associated labels indicating their categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tests import load_datasets_test, preprocess_text_test, check_vectorizer_test, check_train_lda_test\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "\n",
    "[This](https://scikit-learn.org/dev/modules/generated/sklearn.datasets.fetch_20newsgroups.html) is the official scikit documentation on fetch 20 news.\n",
    "\n",
    "TODO:\n",
    "- Load training data from fetch_20newsgroups\n",
    "- Load testing data from fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 20 Newsgroups dataset\n",
    "newsgroups_train = # TODO\n",
    "newsgroups_test = # TODO\n",
    "\n",
    "load_datasets_test(newsgroups_train, newsgroups_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll look at cleaning and pre-processing text here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    tokens = # TODO: Tokenize\n",
    "\n",
    "    tokens = # TODO: lowercase all the tokens\n",
    "\n",
    "    tokens = # TODO: remove stop words\n",
    "\n",
    "    #returns True if all the characters are alphabet letters (a-z)\n",
    "    tokens = [token for token in tokens if token.isalpha()]\n",
    "\n",
    "    tokens = # TODO: lemmatize\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "train_data = [preprocess_text(text) for text in newsgroups_train.data]\n",
    "test_data = [preprocess_text(text) for text in newsgroups_test.data]\n",
    "\n",
    "preprocess_text_test(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "\n",
    "Refer to sklearn documentation on CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = # TODO: CountVectorizer with max_features 10000 and that takes ngrams of 1 and 2, stop words from English\n",
    "train_vectors = # TODO: fit vectorizer on train data\n",
    "\n",
    "check_vectorizer_test(vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA model\n",
    "def train_lda(n_topics, train_vectors):\n",
    "    # TODO: Call LatentDirichletAllocation passing n_topics for components, learning decay of 0.7, random state 42 and number of jobs as -1\n",
    "    # TODO: Fit LDA on train_vectors\n",
    "    return lda\n",
    "\n",
    "\n",
    "\n",
    "# Display top terms for each topic\n",
    "def display_topics(model, feature_names, no_top_words=10):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA with different numbers of topics\n",
    "n_topics_list = [10, 20, 30]\n",
    "lda_models = []\n",
    "for n_topics in n_topics_list:\n",
    "    model = # TODO: Call train_lda for each value in n_topics_list\n",
    "    check_train_lda_test(lda)\n",
    "    lda_models.append(model)\n",
    "\n",
    "# Display topics for each trained LDA model\n",
    "for idx, lda_model in enumerate(lda_models):\n",
    "    print(f\"\\nTop Terms for Model with {n_topics_list[idx]} Topics:\\n\")\n",
    "    display_topics(lda_model, vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coherence score calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute topic coherence using gensim\n",
    "def compute_coherence_score(lda_model, texts, vectorizer):\n",
    "    lda_topics = [[vectorizer.get_feature_names_out()[i] for i in topic.argsort()[:-10 - 1:-1]] for topic in lda_model.components_]\n",
    "    dictionary = Dictionary([text.split() for text in texts])\n",
    "    corpus = [dictionary.doc2bow(text.split()) for text in texts]\n",
    "    coherence_model = CoherenceModel(topics=lda_topics, texts=[text.split() for text in texts],\n",
    "                                     dictionary=dictionary, coherence='u_mass')\n",
    "    return coherence_model.get_coherence()\n",
    "\n",
    "# Evaluate coherence scores\n",
    "coherence_scores = []\n",
    "for lda_model in lda_models:\n",
    "    score = # TODO: call compute_coherence_score for each model\n",
    "    coherence_scores.append(score)\n",
    "    \n",
    "# Plot coherence scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_topics_list, coherence_scores, marker='o')\n",
    "plt.title('Topic Coherence Scores vs. Number of Topics')\n",
    "plt.xlabel('Number of Topics')\n",
    "plt.ylabel('Coherence Score')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
