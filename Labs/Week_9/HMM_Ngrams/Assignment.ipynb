{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Markdov Model and Ngram(Graded): Named Entity Relationship Dataset\n",
    "\n",
    "Welcome to your programming assignment on HMM and NGram! you will build HMM to classify tagged words.\n",
    "\n",
    "\n",
    "Note: This notebook would take more than 8 mins to run depending on GPU Availablity\n",
    "\n",
    "\n",
    "It is a common task that involves identifying and categorizing  named entities. The project include on simpler task of sentence likelihood prediction, that involves in determining the probability of a given sentence being valid based on a Hidden Markov Model(HMM). Let's also implement the Viterbi algorithm to determine the best probable path and compare it with the hmmlearn implementation. Additionally, formulated an HMM and completed the forward and backward function in the Baum-Welch algorithm to improve the HMM model. \n",
    "\n",
    "### Dataset Description\n",
    "\n",
    "#### Content\n",
    "The dataset with 1M x 4 dimensions contains columns = ['# Sentence', 'Word', 'POS', 'Tag'] and is grouped by #Sentence.\n",
    "\n",
    "#### Columns\n",
    "- **Word**: This column contains English dictionary words from the sentence it is taken from.\n",
    "- **POS**: Parts of speech tag\n",
    "\n",
    "#### Instructions\n",
    "- Do not modify any of the codes.\n",
    "- Only write code when prompted. For example in some sections you will find the following,\n",
    "```\n",
    "# YOUR CODE GOES HERE\n",
    "# YOUR CODE ENDS HERE\n",
    "# TODO\n",
    "```\n",
    "\n",
    "Only modify those sections of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:58:43.784412Z",
     "iopub.status.busy": "2024-12-10T17:58:43.784139Z",
     "iopub.status.idle": "2024-12-10T17:58:45.177191Z",
     "shell.execute_reply": "2024-12-10T17:58:45.176274Z",
     "shell.execute_reply.started": "2024-12-10T17:58:43.784388Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import hmmlearn\n",
    "from hmmlearn import hmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:58:52.437418Z",
     "iopub.status.busy": "2024-12-10T17:58:52.437130Z",
     "iopub.status.idle": "2024-12-10T17:58:53.176417Z",
     "shell.execute_reply": "2024-12-10T17:58:53.175543Z",
     "shell.execute_reply.started": "2024-12-10T17:58:52.437393Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt  # show graph\n",
    "import random\n",
    "\n",
    "#some other libraries\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from typing import List\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, \\\n",
    "    f1_score, roc_auc_score\n",
    "\n",
    "from Assignment_helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:58:53.178293Z",
     "iopub.status.busy": "2024-12-10T17:58:53.177740Z",
     "iopub.status.idle": "2024-12-10T17:58:53.874447Z",
     "shell.execute_reply": "2024-12-10T17:58:53.873629Z",
     "shell.execute_reply.started": "2024-12-10T17:58:53.178263Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/NER dataset.csv\", encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains a total of 47960 sentences, with 1,048,575 words. The entities includes 9 named entity types including person names, locations, organizations, dates, times, percentages and others.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: Data pre-processing\n",
    "- Fill missing values in the dataset using forward fill method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:58:53.875488Z",
     "iopub.status.busy": "2024-12-10T17:58:53.875273Z",
     "iopub.status.idle": "2024-12-10T17:58:54.609645Z",
     "shell.execute_reply": "2024-12-10T17:58:54.608784Z",
     "shell.execute_reply.started": "2024-12-10T17:58:53.875471Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data = 0 #TODO: Fill missing values in the dataset\n",
    "data = data.rename(columns={'Sentence #': 'sentence'})\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: This pre-processing (lowercase any words, remove stop words, replace numbers/names by a unique NUM/NAME token, etc.) you should list of english stop words here in the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:58:54.611249Z",
     "iopub.status.busy": "2024-12-10T17:58:54.610989Z",
     "iopub.status.idle": "2024-12-10T17:58:54.616623Z",
     "shell.execute_reply": "2024-12-10T17:58:54.615723Z",
     "shell.execute_reply.started": "2024-12-10T17:58:54.611228Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def pre_processing(text_column):\n",
    "    # lowercase all text in the column\n",
    "    text_column = text_column.str.lower()\n",
    "\n",
    "    # replacing numbers with NUM token\n",
    "    text_column = text_column.str.replace(r'\\d+', 'NUM')\n",
    "\n",
    "    # removing stopwords\n",
    "    stop_words = 0 # TODO: Get the list of stopwords from the nltk library in English\n",
    "    text_column = text_column.apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "    return text_column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Create a new dataset the unique words and the unique tags in the dataset, by calling `pre_process()` to keep both versions and compare the effect of your pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:58:54.617813Z",
     "iopub.status.busy": "2024-12-10T17:58:54.617510Z",
     "iopub.status.idle": "2024-12-10T17:58:56.005276Z",
     "shell.execute_reply": "2024-12-10T17:58:56.004548Z",
     "shell.execute_reply.started": "2024-12-10T17:58:54.617778Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_pre_precessed = 0 # TODO: Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-processing step is performed to clean and transform the data in a way that is more suitable for further analysis. The pre-processing functions performed includes, converting all the words in the 'Word' column to lowercase and this is done to standardize the capitalization of the words and ensure that they are all in the same case. Then replaced all the numeric values in the 'Word' column with the token 'NUM'. This is done to avoid overfitting of the model to specific numerical values that may not be relevant to the task. Then removed the stop words, which are common words that don't convey any significant meaning in the context of the text. The number of unique tags and unique words in the original dataset was 42 and 29764 respectively. After pre-processing and removing empty/null rows, the number of unique words is reduced to 24031."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:58:56.006701Z",
     "iopub.status.busy": "2024-12-10T17:58:56.006436Z",
     "iopub.status.idle": "2024-12-10T17:58:56.013444Z",
     "shell.execute_reply": "2024-12-10T17:58:56.012587Z",
     "shell.execute_reply.started": "2024-12-10T17:58:56.006657Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_pre_precessed.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: There's lot of missing data in this preprocessed frame let's handle it.\n",
    "Assign the pre-processed 'Word' column to the data_processed dataframe.\n",
    "\n",
    "Task: Remove rows where the 'Word' column is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:58:56.016816Z",
     "iopub.status.busy": "2024-12-10T17:58:56.016326Z",
     "iopub.status.idle": "2024-12-10T17:58:56.347418Z",
     "shell.execute_reply": "2024-12-10T17:58:56.346721Z",
     "shell.execute_reply.started": "2024-12-10T17:58:56.016794Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#creating new dataframe with preprocessed word as a column\n",
    "data_processed = data\n",
    "data_processed['Word'] = 0 # TODO: Add the preprocessed data to the new dataframe\n",
    "\n",
    "#removing the rows where word is empty\n",
    "data_processed = 0 # TODO: Remove the rows where the word is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:58:56.348691Z",
     "iopub.status.busy": "2024-12-10T17:58:56.348438Z",
     "iopub.status.idle": "2024-12-10T17:58:56.358638Z",
     "shell.execute_reply": "2024-12-10T17:58:56.357744Z",
     "shell.execute_reply.started": "2024-12-10T17:58:56.348656Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_processed.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:58:56.360660Z",
     "iopub.status.busy": "2024-12-10T17:58:56.360103Z",
     "iopub.status.idle": "2024-12-10T17:58:56.456103Z",
     "shell.execute_reply": "2024-12-10T17:58:56.455155Z",
     "shell.execute_reply.started": "2024-12-10T17:58:56.360628Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tags = list(set(data.POS.values))  # Unique POS tags in the dataset\n",
    "words = list(set(data.Word.values))  # Unique words in the dataset\n",
    "len(tags), len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should have 42 tags and 29764 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Get the unique words from the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:58:56.457374Z",
     "iopub.status.busy": "2024-12-10T17:58:56.457130Z",
     "iopub.status.idle": "2024-12-10T17:58:56.511912Z",
     "shell.execute_reply": "2024-12-10T17:58:56.511012Z",
     "shell.execute_reply.started": "2024-12-10T17:58:56.457354Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "words1 = 0  # TODO: from preprocessed data get unique words in the dataset\n",
    "len(words1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should have length of 29763 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have 42 different tags and 29,764 different words, so the HMM that we construct will have the following properties\n",
    "- The hidden states of the this HMM will correspond to the tags, so we will have 42 hidden states.\n",
    "- The Observations for this HMM will correspond to the sentences and their words.\n",
    "\n",
    "#### Before constructing the HMM, we will split the data into train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:58:56.513660Z",
     "iopub.status.busy": "2024-12-10T17:58:56.513334Z",
     "iopub.status.idle": "2024-12-10T17:58:57.509564Z",
     "shell.execute_reply": "2024-12-10T17:58:57.508878Z",
     "shell.execute_reply.started": "2024-12-10T17:58:56.513631Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y = data.POS\n",
    "X = data.drop('POS', axis=1)\n",
    "\n",
    "gs = GroupShuffleSplit(n_splits=2, test_size=.33, random_state=42)\n",
    "train_ix, test_ix = next(gs.split(X, y, groups=data['sentence']))\n",
    "\n",
    "data_train = data.loc[train_ix]\n",
    "data_test = data.loc[test_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:58:57.510808Z",
     "iopub.status.busy": "2024-12-10T17:58:57.510542Z",
     "iopub.status.idle": "2024-12-10T17:58:57.519627Z",
     "shell.execute_reply": "2024-12-10T17:58:57.518727Z",
     "shell.execute_reply.started": "2024-12-10T17:58:57.510787Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:58:57.521077Z",
     "iopub.status.busy": "2024-12-10T17:58:57.520836Z",
     "iopub.status.idle": "2024-12-10T17:58:57.531943Z",
     "shell.execute_reply": "2024-12-10T17:58:57.531241Z",
     "shell.execute_reply.started": "2024-12-10T17:58:57.521057Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:58:57.533210Z",
     "iopub.status.busy": "2024-12-10T17:58:57.532954Z",
     "iopub.status.idle": "2024-12-10T17:58:58.176560Z",
     "shell.execute_reply": "2024-12-10T17:58:58.175917Z",
     "shell.execute_reply.started": "2024-12-10T17:58:57.533190Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#using preprocessed data \n",
    "\n",
    "y1 = data_processed.POS\n",
    "X1 = data_processed.drop('POS', axis=1)\n",
    "data_processed.reset_index(drop=True, inplace=True)\n",
    "gs = GroupShuffleSplit(n_splits=2, test_size=.33, random_state=42)\n",
    "train_ix1, test_ix1 = next(gs.split(X1, y1, groups=data_processed['sentence']))\n",
    "\n",
    "data_train1 = data_processed.loc[train_ix1]\n",
    "data_test1 = data_processed.loc[test_ix1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:58:58.177849Z",
     "iopub.status.busy": "2024-12-10T17:58:58.177580Z",
     "iopub.status.idle": "2024-12-10T17:58:58.186456Z",
     "shell.execute_reply": "2024-12-10T17:58:58.185525Z",
     "shell.execute_reply.started": "2024-12-10T17:58:58.177827Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_train1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:58:58.188104Z",
     "iopub.status.busy": "2024-12-10T17:58:58.187774Z",
     "iopub.status.idle": "2024-12-10T17:58:58.200125Z",
     "shell.execute_reply": "2024-12-10T17:58:58.199142Z",
     "shell.execute_reply.started": "2024-12-10T17:58:58.188074Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_test1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets encode the POS and Words to be used to generate the HMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:58:58.201404Z",
     "iopub.status.busy": "2024-12-10T17:58:58.201103Z",
     "iopub.status.idle": "2024-12-10T17:58:58.495866Z",
     "shell.execute_reply": "2024-12-10T17:58:58.495012Z",
     "shell.execute_reply.started": "2024-12-10T17:58:58.201383Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dfupdate = data_train.sample(frac=.15, replace=False, random_state=42)\n",
    "dfupdate.Word = 'UNKNOWN'\n",
    "data_train.update(dfupdate)\n",
    "words = list(set(data_train.Word.values))\n",
    "# Convert words and tags into numbers\n",
    "word2id = {w: i for i, w in enumerate(words)}\n",
    "tag2id = {t: i for i, t in enumerate(tags)}\n",
    "id2tag = {i: t for i, t in enumerate(tags)}\n",
    "len(tags), len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lecture you might have heard that the Hidden Markov Models can be learned by using the Baum-Welch algorithm by just using the observations.\n",
    "Although we can learn the Hidden States (tags) using Baum-Welch algorithm,We cannot map them back the states (words) to the tag. So for this exercise we will skip using the BW algorithm and directly create the HMM.\n",
    "\n",
    "For creating the HMM we should build the following three parameters. \n",
    "- `startprob_`\n",
    "- `transmat_`\n",
    "- `emissionprob_`\n",
    "\n",
    "To construct the above mentioned paramters let's first create some useful matrices that will assist us in creating the above three parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:58:58.497652Z",
     "iopub.status.busy": "2024-12-10T17:58:58.497049Z",
     "iopub.status.idle": "2024-12-10T17:58:59.766323Z",
     "shell.execute_reply": "2024-12-10T17:58:59.765477Z",
     "shell.execute_reply.started": "2024-12-10T17:58:58.497621Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "count_tags = dict(data_train.POS.value_counts())  # Total number of POS tags in the dataset\n",
    "# Now let's create the tags to words count\n",
    "count_tags_to_words = data_train.groupby(['POS']).apply(\n",
    "    lambda grp: grp.groupby('Word')['POS'].count().to_dict()).to_dict()\n",
    "# We shall also collect the counts for the first tags in the sentence\n",
    "count_init_tags = dict(data_train.groupby('sentence').first().POS.value_counts())\n",
    "\n",
    "# Create a mapping that stores the frequency of transitions in tags to it's next tags\n",
    "count_tags_to_next_tags = np.zeros((len(tags), len(tags)), dtype=int)\n",
    "sentences = list(data_train.sentence)\n",
    "pos = list(data_train.POS)\n",
    "for i in tqdm(range(len(sentences)), position=0, leave=True):\n",
    "    if (i > 0) and (sentences[i] == sentences[i - 1]):\n",
    "        prevtagid = tag2id[pos[i - 1]]\n",
    "        nexttagid = tag2id[pos[i]]\n",
    "        count_tags_to_next_tags[prevtagid][nexttagid] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Let's build the parameter matrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:58:59.767875Z",
     "iopub.status.busy": "2024-12-10T17:58:59.767584Z",
     "iopub.status.idle": "2024-12-10T17:59:00.290452Z",
     "shell.execute_reply": "2024-12-10T17:59:00.289612Z",
     "shell.execute_reply.started": "2024-12-10T17:58:59.767853Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "startprob = np.zeros((len(tags),))\n",
    "\"\"\"\n",
    "Initializes and populates the start probability, transition probability, and emission probability matrices for a Hidden Markov Model (HMM).\n",
    "\n",
    "Variables:\n",
    "    startprob (numpy.ndarray): A 1D array of size (number of tags,) initialized to zeros, representing the initial state probabilities.\n",
    "    transmat (numpy.ndarray): A 2D array of size (number of tags, number of tags) initialized to zeros, representing the state transition probabilities.\n",
    "    emissionprob (numpy.ndarray): A 2D array of size (number of tags, number of words) initialized to zeros, representing the emission probabilities.\n",
    "    num_sentences (int): The total number of sentences, calculated as the sum of initial tag counts.\n",
    "    sum_tags_to_next_tags (numpy.ndarray): A 1D array representing the sum of transitions from each tag to the next tags.\n",
    "\n",
    "Loop Variables:\n",
    "    tag (str): The current tag in the iteration.\n",
    "    tagid (int): The corresponding ID of the current tag.\n",
    "    floatCountTag (float): The count of the current tag converted to float.\n",
    "    word (str): The current word in the iteration.\n",
    "    wordid (int): The corresponding ID of the current word.\n",
    "    tag2 (str): The next tag in the nested iteration.\n",
    "    tagid2 (int): The corresponding ID of the next tag.\n",
    "\n",
    "Operations:\n",
    "    - Calculates the initial state probabilities for each tag.\n",
    "    - Calculates the emission probabilities for each tag-word pair.\n",
    "    - Calculates the transition probabilities for each tag-tag pair.\n",
    "\"\"\"\n",
    "transmat = np.zeros((len(tags), len(tags)))\n",
    "emissionprob = np.zeros((len(tags), len(words)))\n",
    "num_sentences = sum(count_init_tags.values())\n",
    "sum_tags_to_next_tags = np.sum(count_tags_to_next_tags, axis=1)\n",
    "for tag, tagid in tqdm(tag2id.items(), position=0, leave=True):\n",
    "    floatCountTag = float(count_tags.get(tag, 0))\n",
    "    startprob[tagid] = count_init_tags.get(tag, 0) / num_sentences\n",
    "    for word, wordid in word2id.items():\n",
    "        emissionprob[tagid][wordid] = count_tags_to_words.get(tag, {}).get(word, 0) / floatCountTag\n",
    "    for tag2, tagid2 in tag2id.items():\n",
    "        transmat[tagid][tagid2] = count_tags_to_next_tags[tagid][tagid2] / sum_tags_to_next_tags[tagid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task: Similar to how we built the hidden state transition probability matrix as shown above, you will build the transition probability between the words. With this matrix write a function that can calculate the log likelihood given a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T17:59:00.291989Z",
     "iopub.status.busy": "2024-12-10T17:59:00.291648Z",
     "iopub.status.idle": "2024-12-10T18:04:17.110632Z",
     "shell.execute_reply": "2024-12-10T18:04:17.109723Z",
     "shell.execute_reply.started": "2024-12-10T17:59:00.291959Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#To create word transition matrix\n",
    "\n",
    "#first step is to count the number of times each word appears in the dataset\n",
    "count_words = {}\n",
    "for word in data_train.Word.values:\n",
    "    count_words[word] = count_words.get(word, 0) + 1\n",
    "\n",
    "# then count the number of times a word appears after another word\n",
    "count_word_transitions = {}\n",
    "for sentence in data_train.groupby('sentence'):\n",
    "    words = sentence[1]['Word'].values\n",
    "    for i in range(len(words) - 1):\n",
    "        w1, w2 = words[i], words[i+1]\n",
    "        if w1 not in count_word_transitions:\n",
    "            count_word_transitions[w1] = {}\n",
    "        count_word_transitions[w1][w2] = count_word_transitions[w1].get(w2, 0) + 1\n",
    "\n",
    "# convert the counts to probabilities\n",
    "word_transition_matrix = 0 # TODO: Create a matrix of zeros with the shape of the number of unique words (Hint: Make sure to use the word2id dictionary)\n",
    "sum_words_to_next_words = 0 # TODO: Calculate the sum of words to next words\n",
    "for w1, w1id in word2id.items():\n",
    "    for w2, w2id in word2id.items():\n",
    "        word_transition_matrix[w1id][w2id] = count_word_transitions.get(w1, {}).get(w2, 0) / sum_words_to_next_words\n",
    "print(word_transition_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see the matrix shape of 23608, 23608"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T18:04:17.112179Z",
     "iopub.status.busy": "2024-12-10T18:04:17.111889Z",
     "iopub.status.idle": "2024-12-10T18:04:17.118088Z",
     "shell.execute_reply": "2024-12-10T18:04:17.117110Z",
     "shell.execute_reply.started": "2024-12-10T18:04:17.112156Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_log_likelihood(sentence: List[str], word_transition_matrix) -> float:\n",
    "    \"\"\"\n",
    "    Given a sentence and word_transition_matrix, returns the log-likelihood of the sentence.\n",
    "    \"\"\"\n",
    "    # converting the sentence to a list of word IDs\n",
    "    sentence_ids = [word2id.get(w, word2id['UNKNOWN']) for w in sentence]\n",
    "\n",
    "    # calculating the log-likelihood using the word transition matrix\n",
    "    log_likelihood = np.log(word_transition_matrix[sentence_ids[0]][sentence_ids[1]])\n",
    "    for i in range(1, len(sentence_ids) - 1):\n",
    "        log_likelihood += np.log(word_transition_matrix[sentence_ids[i]][sentence_ids[i+1]] + 1e-10)\n",
    "    return log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T18:04:17.119752Z",
     "iopub.status.busy": "2024-12-10T18:04:17.119177Z",
     "iopub.status.idle": "2024-12-10T18:04:17.130302Z",
     "shell.execute_reply": "2024-12-10T18:04:17.129452Z",
     "shell.execute_reply.started": "2024-12-10T18:04:17.119721Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "calculate_log_likelihood([0], word_transition_matrix) # TODO: Replace the 0 with a test sentence of tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, a word transition probability matrix is built and a function that can calculate the log likelihood given a sentence. The transition probability matrix was created by first counting the number of times each word appeared in the training dataset and then counting the number of times a word appeared after another word. These counts were then converted to probabilities to create the transition probability matrix.\n",
    "\n",
    "Then wrote a function to calculate the log-likelihood of a given sentence using the word transition probabilities. The sentence was first converted into a list of word IDs, and then the log-likelihood was calculated using the word transition probabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task: Now we will continue to constructing the HMM.\n",
    "\n",
    "We will use the hmmlearn implementation to initialize the HMM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T18:04:17.131615Z",
     "iopub.status.busy": "2024-12-10T18:04:17.131365Z",
     "iopub.status.idle": "2024-12-10T18:04:17.137926Z",
     "shell.execute_reply": "2024-12-10T18:04:17.137244Z",
     "shell.execute_reply.started": "2024-12-10T18:04:17.131595Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = hmm.MultinomialHMM(n_components=0, algorithm='', random_state=42) # TODO: Create a Multinomial HMM model with the number of components equal to the number of unique tags using the vertibi algorithm.\n",
    "model.startprob_ = startprob\n",
    "model.transmat_ = transmat\n",
    "model.emissionprob_ = emissionprob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before using the HMM to predict the POS tags, we have to fix the training set as some of the words and tags in the test data might not appear in the training data so we collect this data to use it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T18:04:17.139468Z",
     "iopub.status.busy": "2024-12-10T18:04:17.138994Z",
     "iopub.status.idle": "2024-12-10T18:04:17.990333Z",
     "shell.execute_reply": "2024-12-10T18:04:17.989647Z",
     "shell.execute_reply.started": "2024-12-10T18:04:17.139439Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_test.loc[~data_test['Word'].isin(words), 'Word'] = 'UNKNOWN'\n",
    "word_test = list(data_test.Word)\n",
    "samples = []\n",
    "for i, val in enumerate(word_test):\n",
    "    samples.append([word2id[val]])\n",
    "\n",
    "    \n",
    "lengths = []\n",
    "count = 0\n",
    "sentences = list(data_test.sentence)\n",
    "for i in tqdm(range(len(sentences)), position=0, leave=True):\n",
    "    if (i > 0) and (sentences[i] == sentences[i - 1]):\n",
    "        count += 1\n",
    "    elif i > 0:\n",
    "        lengths.append(count)\n",
    "        count = 1\n",
    "    else:\n",
    "        count = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the HMM ready lets predict the best path from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T18:04:17.995141Z",
     "iopub.status.busy": "2024-12-10T18:04:17.994904Z",
     "iopub.status.idle": "2024-12-10T18:05:48.705383Z",
     "shell.execute_reply": "2024-12-10T18:05:48.704533Z",
     "shell.execute_reply.started": "2024-12-10T18:04:17.995122Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pos_predict = model.predict(samples, lengths)\n",
    "pos_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hmmlearn predict function will give the best probable path for the given sentence using the Viterbi algorithm.\n",
    "\n",
    "### Task: Using the model parameters (startprob_, transmat_, emissionprob_) write the viterbi algorithm to calculate the best probable path and compare it with the hmmlearn implementation.\n",
    "\n",
    "Now before using these matrices \n",
    "\n",
    "viterbi algorithm to find the best probable path using the model parameters startprob, transmat, emissionprob. The algorithm takes in the initial probabilities, transition probabilities, emission probabilities and a list of obeservations and returns an array of the indices of the best hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T18:05:48.706821Z",
     "iopub.status.busy": "2024-12-10T18:05:48.706530Z",
     "iopub.status.idle": "2024-12-10T18:05:48.713561Z",
     "shell.execute_reply": "2024-12-10T18:05:48.712744Z",
     "shell.execute_reply.started": "2024-12-10T18:05:48.706799Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def viterbi(pi: np.array, a: np.array, b: np.array, obs: List) -> np.array:\n",
    "    \"\"\"\n",
    "     Write the viterbi algorithm from scratch to find the best probable path\n",
    "     attr:\n",
    "       pi: initial probabilities\n",
    "       a: transition probabilities\n",
    "       b: emission probabilities\n",
    "       obs: list of observations\n",
    "     return:\n",
    "       array of the indices of the best hidden states\n",
    "    \"\"\"\n",
    "    # state space cardinality\n",
    "    K = a.shape[0]\n",
    "\n",
    "    # observation sequence length\n",
    "    T = len(obs)\n",
    "\n",
    "    # initializing the tracking tables from first observation\n",
    "    delta = np.zeros((T, K))\n",
    "    psi = np.zeros((T, K))\n",
    "    delta[0] = 0 # TODO: Initialize the delta table with the initial probabilities and the first observation\n",
    "\n",
    "    # iterating throught the observations updating the tracking tables\n",
    "    for t in range(1, T):\n",
    "        for j in range(K):\n",
    "            delta[t, j] = np.max(delta[t-1] * a[:, j] * b[j, obs[t]]) \n",
    "            psi[t, j] = 0 # TODO: Update the delta and psi tables using argmax function\n",
    "\n",
    "    # build the output, optimal model trajectory\n",
    "    x = np.zeros(T, dtype=int)\n",
    "    x[T-1] = np.argmax(delta[T-1])\n",
    "    for t in range(T-2, -1, -1):\n",
    "        x[t] = psi[t+1, x[t+1]]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Let's try to form our own HMM\n",
    "In this task you will try to formulate your own HMM. Image a toy example that you think that closely relates to a Hidden Markov Model.\n",
    "\n",
    "Steps:\n",
    " 1. Define your hidden states\n",
    " 2. Define your observable states\n",
    " 3. Randomly generate your observations\n",
    "\n",
    "Below is an example to demonstrate:\n",
    "\n",
    "-In this toy HMM example, we have two hidden states 'healthy' and 'sick' these states relate to the state of a pet. In this example we cannot exactly know the situation of the pet if it is 'healthy' or 'sick'\n",
    "\n",
    "-The observable states in this formulation is the what our pet is doing, whether it is sleeping, eating or pooping. We ideally want to determine if the pet is sick or not using these observable states\n",
    "\n",
    "\n",
    "```python\n",
    "hidden_states = ['healthy', 'sick']\n",
    "observable_states = ['sleeping', 'eating', 'pooping']\n",
    "observations = []\n",
    "for i in range(100):\n",
    "  observations.append(random.choice(observable_states))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T18:05:48.715222Z",
     "iopub.status.busy": "2024-12-10T18:05:48.714895Z",
     "iopub.status.idle": "2024-12-10T18:05:48.724248Z",
     "shell.execute_reply": "2024-12-10T18:05:48.723422Z",
     "shell.execute_reply.started": "2024-12-10T18:05:48.715193Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "hidden_states = 0 # TODO: Create a list of all the weather seasons\n",
    "observable_states = 0 #TODO: Create a list of all possible of how the weather feels like\n",
    "observations =  []\n",
    "\n",
    "for i in range(40):\n",
    "  obs_index = random.randint(0, len(observable_states)-1) # random index corresponding to the observable state\n",
    "  observations.append(obs_index) # then adding the index to the observations list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T18:05:48.726212Z",
     "iopub.status.busy": "2024-12-10T18:05:48.725520Z",
     "iopub.status.idle": "2024-12-10T18:05:48.762171Z",
     "shell.execute_reply": "2024-12-10T18:05:48.761454Z",
     "shell.execute_reply.started": "2024-12-10T18:05:48.726190Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "hidden_state_sequence = viterbi(startprob, transmat, emissionprob, observations)\n",
    "\n",
    "print(\"Observations:\", observations)\n",
    "print(\"Viterbi sequence:\", hidden_state_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even tough we have generated the data randomly, for the learning purposes, let's try to learn an HMM from this data. let's use Baum-Welch algorithm\n",
    "\n",
    "### TASK: let's try with observable map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T18:05:48.763512Z",
     "iopub.status.busy": "2024-12-10T18:05:48.763285Z",
     "iopub.status.idle": "2024-12-10T18:05:53.343411Z",
     "shell.execute_reply": "2024-12-10T18:05:53.342576Z",
     "shell.execute_reply.started": "2024-12-10T18:05:48.763493Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "hidden_states = ['healthy', 'sick']\n",
    "observable_states = ['sleeping', 'eating', 'pooping']\n",
    "observable_map = 0 # TODO: Create a dictionary mapping the observable states to integers\n",
    "observations = []\n",
    "for i in range(40):\n",
    "    observations.append(observable_map[random.choice(observable_states)])\n",
    "\n",
    "A, B = baum_welch(observations=observations, observations_vocab=np.array(list(observable_map.values())),\n",
    "                  n_hidden_states=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemented the Baum-Welch algorithm for estimating the HMM parameters by the forward, backward, and E-step functions to compute the gamma and sigma probabilities. These probabilities are used to estimate the HMM parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-10T18:05:53.345009Z",
     "iopub.status.busy": "2024-12-10T18:05:53.344633Z",
     "iopub.status.idle": "2024-12-10T18:05:53.391628Z",
     "shell.execute_reply": "2024-12-10T18:05:53.390840Z",
     "shell.execute_reply.started": "2024-12-10T18:05:53.344974Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "hidden_state_sequence = viterbi(startprob, transmat, emissionprob, observations)\n",
    "\n",
    "print(\"Observations:\", observations)\n",
    "print(\"Viterbi sequence:\", hidden_state_sequence)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1232095,
     "sourceId": 2056195,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30476,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
