{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47af4f5c",
   "metadata": {},
   "source": [
    "### Hidden Markov Model and NGram\n",
    "#### Breif recap of Hidden Markov Model\n",
    "Hidden Markov Models (HMMs) are statistical models that represent systems with hidden states. They are used to model sequential data where the system being modeled is assumed to follow a Markov process with hidden states. HMMs are characterized by:\n",
    "\n",
    "- **States**: The hidden states of the system.\n",
    "- **Observations**: The observed data that is generated by the hidden states.\n",
    "- **Transition Probabilities**: The probabilities of transitioning from one state to another.\n",
    "- **Emission Probabilities**: The probabilities of observing a particular observation given a state.\n",
    "- **Initial Probabilities**: The probabilities of the system starting in each state.\n",
    "\n",
    "HMMs are widely used in various applications such as speech recognition, part-of-speech tagging, and bioinformatics.\n",
    "\n",
    "#### Brief recap of NGram\n",
    "N-grams are contiguous sequences of n items from a given sample of text or speech. They are used in various natural language processing tasks to capture the context and structure of the text. N-grams can be unigrams (single words), bigrams (pairs of words), trigrams (triplets of words), and so on.\n",
    "\n",
    "- **Unigram**: A single word. Example: \"the\"\n",
    "- **Bigram**: A pair of consecutive words. Example: \"the cat\"\n",
    "- **Trigram**: A triplet of consecutive words. Example: \"the cat sat\"\n",
    "\n",
    "N-grams are useful for tasks such as text classification, language modeling, and machine translation. They help in understanding the context and meaning of the text by considering the relationships between consecutive words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79de251",
   "metadata": {},
   "source": [
    "## NGram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8003fa53",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "The SMS Spam Collection is a dataset of SMS messages tagged for spam research. It contains 5,574 English messages labeled as either ham (legitimate) or spam.\n",
    "\n",
    "#### Content\n",
    "\n",
    "Each line in the dataset consists of two columns: \n",
    "- **v1**: The label (ham or spam)\n",
    "- **v2**: The raw text of the message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bfb81a-fa85-4706-be4a-5945a9804397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca51ec4b-3116-4de1-9b95-72d56769c59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing data\n",
    "df = pd.read_csv('spam.csv', encoding='latin-1')\n",
    "df.drop(columns=['Unnamed: 2','Unnamed: 3',\t'Unnamed: 4'], inplace = True)\n",
    "df.columns = ['label', 'message']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f10981",
   "metadata": {},
   "source": [
    "### Data Cleaning and Preprocessing\n",
    "\n",
    "In this section, we will clean and preprocess the data to prepare it for analysis. This involves removing unwanted characters, converting text to lowercase, and removing stopwords. We will also use stemming and lemmatization techniques to reduce words to their root forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fd2526-9d53-4186-9a26-2647711ad1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d6649c",
   "metadata": {},
   "source": [
    "#### Stemming\n",
    "\n",
    "Stemming is the process of reducing a word to its base or root form. This is useful in natural language processing to ensure that different forms of a word are treated as the same word. For example, \"running\" and \"runner\" can be reduced to the root word \"run\". One of the most common stemming algorithms is the Porter Stemmer.\n",
    "\n",
    "#### Porter Stemmer\n",
    "\n",
    "The Porter Stemmer is a widely used stemming algorithm that applies a series of rules to transform words into their root forms. It was developed by Martin Porter in 1980 and is known for its simplicity and effectiveness. The algorithm works by iteratively applying a set of rules to remove common suffixes from words.\n",
    "\n",
    "Example:\n",
    "- \"running\" -> \"run\"\n",
    "- \"runner\" -> \"run\"\n",
    "- \"happiness\" -> \"happi\"\n",
    "\n",
    "In the context of our lab, we will use the Porter Stemmer to preprocess the text data and reduce words to their root forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ab24a9-40cd-4b35-82d0-64aa48e136d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b87c4a-4a14-41f9-b3f7-75989570d8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data cleaning and preprocessing\n",
    "corpus = []\n",
    "for i in range(len(df)):\n",
    "    review = re.sub('[^a-zA-Z]',' ', df['message'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabe4c3c",
   "metadata": {},
   "source": [
    "\n",
    "#### Creating Bag of Words Model\n",
    "\n",
    "In this section, we will create a Bag of Words (BoW) model using the `CountVectorizer` from the `sklearn.feature_extraction.text` module. The BoW model is a common text representation technique used in natural language processing. It converts text data into numerical feature vectors, where each feature represents the frequency or presence of a word in the text.\n",
    "\n",
    "We will use the `CountVectorizer` with the following parameters:\n",
    "- `max_features=2500`: Limits the number of features to the top 2500 most frequent words.\n",
    "- `binary=True`: Indicates that the feature values should be binary (1 if the word is present, 0 if not).\n",
    "\n",
    "The resulting feature vectors will be stored in the variable `X`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6384c687-a92b-4e54-a6af-225e244898cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating bag of words model:\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=2500, binary=True)\n",
    "X = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6327a9a9-842c-4b04-9a75-845c9f159eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X, columns=cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2bf113-9407-4775-9511-d6ac3525142f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing using Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c590ed81-9cdc-4e56-8ec3-dfe567a48721",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    review = re.sub('[^a-zA-Z]',' ', df['message'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    [lemma.lemmatize(word) for word in review if not word in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a7d54c-c325-4fb2-8e48-28bcdd33a50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features=2500, binary=True)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "pd.DataFrame(X, columns=cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f3d52d-01c9-4660-aed9-c558b144ddac",
   "metadata": {},
   "source": [
    "<p>sklearn offers a parameter known as N gram for creation of BOW. \n",
    "<p>Consider the below set of messages:\n",
    "<li>'Boy is Good'\n",
    "<li>'Boy is not Good'\n",
    "<p>The vector representation of the above would be [1,1,1,0] and [1,1,1,1] given the vocabulary is ['Boy','is','good','not']\n",
    "<p>It can be observed that the cosine similarity of the above two vectors is high but the meaning implies otherwise.\n",
    "<p>To counter this issue, N gram can be used, which helps adding combination of 2 or more words to vocabulary.\n",
    "<p>for, N gram = [1,2] ---> vocab: ['Boy','is','good','not', 'Boy is', 'is good', 'good not'....]\n",
    "<p>vect representation: sentence 1 ---> [1,1,1,0,1,1,0] and sentence 2 ---> [1,1,1,1,1,1,1]\n",
    "<p>Now the similarity is reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28960f5c-4c7d-45e5-8535-57b6faf82574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using N gram: \n",
    "# Combination of unigram and bigram = [1,2]\n",
    "# Combination of unigram and trigram = [1,3]\n",
    "# Combination of bigram and trigram = [2,3] so on..\n",
    "\n",
    "cv = CountVectorizer(max_features=2500, binary=True,ngram_range=(1, 2))\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "pd.DataFrame(X, columns=cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f54ca9a",
   "metadata": {},
   "source": [
    "# Hidden markov Model\n",
    "we are gonna do spellcheck and auto complete using 'HMM\n",
    "\n",
    "The code that would fit at $PLACEHOLDER$ without ``` is:\n",
    "\n",
    "To perform spellcheck and autocomplete using Hidden Markov Model (HMM), we will use the following steps:\n",
    "\n",
    "1. **Spellcheck**: We will use a pre-trained HMM tagger to identify and correct misspelled words in a given sentence. The HMM tagger will help us determine the most likely sequence of words, and we will use a dictionary of common misspellings to suggest corrections.\n",
    "\n",
    "2. **Autocomplete**: We will build a bigram model from a corpus of text to predict the next word(s) in a given sentence. The bigram model will help us generate probable word sequences based on the context provided by the input text.\n",
    "\n",
    "The implementation details are provided in the subsequent code cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff587d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import reuters\n",
    "from collections import defaultdict, Counter\n",
    "from lab_helpers import *\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('reuters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3d048d",
   "metadata": {},
   "source": [
    "### Autocomplete using Hidden Markov Model\n",
    "\n",
    "In this section, we will implement an autocomplete feature using a Hidden Markov Model (HMM). The steps involved are:\n",
    "\n",
    "1. **Building a Bigram Model**: a bigram model from a corpus of text. The bigram model will help us predict the next word based on the previous word in the sequence.\n",
    "2. **Autocomplete Function**: We already defined a function that takes an input text and uses the bigram model to predict the next few words, completing the input text.\n",
    "\n",
    "The code implementation is provided in the subsequent code cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7804a9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = reuters.sents(categories='acq')\n",
    "\n",
    "def build_bigram_model(corpus):\n",
    "    bigram_model = defaultdict(Counter)\n",
    "    for sentence in corpus:\n",
    "        sentence = [word.lower() for word in sentence]\n",
    "        for w1, w2 in ngrams(sentence, 2, pad_left=True, pad_right=True, left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\"):\n",
    "            bigram_model[w1][w2] += 1\n",
    "    return bigram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533d9f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model = build_bigram_model(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca4146b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Machine learning is\"\n",
    "completed_text = autocomplete(input_text, bigram_model, num_words=3)\n",
    "print(\"Autocomplete:\", completed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a95536",
   "metadata": {},
   "source": [
    "### Autocomplete using Custom NLTK Data\n",
    "\n",
    "In this section, we will implement an autocomplete feature using a Hidden Markov Model (HMM) with a custom NLTK data path. The steps involved are:\n",
    "\n",
    "1. **Setting Up Custom NLTK Data Path**: We will set up a custom path for NLTK data downloads and ensure the necessary corpora are available.\n",
    "2. **Building a Bigram Model**: We will build a bigram model from the Gutenberg corpus. The bigram model will help us predict the next word based on the previous word in the sequence.\n",
    "3. **Autocomplete Function**: We already defined a function that takes an input text and uses the bigram model to predict the next few words, completing the input text.\n",
    "\n",
    "The code implementation is provided in the subsequent code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc3db51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02d0c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_nltk_path = os.path.expanduser('~/custom_nltk_data')\n",
    "if not os.path.exists(custom_nltk_path):\n",
    "    os.makedirs(custom_nltk_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fceff653",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt', download_dir=custom_nltk_path)\n",
    "nltk.download('gutenberg', download_dir=custom_nltk_path)\n",
    "\n",
    "\n",
    "nltk.data.path.append(custom_nltk_path)\n",
    "\n",
    "corpus = nltk.corpus.gutenberg.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfae67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bigram_model(corpus):\n",
    "    bigram_model = defaultdict(Counter)\n",
    "    for sentence in corpus:\n",
    "        sentence = [word.lower() for word in sentence]\n",
    "        for w1, w2 in ngrams(sentence, 2, pad_left=True, pad_right=True, left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\"):\n",
    "            bigram_model[w1][w2] += 1\n",
    "    return bigram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25e1a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model = build_bigram_model(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28120d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"machine learning is\"\n",
    "completed_text = autocomplete_HMM(input_text, bigram_model, num_words=3)\n",
    "print(\"Autocomplete:\", completed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f24130",
   "metadata": {},
   "source": [
    "### Spellcheck using Hidden Markov Model\n",
    "\n",
    "In this section, we will implement a spellcheck feature using a Hidden Markov Model (HMM). The steps involved are:\n",
    "\n",
    "1. **Training the HMM Tagger**: We will train an HMM tagger using the treebank corpus. The HMM tagger will help us determine the most likely sequence of words.\n",
    "2. **Spellcheck Function**: We will define a function that takes an input sentence and uses the HMM tagger to identify and correct misspelled words. We will use a dictionary of common misspellings to suggest corrections.\n",
    "\n",
    "The code implementation is provided in the subsequent code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a80cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank, words\n",
    "from nltk.tag import hmm\n",
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "nltk.download('treebank')\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f217e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents = treebank.tagged_sents(tagset='universal')\n",
    "english_words = set(words.words())\n",
    "train_vocab = set(word.lower() for sent in train_sents for word, _ in sent)\n",
    "full_vocabulary = english_words.union(train_vocab)\n",
    "\n",
    "trainer = hmm.HiddenMarkovModelTrainer()\n",
    "tagger = trainer.train(train_sents)\n",
    "\n",
    "common_misspellings = {\n",
    "    'teh': 'the',\n",
    "    'quikc': 'quick',\n",
    "    'brownn': 'brown',\n",
    "    'fxo': 'fox',\n",
    "    'jupms': 'jumps',\n",
    "    'lazzy': 'lazy',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24114cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_correct(word):\n",
    "    return word.lower() in full_vocabulary\n",
    "\n",
    "def hmm_spell_checker(sentence, tagger, common_misspellings):\n",
    "    corrected_sentence = []\n",
    "\n",
    "    for i, word in enumerate(sentence):\n",
    "        if not is_correct(word):\n",
    "            correction = common_misspellings.get(word.lower(), None)\n",
    "            if correction:\n",
    "                corrected_sentence.append(correction)\n",
    "                print(f\"Correcting '{word}' to '{correction}'\")\n",
    "            else:\n",
    "                suggested_word = suggest_corrections(word, full_vocabulary, max_distance=2)\n",
    "                corrected_sentence.append(suggested_word)\n",
    "                print(f\"Suggesting '{suggested_word}' for '{word}'\")\n",
    "        else:\n",
    "            corrected_sentence.append(word)\n",
    "\n",
    "    return \" \".join(corrected_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566e4c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = ['The', 'quikc', 'brownn', 'fxo', 'jupms', 'over', 'the', 'lazzy', 'dog', 'cta', 'cet','catt']\n",
    "corrected_text = hmm_spell_checker(input_sentence, tagger, common_misspellings)\n",
    "print(\"\\nCorrected Sentence:\")\n",
    "print(corrected_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80a2ffc",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this notebook, we explored various natural language processing techniques and models, including Hidden Markov Models (HMMs) and N-grams. We covered the following key points:\n",
    "\n",
    "1. **Hidden Markov Models (HMMs)**: We provided a brief recap of HMMs, their components, and their applications in tasks such as speech recognition and part-of-speech tagging.\n",
    "\n",
    "2. **N-Grams**: We discussed N-grams and their use in capturing the context and structure of text. We implemented a Bag of Words (BoW) model using unigrams, bigrams, and trigrams.\n",
    "\n",
    "3. **Data Cleaning and Preprocessing**: We cleaned and preprocessed the SMS Spam Collection dataset, including removing unwanted characters, converting text to lowercase, and applying stemming and lemmatization techniques.\n",
    "\n",
    "4. **Bag of Words Model**: We created a BoW model using the `CountVectorizer` from `sklearn`, converting text data into numerical feature vectors.\n",
    "\n",
    "5. **Autocomplete using HMM**: We implemented an autocomplete feature using a bigram model built from the Reuters and Gutenberg corpora.\n",
    "\n",
    "6. **Spellcheck using HMM**: We implemented a spellcheck feature using an HMM tagger trained on the treebank corpus and a dictionary of common misspellings.\n",
    "\n",
    "These techniques and models are fundamental in natural language processing and can be applied to various tasks such as text classification, language modeling, and text generation. By understanding and implementing these methods, we can enhance our ability to process and analyze textual data effectively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
