{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding Tokenizer from scratch\n",
    "\n",
    "In this assignment, we will build BPE tokenizer used in GPT models.\n",
    "BPE has some changes compared to WordPiece. For example, BPE does not use a normalizer for tokenizer. We also don’t need to specify an unk_token because GPT-2 uses byte-level BPE, which doesn’t require it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and libraries\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, processors\n",
    "from tests import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load the specified dataset and return the text data.\n",
    "    \"\"\"\n",
    "    dataset = # TODO: Load wikitext dataset wikitext-2-raw-v1 as the config \n",
    "    return dataset[\"train\"][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_bpe_tokenizer():\n",
    "    \"\"\"\n",
    "    Initialize a Byte Pair Encoding (BPE) tokenizer with a Whitespace pre-tokenizer.\n",
    "    \"\"\"\n",
    "    tokenizer = # TODO: Initialize tokenizer with BPE as model\n",
    "    tokenizer.pre_tokenizer = # TODO: Add whitespace removal as pre-tokenizer step\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below code block, we will train BPE on the wiki dataset. We will be adding some special tokens:\n",
    "\n",
    "- Padding token: token used to pad sequences to a uniform length in a batch for processing\n",
    "- Unkwon token: Represents unknown words or tokens not found in the model's vocabulary\n",
    "- Classification token: A special token added at the start of a sequence for classification tasks.\n",
    "- Separator token: Used to separate or mark boundaries between sequences in multi-sequence tasks.\n",
    "- Mask token: A placeholder token used in masked language modeling to predict masked words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe_tokenizer(tokenizer, texts, vocab_size=30000, min_frequency=2, special_tokens=None):\n",
    "    \"\"\"\n",
    "    Train a BPE tokenizer on the provided texts.\n",
    "    \"\"\"\n",
    "    if special_tokens is None:\n",
    "        special_tokens = # TODO: Make a list of ALL the tokens mentioned above\n",
    "    \n",
    "    trainer = # TODO: Initialize trainer with vocab_size, min_frequency, and special tokens\n",
    "\n",
    "    def batch_iterator(batch_size=1000):\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            yield texts[i : i + batch_size]\n",
    "\n",
    "    # TODO: train tokenizer using train_from_iterator function passing relevant parameters\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `configure_post_processing` function sets up rules to add special tokens (e.g., `[CLS]`, `[SEP]`) to tokenized sequences for single or paired inputs, ensuring proper formatting for downstream tasks. It also configures a BPE decoder to reconstruct text from token IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_post_processing(tokenizer):\n",
    "    \"\"\"\n",
    "    Configure the post-processing and decoding rules for the tokenizer.\n",
    "    \"\"\"\n",
    "    tokenizer.post_processor = processors.TemplateProcessing(\n",
    "        single=\"[CLS] $A [SEP]\",\n",
    "        pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "        special_tokens=[\n",
    "            (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "            (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "        ],\n",
    "    )\n",
    "    tokenizer.decoder = decoders.BPEDecoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving tokenizer to the specified path so that it can be used again. This saves training effort and also can be used for multiple projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tokenizer(tokenizer, filepath):\n",
    "    \"\"\"\n",
    "    Save the tokenizer to the specified filepath.\n",
    "    \"\"\"\n",
    "    # TODO: Save tokenizer at the path specified\n",
    "    tokenizer.save(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tokenizer(tokenizer, text):\n",
    "    \"\"\"\n",
    "    Test the tokenizer on a sample text and return the tokens and IDs.\n",
    "    \"\"\"\n",
    "    # TODO: Encode the text using the tokenizer. Return tokens and corresponding IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will assemble all the logic to train a BPE tokenizer and test the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "texts = # TODO: Load data\n",
    "test_load_data(texts)\n",
    "\n",
    "tokenizer = # TODO: # Initialize the tokenizer\n",
    "test_initialize_bpe_tokenizer(tokenizer)\n",
    "\n",
    "tokenizer = # TODO: # Train the tokenizer\n",
    "test_train_bpe_tokenizer(tokenizer)\n",
    "\n",
    "# TODO: Configure post-processing and decoding\n",
    "\n",
    "# TODO: Save the tokenizer\n",
    "\n",
    "# Test the tokenizer\n",
    "test_text = \"Natural Language Processing is fascinating.\"\n",
    "tokens, ids = # TODO: test tokenizer\n",
    "\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"IDs: {ids}\")\n",
    "\n",
    "test_tokenizer_func(tokens, ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
