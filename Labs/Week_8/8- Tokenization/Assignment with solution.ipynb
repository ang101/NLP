{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, processors\n",
    "from tests import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Load the specified dataset and return the text data.\n",
    "    \"\"\"\n",
    "    dataset = load_dataset(\"wikitext\", name=\"wikitext-2-raw-v1\")\n",
    "    return dataset[\"train\"][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_bpe_tokenizer():\n",
    "    \"\"\"\n",
    "    Initialize a Byte Pair Encoding (BPE) tokenizer with a Whitespace pre-tokenizer.\n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe_tokenizer(tokenizer, texts, vocab_size=30000, min_frequency=2, special_tokens=None):\n",
    "    \"\"\"\n",
    "    Train a BPE tokenizer on the provided texts.\n",
    "    \"\"\"\n",
    "    if special_tokens is None:\n",
    "        special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "    \n",
    "    trainer = trainers.BpeTrainer(\n",
    "        vocab_size=vocab_size,\n",
    "        min_frequency=min_frequency,\n",
    "        special_tokens=special_tokens\n",
    "    )\n",
    "\n",
    "    def batch_iterator(batch_size=1000):\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            yield texts[i : i + batch_size]\n",
    "\n",
    "    tokenizer.train_from_iterator(batch_iterator(), trainer)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_post_processing(tokenizer):\n",
    "    \"\"\"\n",
    "    Configure the post-processing and decoding rules for the tokenizer.\n",
    "    \"\"\"\n",
    "    tokenizer.post_processor = processors.TemplateProcessing(\n",
    "        single=\"[CLS] $A [SEP]\",\n",
    "        pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "        special_tokens=[\n",
    "            (\"[CLS]\", tokenizer.token_to_id(\"[CLS]\")),\n",
    "            (\"[SEP]\", tokenizer.token_to_id(\"[SEP]\")),\n",
    "        ],\n",
    "    )\n",
    "    tokenizer.decoder = decoders.BPEDecoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tokenizer(tokenizer, filepath):\n",
    "    \"\"\"\n",
    "    Save the tokenizer to the specified filepath.\n",
    "    \"\"\"\n",
    "    tokenizer.save(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tokenizer(tokenizer, text):\n",
    "    \"\"\"\n",
    "    Test the tokenizer on a sample text and return the tokens and IDs.\n",
    "    \"\"\"\n",
    "    output = tokenizer.encode(text)\n",
    "    return output.tokens, output.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Tokens: ['[CLS]', 'Natural', 'Language', 'Pro', 'cess', 'ing', 'is', 'fac', 'inating', '.', '[SEP]']\n",
      "IDs: [2, 9634, 19539, 2101, 1379, 1035, 1034, 2126, 17091, 18, 3]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Tokens generated and expected tokens do not match",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokens: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIDs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mids\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m \u001b[43mtest_tokenizer_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/IE7500/Labs/NLP/Labs/Tokenization/tests.py:40\u001b[0m, in \u001b[0;36mtest_tokenizer_func\u001b[0;34m(tokens, ids)\u001b[0m\n\u001b[1;32m     38\u001b[0m expected_tokens \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[CLS]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNatural\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLanguage\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPro\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcess\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ming\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfascinating\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[SEP]\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     39\u001b[0m expected_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m9634\u001b[39m, \u001b[38;5;241m19539\u001b[39m, \u001b[38;5;241m2101\u001b[39m, \u001b[38;5;241m1379\u001b[39m, \u001b[38;5;241m1035\u001b[39m, \u001b[38;5;241m1034\u001b[39m, \u001b[38;5;241m23616\u001b[39m, \u001b[38;5;241m18\u001b[39m, \u001b[38;5;241m3\u001b[39m]\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m tokens \u001b[38;5;241m==\u001b[39m expected_tokens, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokens generated and expected tokens do not match\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m ids \u001b[38;5;241m==\u001b[39m expected_ids, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIDs generated and expected IDs do not match\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Tokens generated and expected tokens do not match"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "texts = load_data()\n",
    "\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = initialize_bpe_tokenizer()\n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer = train_bpe_tokenizer(tokenizer, texts)\n",
    "\n",
    "# Configure post-processing and decoding\n",
    "configure_post_processing(tokenizer)\n",
    "\n",
    "# Save the tokenizer\n",
    "save_tokenizer(tokenizer, \"bpe_tokenizer-wikitext2.json\")\n",
    "\n",
    "# Test the tokenizer\n",
    "test_text = \"Natural Language Processing is fascinating.\"\n",
    "tokens, ids = test_tokenizer(tokenizer, test_text)\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"IDs: {ids}\")\n",
    "\n",
    "test_tokenizer_func(tokens, ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
